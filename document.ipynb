{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdcd835a",
   "metadata": {},
   "source": [
    "Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4768f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Document Datastructure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9292d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Santosh Tiwari', 'date_created': '2025-09-22'}, page_content=' This is the main text contents I am using to create a RAG')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content=\" This is the main text contents I am using to create a RAG\",\n",
    "    metadata={\n",
    "        \"source\":\"example.txt\",\n",
    "        \"pages\": 1,\n",
    "        \"author\": \"Santosh Tiwari\",\n",
    "        \"date_created\":\"2025-09-22\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3457f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple txt file\n",
    "\n",
    "import os\n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "581544da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5f72b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "### TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bbcf61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###DirectoryLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "##Load all the text file from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files\n",
    "    loader_cls= TextLoader, ##Loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b0fcdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 0}, page_content='Large \\nLanguage \\nModels\\nIntroduction to Large Language \\nModels'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 1}, page_content='Language models\\n•\\nRemember the simple n-gram language model\\n•\\nAssigns probabilities to sequences of words\\n•\\nGenerate text by sampling possible next words\\n•\\nIs trained on counts computed from lots of text\\n•\\nLarge language models are similar and different:\\n•\\nAssigns probabilities to sequences of words\\n•\\nGenerate text by sampling possible next words\\n•\\nAre trained by learning to guess the next word'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 2}, page_content='Large language models\\n•\\nEven through pretrained only to predict words\\n•\\nLearn a lot of useful language knowledge\\n•\\nSince training on a lot of text'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 3}, page_content='Three architectures for large language models\\nDecoders  \\n \\nEncoders  \\n   Encoder-decoders\\nGPT, Claude,  \\nBERT family,  \\nFlan-T5, Whisper\\nLlama  \\n \\n \\nHuBERT\\nMixtral'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 4}, page_content='Encoders\\nMany varieties!\\n• Popular: Masked Language Models (MLMs)\\n• BERT family\\n• Trained by predicting words from surrounding \\nwords on both sides\\n• Are usually finetuned (trained on supervised data) \\nfor classification tasks.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 5}, page_content='Encoder-Decoders\\n• Trained to map from one sequence to another\\n• Very popular for:\\n• machine translation (map from one language to \\nanother)\\n• speech recognition (map from acoustics to words)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 6}, page_content='Large \\nLanguage \\nModels\\nIntroduction to Large Language \\nModels'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 7}, page_content='Large \\nLanguage \\nModels\\nLarge Language Models: What \\ntasks can they do?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 8}, page_content='Big idea\\nMany tasks can be turned into tasks of \\npredicting words!'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 9}, page_content='This lecture: decoder-only models\\nAlso called:\\n• Causal LLMs\\n• Autoregressive LLMs\\n• Left-to-right LLMs\\n• Predict words left to right'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 10}, page_content='Conditional Generation: Generating text \\nconditioned on previous text!\\nPreﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand\\nthanks\\nfor\\nall\\nthe\\nthe\\n…\\nU\\nU\\nUnencoder layer\\nLanguage \\nModeling\\nHead\\nlogits\\nSo\\nE\\ni\\n+\\nE\\ni\\n+\\nE\\ni\\n+\\nE\\ni\\n+\\nE\\ni\\n+\\nE\\ni\\n+\\nE\\ni\\n+\\n…'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 11}, page_content='Many practical NLP tasks can be cast as word prediction!\\nSentiment analysis: “I like Jackie Chan”\\n1. We give the language model this string:\\nThe sentiment of the sentence \"I \\nlike Jackie Chan\" is: \\n2. And see what word it thinks comes next:\\nP(positive|The sentiment of the sentence ‘‘I like Jackie Chan\" is:)\\nP(negative|The sentiment of the sentence ‘‘I like Jackie Chan\" is:)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 12}, page_content='Framing lots of tasks as conditional generation\\nQA: “Who wrote The Origin of Species”\\n1.\\nWe give the language model this string:\\n2.\\nAnd see what word it thinks comes next:\\n3.\\nAnd iterate:\\nQ: Who wrote the book ‘‘The Origin of Species\"?\\nA:\\nP(w|Q: Who wrote the book ‘‘The Origin of Species\"?\\nA:)\\nP(w|Q: Who wrote the book ‘‘The Origin of Species\"?\\nA: Charles)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 13}, page_content='Summarization\\nThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff\\nand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says.\\nBut not if you live in New England or surrounding states. “We will not ship snow to any states\\nin the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging\\nsnow!”\\nHis website and social media accounts claim to have ﬁlled more than 133 orders for snow – more\\nthan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a\\nrecord this winter for the snowiest month in its history. Most residents see the huge piles of snow\\nchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.\\nAccording to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-\\neling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. He\\njoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.\\nOriginal\\nSummary'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 14}, page_content='LLMs for summarization (using  tl;dr)\\nOriginal Story\\nGenerated Summary\\n…\\nidea\\nKyle\\nwas\\nborn.\\nKyle\\nWaring\\nWaring\\nonly\\nThe\\n…\\nwill\\nDelimiter\\nwill\\nU\\nU\\nU\\ntl;dr\\nLM Head\\nE\\nE\\nE\\nE\\nE\\nE\\nE\\nE\\n…'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 15}, page_content='Large \\nLanguage \\nModels\\nLarge Language Models: What \\ntasks can they do?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 16}, page_content='Large \\nLanguage \\nModels\\nSampling for LLM Generation'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 17}, page_content='Decoding and Sampling\\nThis task of choosing a word to generate based on the model’s \\nprobabilities is called decoding. \\nThe most common method for decoding in LLMs: sampling. \\nSampling from a model’s distribution over words:\\n• choose random words according to their probability assigned \\nby the model. \\nAfter each token we’ll sample words to generate according \\nto their probability conditioned on our previous choices, \\n• A transformer language model will give the probability'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 18}, page_content='Random sampling\\ni 1\\nwi ⇠p(w)\\nwhile wi != EOS\\ni i + 1\\nwi ⇠p(wi | w<i)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 19}, page_content=\"Random sampling doesn't work very well\\nEven though random sampling mostly generate \\nsensible, high-probable words, \\nThere are many odd, low- probability words in the tail \\nof the distribution \\nEach one is low- probability but added up they \\nconstitute a large portion of the distribution \\nSo they get picked enough to generate weird \\nsentences\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 20}, page_content='Factors in word sampling: quality and diversity\\nEmphasize high-probability words \\n + quality: more  accurate, coherent, and factual, \\n- diversity: boring, repetitive. \\nEmphasize middle-probability words \\n+ diversity: more creative, diverse, \\n- quality: less factual, incoherent'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 21}, page_content='Top-k sampling:\\n1. Choose # of words k \\n2. For each word in the vocabulary V , use the language model to \\ncompute the likelihood of this word given the context p(wt |w<t ) \\n3. Sort the words by likelihood, keep only the top k most probable \\nwords. \\n4. Renormalize the scores of the k words to be a legitimate \\nprobability distribution. \\n5. Randomly sample a word from within these remaining k most-\\nprobable words according to its probability.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 22}, page_content='Top-p sampling (= nucleus sampling)\\nProblem with top-k:  k is fixed so may cover very different \\namounts of probability mass in different situations\\nIdea: Instead, keep the top p percent of the probability mass\\nGiven a distribution P(wt |w<t ), the top-p vocabulary V ( p) \\nis the smallest set of words such that \\nHoltzman et al., 2020 \\nX\\nw2V (p)\\nP(w|w<t) ≥p'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 23}, page_content='Temperature sampling\\nReshape the distribution instead of truncating it\\nIntuition from thermodynamics, \\n•\\na system at high temperature is flexible and can explore many \\npossible states,\\n•\\na system at lower temperature is likely to explore a subset of \\nlower energy (better) states.\\n In low-temperature sampling,  (τ ≤ 1) we smoothly\\n•\\nincrease the probability of the most probable words\\n•\\ndecrease the probability of the rare words.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 24}, page_content='Temperature sampling\\nDivide the logit by a temperature parameter τ before \\npassing it through the softmax.\\nInstead of\\nWe do  \\ny = softmax(u)\\ny = softmax(u/t)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 25}, page_content='Temperature sampling\\nWhy does this work?\\n• When τ is close to 1 the distribution doesn’t change much. \\n• The lower τ is, the larger the scores being passed to the softmax\\n• Softmax pushes high values toward 1 and low values toward 0. \\n• Large inputs pushes high-probability words higher and low probability \\nword lower,  making the distribution more greedy. \\n• As τ approaches 0, the probability of most likely word approaches 1 \\ny = softmax(u/t)\\n0 ≤ τ ≤ 1'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 26}, page_content='Large \\nLanguage \\nModels\\nSampling for LLM Generation'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 27}, page_content='Large \\nLanguage \\nModels\\nPretraining Large Language \\nModels: Algorithm'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 28}, page_content='Pretraining\\nThe big idea that underlies all the amazing \\nperformance of language models\\nFirst pretrain a transformer model on enormous \\namounts of text\\nThen apply it to new tasks.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 29}, page_content='Self-supervised training algorithm\\nWe just train them to predict the next word!\\n1. Take a corpus of text \\n2. At each time step t \\ni.\\nask the model to predict the next word \\nii. train the model using gradient descent to minimize the \\nerror in this prediction\\n\"Self-supervised\" because it just uses the next word as the \\nlabel!'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 30}, page_content='Intuition of language model training: loss\\n•\\nSame loss function: cross-entropy loss\\n•\\nWe want the model to assign a high probability to true \\nword w\\n•\\n= want loss to be high if the model assigns too low a \\nprobability to w\\n•\\nCE Loss: The negative log probability that the model \\nassigns to the true next word w\\n•\\nIf the model assigns too low a probability to w\\n•\\nWe move the model weights in the direction that assigns a \\nhigher probability to w'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 31}, page_content='Cross-entropy loss for language modeling\\nCE loss: difference between the correct probability distribution and the predicted \\ndistribution \\nThe correct distribution yt knows the next word, so is 1 for the actual next \\nword and 0 for the others.\\nSo in this sum, all terms get multiplied by zero except one: the logp the \\nmodel assigns to the correct next word, so:\\n \\nLCE = −\\nX\\nw2V\\nyt[w]log ˆyt[w]\\nLCE(ˆyt,yt) = −log ˆyt[wt+1]'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 32}, page_content='Teacher forcing\\n• At each token position t, model sees correct tokens w1:t, \\n•\\nComputes  loss (–log probability) for the next token wt+1 \\n• At next token position t+1 we ignore what model predicted \\nfor wt+1 \\n•\\nInstead we take the correct word wt+1, add it to context, move on'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 33}, page_content='Training a transformer language model\\nlong\\nand\\nthanks\\nfor\\nNext token\\nall\\nLoss\\n…\\n=\\nQ\\n=\">AB9nicbVDLSsNAFL2pr1pfURcu3AwWwY0lEakui25cVrAPaEqYTCbt0EkmzEzEvIrbkTcKPgZ/oJ/Y9Jm09YDA4dznD\\nvPV7MmdKW9WtU1tY3Nreq27Wd3b39A/PwqKtEIgntEMGF7HtYUc4i2tFMc9qPJcWhx2nPm9wXfu+ZSsVE9KSnMR2GeBSxgBGsc\\n8k1Ty4dLkZo6qZOiPVYhimO/CyruWbdalgzoFVil6QOJdqu+eP4giQhjThWKmBbcV6mGKpGeE0qzmJojEmEzyi6WztDJ3nko8\\nCIfMXaTRTF3I4VGoaenmy2E0te4X4nzdIdHA7TFkUJ5pGZD4oSDjSAhUdIJ9JSjSf5gQTyfINERljiYnOmypOt5cPXSXdq4bd\\nbDQfr+utu7KEKpzCGVyADTfQgdoQwcIZPAGn/BlvBivxrvxMY9WjPLPMSzA+P4DPEiSHA=</latexit>\\n−log yand\\nStacked\\nTransformer\\nBlocks\\nSo\\nlong\\nand\\nthanks\\nfor\\n…\\n…\\n…\\nU\\nInput tokens\\nx1\\nx2\\nLanguage\\nModeling\\nHead\\nx3\\nx4\\nx5\\nInput\\nEncoding\\nE\\n1\\n+\\nE\\n2\\n+\\nE\\n3\\n+\\nE\\n4\\n+\\nE\\n5\\n+\\n…\\n…\\n…\\n…\\n…\\nU\\nU\\nU\\nU\\n…\\nlogits\\nlogits\\nlogits\\nlogits\\nlogits\\n…\\n−log ythanks'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 34}, page_content='Large \\nLanguage \\nModels\\nPretraining Large Language \\nModels: Algorithm'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 35}, page_content='Large \\nLanguage \\nModels\\nPretraining data for LLMs'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 36}, page_content=\"LLMs are mainly trained on the web\\nCommon crawl, snapshots of the entire web produced by \\nthe non- profit Common Crawl with billions of pages\\nColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156 \\nbillion tokens of English,  filtered\\n What's in it? Mostly patent text documents, Wikipedia, and \\nnews sites\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 37}, page_content='The Pile: a pretraining corpus\\nweb\\nacademics\\nbooks\\ndialog'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 38}, page_content='Filtering for quality and safety\\nQuality is subjective\\n•\\nMany LLMs attempt to match Wikipedia, books, particular \\nwebsites\\n•\\nNeed to remove boilerplate, adult content\\n•\\nDeduplication at many levels (URLs, documents, even lines)\\nSafety also subjective\\n•\\nToxicity detection is important, although that has mixed results\\n•\\nCan mistakenly flag data written in dialects like African American \\nEnglish'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 39}, page_content='What does a model learn from pretraining?\\n• There are canines everywhere! One dog in the \\nfront room, and two dogs\\n• It wasn\\'t just big it was enormous\\n• The author of \"A Room of One\\'s Own\" is Virginia \\nWoolf\\n• The doctor told me that he\\n• The square root of 4 is 2'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 40}, page_content='Big idea\\nText contains enormous amounts of knowledge\\nPretraining on lots of text with all that \\nknowledge is what gives language models their \\nability to do so much'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 41}, page_content=\"But there are problems with scraping from the web\\nCopyright: much of the text in these datasets is copyrighted\\n•\\nNot clear if fair use doctrine in US allows for this use\\n•\\nThis remains an open legal question\\nData consent\\n•\\nWebsite owners can indicate they don't want their site crawled\\nPrivacy: \\n•\\nWebsites can contain private IP addresses and phone numbers\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 42}, page_content='Large \\nLanguage \\nModels\\nPretraining data for LLMs'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 43}, page_content='Large \\nLanguage \\nModels\\nFinetuning'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 44}, page_content=\"Finetuning for daptation to new domains\\nWhat happens if we need our LLM to work well on a domain \\nit didn't see in pretraining?\\nPerhaps some specific medical or legal domain?\\nOr maybe a multilingual LM needs to see more data on some \\nlanguage that was rare in pretraining?\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 45}, page_content='Finetuning\\nFine-\\ntuning \\nData\\nPretraining Data\\nPretraining\\n…\\n…\\n…\\nFine-tuning\\n…\\n…\\n…\\nPretrained LM\\nFine-tuned LM'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 46}, page_content='\"Finetuning\" means 4 different things\\nWe\\'ll discuss 1 here, and 3 in later lectures\\nIn all four cases, finetuning means:\\ntaking a pretrained model and further adapting \\nsome or all of its parameters to some new data'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 47}, page_content='1. Finetuning as \"continued pretraining\" on new data\\n• Further train all the parameters of model on new data\\n•\\nusing the same method (word prediction) and loss function \\n(cross-entropy loss) as for pretraining.\\n•\\nas if the new data were at the tail end of the pretraining data\\n• Hence sometimes called continued pretraining'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 48}, page_content='Large \\nLanguage \\nModels\\nFinetuning'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 49}, page_content='Large \\nLanguage \\nModels\\nEvaluating Large Language \\nModels'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 50}, page_content='Perplexity\\nJust as for n-gram grammars, we use perplexity to measure how \\nwell the LM predicts unseen text\\nThe perplexity of a model θ on an unseen test set is the inverse \\nprobability that θ assigns to the test set, normalized by the test \\nset length. \\nFor a test set of n tokens w1:n the perplexity is :\\nPerplexityq(w1:n) = Pq(w1:n)−1\\nn\\n=\\nn\\ns\\n1\\nPq(w1:n)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 51}, page_content='•\\nProbability depends on size of test set\\n•\\nProbability gets smaller the longer the text\\n•\\nBetter: a metric that is per-word, normalized by length\\n•\\nPerplexity is the inverse probability of the test set, normalized by \\nthe number of words\\n(The inverse comes from the original definition of perplexity from cross-\\nentropy rate in information theory)\\nProbability range is  [0,1], perplexity range is [1,∞]\\nWhy perplexity instead of raw probability of the test set?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 52}, page_content='Perplexity\\n•\\nThe higher the probability of the word sequence, the lower the \\nperplexity.\\n•\\nThus the lower the perplexity of a model on the data, the better the \\nmodel. \\n•\\nMinimizing perplexity is the same as maximizing probability\\nAlso: perplexity is sensitive to length/tokenization so best used when \\ncomparing LMs that use the same tokenizer.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 53}, page_content='Many other factors that we evaluate, like:\\nSize\\n Big models take lots of GPUs and time to train, memory to store\\nEnergy usage\\nCan measure kWh or kilograms of CO2 emitted \\nFairness\\nBenchmarks measure gendered and racial stereotypes, or decreased \\nperformance for language from or about some groups.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 54}, page_content='Large \\nLanguage \\nModels\\nDealing with Scale'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 55}, page_content='Scaling Laws\\nLLM performance depends on\\n• Model size: the number of parameters not counting \\nembeddings\\n• Dataset size: the amount of training data\\n• Compute: Amount of compute (in FLOPS or etc\\nCan improve a model by adding  parameters (more layers, \\nwider contexts), more data, or training for more iterations\\nThe performance of a large language model (the loss) scales \\nas a power-law with each of these three'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 56}, page_content='Scaling Laws\\nLoss L as a function of # parameters N, dataset size D, compute budget C (if other \\ntwo are held constant)\\nL(N) =\\n✓Nc\\nN\\n◆aN\\nL(D) =\\n✓Dc\\nD\\n◆aD\\nL(C) =\\n✓Cc\\nC\\n◆aC\\nScaling laws can be used early in training to predict what the loss would be if we were \\nto add more data or increase model size.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 57}, page_content='Number of non-embedding parameters N\\nN ⇡2 d nlayer(2 dattn +dff)\\n⇡12 nlayer d2\\n(assuming dattn = dff/4 = d)\\nThus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 × 96 × \\n122882 ≈ 175 billion parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 58}, page_content=\"KV Cache\\nIn training, we can compute attention very efficiently in parallel:\\nBut not at inference! We generate the next tokens one at a time!\\nFor a new token x, need to multiply by WQ, WK, and WV to get query, key, \\nvalues\\nBut don't want to recompute the key and value vectors for all the prior \\ntokens x<i\\nInstead, store key and value vectors in memory in the KV cache, and \\nthen we can just grab them from the cache \\nA = softmax\\n✓QK|\\npdk\\n◆\\nV\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 59}, page_content='KV Cache\\nq4\\nk1\\nk2\\nk4\\nQ\\nKT\\nQKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq4•k1 q4•k2 q4•k3 q4•k4\\nx\\n=\\n=\\nx\\na4\\nA\\n1 x dk\\ndk x N\\n1 x N\\nN x dv\\n1 x dv\\nk3\\nv1\\nv2\\nv3\\nv4\\nV\\nx\\nN x dv\\n=\\na1\\na2\\na3\\na4\\nA\\nN x dv\\nq1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ\\nKT\\nQKT\\nq1•k1\\nq2•k1 q2•k2\\nq4•k1 q4•k2 q4•k3 q4•k4\\nq3•k1 q3•k2 q3•k3\\nq1•k2\\nq2•k3\\nq1•k3\\nq3•k4\\nq2•k4\\nq1•k4\\nx\\n=\\nk\\nN x dk\\ndk x N\\nN x N'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 60}, page_content='Parameter-Efficient Finetuning\\nAdapting to a new domain by continued pretraining (finetuning) is a \\nproblem with huge LLMs.\\n• Enormous numbers of parameters to train \\n• Each pass of batch gradient descent has to backpropagate through \\nmany many huge layers. \\n• Expensive in processing power, in memory, and in time. \\nInstead, parameter-efficient fine tuning (PEFT)\\n• Efficiently select a subset of parameters to update when finetuning.\\n• E.g., freeze some of the parameters (don’t change them), \\n• And only update some a few parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 61}, page_content='LoRA (Low-Rank Adaptation)\\n•\\nTrransformers have many dense matrix multiply \\nlayers\\n•\\nLike WQ, WK, WV, WO layers in attention\\n•\\nInstead of updating these layers during finetuning, \\n•\\nFreeze these layers \\n•\\nUpdate a low-rank approximation with fewer \\nparameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 62}, page_content='LoRA\\n•\\nConsider a matrix W (shape [N × d])  that needs to be updated during finetuning \\nvia gradient descent. \\n•\\nNormally updates are ∆W  (shape [N × d])\\n•\\nIn LoRA, we freeze W and update instead a low-rank decomposition of W:\\n•\\nA of shape [N×r], \\n•\\nB of shape [r×d], r is very small  (like 1 or 2)\\n•\\nThat is, during  finetuning we update A and B instead of W. \\n•\\nReplace W + ∆W with W + BA. \\nForward pass: instead of \\n \\n \\n \\n \\nh = xW \\nWe do\\n \\n \\n \\n \\nh = xW + xAB'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 63}, page_content='LoRA\\nh\\nPretrained \\nWeights\\nW\\nd\\nk\\nr\\nk\\nA\\nB\\nr\\nx\\nd\\n1\\n1\\nk\\nd\\n×'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 64}, page_content='Large \\nLanguage \\nModels\\nDealing with Scale'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 65}, page_content='Large \\nLanguage \\nModels\\nHarms of Large Language \\nModels'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 66}, page_content='Hallucination'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 67}, page_content='Copyright'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 68}, page_content='Privacy'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 69}, page_content='Toxicity and Abuse'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 70}, page_content='Misinformation'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'file_path': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': \"D:20240820140818Z00'00'\", 'trapped': '', 'modDate': \"D:20240820140818Z00'00'\", 'creationDate': \"D:20240820140818Z00'00'\", 'page': 71}, page_content='Large \\nLanguage \\nModels\\nHarms of Large Language \\nModels')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###PDF Loader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "\n",
    "##Load all the text file from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/PDF\",\n",
    "    glob=\"**/*.PDF\", ## Pattern to match files\n",
    "    loader_cls= PyMuPDFLoader, ##Loader class to use\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "PDF_documents=dir_loader.load()\n",
    "PDF_documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YTRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
