{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b7e75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b53f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files to process\n",
      "\n",
      "Processing: LLM24aug.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 26 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 44 0 (offset 0)\n",
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 62 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 73 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 86 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 90 0 (offset 0)\n",
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "Ignoring wrong pointing object 112 0 (offset 0)\n",
      "Ignoring wrong pointing object 114 0 (offset 0)\n",
      "Ignoring wrong pointing object 116 0 (offset 0)\n",
      "Ignoring wrong pointing object 123 0 (offset 0)\n",
      "Ignoring wrong pointing object 158 0 (offset 0)\n",
      "Ignoring wrong pointing object 194 0 (offset 0)\n",
      "Ignoring wrong pointing object 204 0 (offset 0)\n",
      "Ignoring wrong pointing object 209 0 (offset 0)\n",
      "Ignoring wrong pointing object 244 0 (offset 0)\n",
      "Ignoring wrong pointing object 246 0 (offset 0)\n",
      "Ignoring wrong pointing object 248 0 (offset 0)\n",
      "Ignoring wrong pointing object 259 0 (offset 0)\n",
      "Ignoring wrong pointing object 276 0 (offset 0)\n",
      "Ignoring wrong pointing object 278 0 (offset 0)\n",
      "Ignoring wrong pointing object 280 0 (offset 0)\n",
      "Ignoring wrong pointing object 282 0 (offset 0)\n",
      "Ignoring wrong pointing object 288 0 (offset 0)\n",
      "Ignoring wrong pointing object 290 0 (offset 0)\n",
      "Ignoring wrong pointing object 292 0 (offset 0)\n",
      "Ignoring wrong pointing object 302 0 (offset 0)\n",
      "Ignoring wrong pointing object 304 0 (offset 0)\n",
      "Ignoring wrong pointing object 306 0 (offset 0)\n",
      "Ignoring wrong pointing object 312 0 (offset 0)\n",
      "Ignoring wrong pointing object 314 0 (offset 0)\n",
      "Ignoring wrong pointing object 329 0 (offset 0)\n",
      "Ignoring wrong pointing object 331 0 (offset 0)\n",
      "Ignoring wrong pointing object 333 0 (offset 0)\n",
      "Ignoring wrong pointing object 343 0 (offset 0)\n",
      "Ignoring wrong pointing object 345 0 (offset 0)\n",
      "Ignoring wrong pointing object 347 0 (offset 0)\n",
      "Ignoring wrong pointing object 369 0 (offset 0)\n",
      "Ignoring wrong pointing object 379 0 (offset 0)\n",
      "Ignoring wrong pointing object 381 0 (offset 0)\n",
      "Ignoring wrong pointing object 385 0 (offset 0)\n",
      "Ignoring wrong pointing object 390 0 (offset 0)\n",
      "Ignoring wrong pointing object 392 0 (offset 0)\n",
      "Ignoring wrong pointing object 397 0 (offset 0)\n",
      "Ignoring wrong pointing object 402 0 (offset 0)\n",
      "Ignoring wrong pointing object 404 0 (offset 0)\n",
      "Ignoring wrong pointing object 412 0 (offset 0)\n",
      "Ignoring wrong pointing object 414 0 (offset 0)\n",
      "Ignoring wrong pointing object 416 0 (offset 0)\n",
      "Ignoring wrong pointing object 418 0 (offset 0)\n",
      "Ignoring wrong pointing object 429 0 (offset 0)\n",
      "Ignoring wrong pointing object 440 0 (offset 0)\n",
      "Ignoring wrong pointing object 487 0 (offset 0)\n",
      "Ignoring wrong pointing object 574 0 (offset 0)\n",
      "Ignoring wrong pointing object 576 0 (offset 0)\n",
      "Ignoring wrong pointing object 578 0 (offset 0)\n",
      "Ignoring wrong pointing object 580 0 (offset 0)\n",
      "Ignoring wrong pointing object 582 0 (offset 0)\n",
      "Ignoring wrong pointing object 586 0 (offset 0)\n",
      "Ignoring wrong pointing object 590 0 (offset 0)\n",
      "Ignoring wrong pointing object 592 0 (offset 0)\n",
      "Ignoring wrong pointing object 598 0 (offset 0)\n",
      "Ignoring wrong pointing object 621 0 (offset 0)\n",
      "Ignoring wrong pointing object 627 0 (offset 0)\n",
      "Ignoring wrong pointing object 629 0 (offset 0)\n",
      "Ignoring wrong pointing object 631 0 (offset 0)\n",
      "Ignoring wrong pointing object 642 0 (offset 0)\n",
      "Ignoring wrong pointing object 644 0 (offset 0)\n",
      "Ignoring wrong pointing object 646 0 (offset 0)\n",
      "Ignoring wrong pointing object 649 0 (offset 0)\n",
      "Ignoring wrong pointing object 659 0 (offset 0)\n",
      "Ignoring wrong pointing object 661 0 (offset 0)\n",
      "Ignoring wrong pointing object 663 0 (offset 0)\n",
      "Ignoring wrong pointing object 669 0 (offset 0)\n",
      "Ignoring wrong pointing object 677 0 (offset 0)\n",
      "Ignoring wrong pointing object 681 0 (offset 0)\n",
      "Ignoring wrong pointing object 683 0 (offset 0)\n",
      "Ignoring wrong pointing object 687 0 (offset 0)\n",
      "Ignoring wrong pointing object 689 0 (offset 0)\n",
      "Ignoring wrong pointing object 693 0 (offset 0)\n",
      "Ignoring wrong pointing object 695 0 (offset 0)\n",
      "Ignoring wrong pointing object 705 0 (offset 0)\n",
      "Ignoring wrong pointing object 711 0 (offset 0)\n",
      "Ignoring wrong pointing object 713 0 (offset 0)\n",
      "Ignoring wrong pointing object 715 0 (offset 0)\n",
      "Ignoring wrong pointing object 717 0 (offset 0)\n",
      "Ignoring wrong pointing object 719 0 (offset 0)\n",
      "Ignoring wrong pointing object 727 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Loaded 72 pages\n",
      "\n",
      "Total documents loaded: 72\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79ccc305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 0, 'page_label': '1', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsIntroduction to Large Language Models'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 1, 'page_label': '2', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Language models•Remember the simple n-gram language model•Assigns probabilities to sequences of words•Generate text by sampling possible next words•Is trained on counts computed from lots of text•Large language models are similar and different:•Assigns probabilities to sequences of words•Generate text by sampling possible next words•Are trained by learning to guess the next word'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 2, 'page_label': '3', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large language models•Even through pretrained only to predict words•Learn a lot of useful language knowledge•Since training on a lot of text'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 3, 'page_label': '4', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Three architectures for large language models\\nDecoders   Encoders     Encoder-decodersGPT, Claude,  BERT family,  Flan-T5, WhisperLlama    HuBERTMixtral\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 4, 'page_label': '5', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='EncodersMany varieties!•Popular: Masked Language Models (MLMs)•BERT family•Trained by predicting words from surrounding words on both sides•Are usually finetuned (trained on supervised data) for classification tasks.\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 5, 'page_label': '6', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Encoder-Decoders\\n•Trained to map from one sequence to another•Very popular for:•machine translation (map from one language to another)•speech recognition (map from acoustics to words)\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 6, 'page_label': '7', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsIntroduction to Large Language Models'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 7, 'page_label': '8', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsLarge Language Models: What tasks can they do?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 8, 'page_label': '9', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Big ideaMany tasks can be turned into tasks of predicting words!'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 9, 'page_label': '10', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='This lecture: decoder-only modelsAlso called:•Causal LLMs•Autoregressive LLMs•Left-to-right LLMs•Predict words left to right\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 10, 'page_label': '11', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Conditional Generation: Generating text conditioned on previous text!\\nPreﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead logits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 11, 'page_label': '12', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Many practical NLP tasks can be cast as word prediction!Sentiment analysis: “I like Jackie Chan”1.We give the language model this string:The sentiment of the sentence \"I like Jackie Chan\" is: 2.And see what word it thinks comes next:\\n10.1 • L ARGE L ANGUAGE M ODELS WITH T RANSFORMERS 3\\nPreﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead logits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…\\nFigure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language\\nmodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.\\nword “negative” to see which is higher:\\nP ( positive | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nP ( negative | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is\\npositive, otherwise we say the sentiment is negative.\\nWe can also cast more complex tasks as word prediction. Consider question\\nanswering, in which the system is given a question (for example a question with\\na simple factual answer) and must give a textual answer; we introduce this task in\\ndetail in Chapter 15. We can cast the task of question answering as word prediction\\nby giving a language model a question and a token like A: suggesting that an answer\\nshould come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute the probability distribution over possible\\nnext words given this preﬁx:\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: )\\nand look at which words w have high probabilities, we might expect to see that\\nCharles is very likely, and then if we choose Charles and continue and ask\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles )\\nwe might now see that Darwin is the most probable token, and select it.\\nConditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext\\nsummarization\\ntext, such as a full-length article, and produce an effective shorter summary of it. We\\ncan cast summarization as language modeling by giving a large language model a\\ntext, and follow the text by a token like tl;dr ; this token is short for something like'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Framing lots of tasks as conditional generationQA: “Who wrote The Origin of Species”1.We give the language model this string:2.And see what word it thinks comes next:\\n3.And iterate:\\n20 C HAPTER 10 • T RANSFORMERS AND L ARGE L ANGUAGE M ODELS\\nPreﬁx Text\\nCompletion Text\\nInput\\nEmbeddings\\nTransformer\\nBlocks\\nSample from Softmax\\nSo long\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nlinear layer\\nFigure 10.15 Autoregressive text completion with transformer-based large language models.\\nword “negative” to see which is higher:\\nP ( positive | The sentiment of the sentence “I like Jackie Chan” is: )\\nP ( negative | The sentiment of the sentence “I like Jackie Chan” is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is\\npositive, otherwise we say the sentiment is negative.\\nWe can also cast more complex tasks as word prediction. Consider the task\\nof answering simple questions, a task we return to in Chapter 14. In this task the\\nsystem is given some question and must give a textual answer. We can cast the task\\nof question answering as word prediction by giving a language model a question and\\na token like A: suggesting that an answer should come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute\\nP ( w | Q: Who wrote the book “The Origin of Species”? A: )\\nand look at which words w have high probabilities, we might expect to see that\\nCharles is very likely, and then if we choose Charles and continue and ask\\nP ( w | Q: Who wrote the book “The Origin of Species”? A: Charles )\\nwe might now see that Darwin is the most probable word, and select it.\\nConditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext\\nsummarization\\ntext, such as a full-length article, and produce an effective shorter summary of it.\\nWe can cast summarization as language modeling by giving a large language model\\na text, and follow the text by a token like tl;dr ; this token is short for something\\nlike ‘too long; don’t read’ and in recent years people often use this token, especially\\nin informal work emails, when they are going to give a short summary. We can\\nthen do conditional generation: give the language model this preﬁx, and then ask\\n10.1 • L ARGE L ANGUAGE M ODELS WITH T RANSFORMERS 3\\nPreﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead\\nlogits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…\\nFigure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language\\nmodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.\\nword “negative” to see which is higher:\\nP ( positive | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nP ( negative | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is\\npositive, otherwise we say the sentiment is negative.\\nWe can also cast more complex tasks as word prediction. Consider question\\nanswering, in which the system is given a question (for example a question with\\na simple factual answer) and must give a textual answer; we introduce this task in\\ndetail in Chapter 15. We can cast the task of question answering as word prediction\\nby giving a language model a question and a token like A: suggesting that an answer\\nshould come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute the probability distribution over possible\\nnext words given this preﬁx:\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: )\\nand look at which words w have high probabilities, we might expect to see that\\nCharles is very likely, and then if we choose Charles and continue and ask\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles )\\nwe might now see that Darwin is the most probable token, and select it.\\nConditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext\\nsummarization\\ntext, such as a full-length article, and produce an effective shorter summary of it. We\\ncan cast summarization as language modeling by giving a large language model a\\ntext, and follow the text by a token like tl;dr ; this token is short for something like\\n10.1 • L ARGE L ANGUAGE M ODELS WITH T RANSFORMERS 3\\nPreﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead logits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…\\nFigure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language\\nmodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.\\nword “negative” to see which is higher:\\nP ( positive | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nP ( negative | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is\\npositive, otherwise we say the sentiment is negative.\\nWe can also cast more complex tasks as word prediction. Consider question\\nanswering, in which the system is given a question (for example a question with\\na simple factual answer) and must give a textual answer; we introduce this task in\\ndetail in Chapter 15. We can cast the task of question answering as word prediction\\nby giving a language model a question and a token like A: suggesting that an answer\\nshould come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute the probability distribution over possible\\nnext words given this preﬁx:\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: )\\nand look at which words w have high probabilities, we might expect to see that\\nCharles is very likely, and then if we choose Charles and continue and ask\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles )\\nwe might now see that Darwin is the most probable token, and select it.\\nConditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext\\nsummarization\\ntext, such as a full-length article, and produce an effective shorter summary of it. We\\ncan cast summarization as language modeling by giving a large language model a\\ntext, and follow the text by a token like tl;dr ; this token is short for something like'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Summarization\\n4 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\n‘too long; didn’t read’ and in recent years people often use this token, especially in\\ninformal work emails, when they are going to give a short summary. Since this token\\nis sufﬁciently frequent in language model training data, language models have seen\\nmany texts in which the token occurs before a summary, and hence will interpret the\\ntoken as instructions to generate a summary. We can then do conditional generation:\\ngive the language model this preﬁx, and then have it generate the following words,\\none by one, and take the entire response as a summary. Fig. 10.2 shows an example\\nof a text and a human-produced summary from a widely-used summarization corpus\\nconsisting of CNN and Daily Mirror news articles.\\nOriginal Article\\nThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff\\nand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says.\\nBut not if you live in New England or surrounding states. “We will not ship snow to any states\\nin the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging\\nsnow!”\\nHis website and social media accounts claim to have ﬁlled more than 133 orders for snow – more\\nthan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a\\nrecord this winter for the snowiest month in its history. Most residents see the huge piles of snow\\nchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.\\nAccording to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-\\neling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. He\\njoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]\\nSummary\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.\\nFigure 10.2 Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus\\n( Hermann et al. , 2015 ), ( Nallapati et al. , 2016 ).\\nIf we take this full article and append the token tl;dr , we can use this as the con-\\ntext to prime the generation process to produce a summary as illustrated in Fig. 10.3 .\\nAgain, what makes transformers able to succeed at this task (as compared, say, to\\nthe primitive n-gram language model) is that attention can incorporate information\\nfrom the large context window, giving the model access to the original article as well\\nas to the newly generated text throughout the process.\\nWhich words do we generate at each step? One simple way to generate words\\nis to always generate the most likely word given the context. Generating the most\\nlikely word given the context is called greedy decoding . A greedy algorithm is one\\ngreedy\\ndecoding\\nthat make a choice that is locally optimal, whether or not it will turn out to have\\nbeen the best choice with hindsight. Thus in greedy decoding, at each time step in\\ngeneration, the output y t is chosen by computing the probability for each possible\\noutput (every word in the vocabulary) and then choosing the highest probability\\nword (the argmax):\\nˆw t = argmax w 2 V P ( w | w < t ) (10.1)\\nIn practice, however, we don’t use greedy decoding with large language models.\\nA major problem with greedy decoding is that because the words it chooses are (by\\ndeﬁnition) extremely predictable, the resulting text is generic and often quite repeti-\\ntive. Indeed, greedy decoding is so predictable that it is deterministic; if the context\\n4 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\n‘too long; didn’t read’ and in recent years people often use this token, especially in\\ninformal work emails, when they are going to give a short summary. Since this token\\nis sufﬁciently frequent in language model training data, language models have seen\\nmany texts in which the token occurs before a summary, and hence will interpret the\\ntoken as instructions to generate a summary. We can then do conditional generation:\\ngive the language model this preﬁx, and then have it generate the following words,\\none by one, and take the entire response as a summary. Fig. 10.2 shows an example\\nof a text and a human-produced summary from a widely-used summarization corpus\\nconsisting of CNN and Daily Mirror news articles.\\nOriginal Article\\nThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff\\nand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says.\\nBut not if you live in New England or surrounding states. “We will not ship snow to any states\\nin the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging\\nsnow!”\\nHis website and social media accounts claim to have ﬁlled more than 133 orders for snow – more\\nthan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a\\nrecord this winter for the snowiest month in its history. Most residents see the huge piles of snow\\nchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.\\nAccording to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-\\neling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. He\\njoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]\\nSummary\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.\\nFigure 10.2 Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus\\n( Hermann et al. , 2015 ), ( Nallapati et al. , 2016 ).\\nIf we take this full article and append the token tl;dr , we can use this as the con-\\ntext to prime the generation process to produce a summary as illustrated in Fig. 10.3 .\\nAgain, what makes transformers able to succeed at this task (as compared, say, to\\nthe primitive n-gram language model) is that attention can incorporate information\\nfrom the large context window, giving the model access to the original article as well\\nas to the newly generated text throughout the process.\\nWhich words do we generate at each step? One simple way to generate words\\nis to always generate the most likely word given the context. Generating the most\\nlikely word given the context is called greedy decoding . A greedy algorithm is one\\ngreedy\\ndecoding\\nthat make a choice that is locally optimal, whether or not it will turn out to have\\nbeen the best choice with hindsight. Thus in greedy decoding, at each time step in\\ngeneration, the output y t is chosen by computing the probability for each possible\\noutput (every word in the vocabulary) and then choosing the highest probability\\nword (the argmax):\\nˆw t = argmax w 2 V\\nP ( w | w < t ) (10.1)\\nIn practice, however, we don’t use greedy decoding with large language models.\\nA major problem with greedy decoding is that because the words it chooses are (by\\ndeﬁnition) extremely predictable, the resulting text is generic and often quite repeti-\\ntive. Indeed, greedy decoding is so predictable that it is deterministic; if the context\\nOriginal\\nSummary'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 14, 'page_label': '15', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='LLMs for summarization (using  tl;dr)\\nOriginal Story\\nGenerated Summary\\n… idea\\nKyle\\nwas born. Kyle\\nWaring\\nWaringonlyThe\\n…\\nwill\\nDelimiter\\nwill\\nU U U\\ntl;dr\\nLM Head\\nE\\n E\\n E\\n E\\n E\\n E\\n E\\n E\\n…'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 15, 'page_label': '16', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsLarge Language Models: What tasks can they do?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 16, 'page_label': '17', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsSampling for LLM Generation'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 17, 'page_label': '18', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Decoding and SamplingThis task of choosing a word to generate based on the model’s probabilities is called decoding. The most common method for decoding in LLMs: sampling. Sampling from a model’s distribution over words:•choose random words according to their probability assigned by the model. After each token we’ll sample words to generate according to their probability conditioned on our previous choices, •A transformer language model will give the probability'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 18, 'page_label': '19', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Random sampling\\n6 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\nas deﬁned by the model. Thus we are more likely to generate words that the model\\nthinks have a high probability in the context and less likely to generate words that\\nthe model thinks have a low probability.\\nWe saw back in Chapter 3 on page ?? how to generate text from a unigram lan-\\nguage model , by repeatedly randomly sampling words according to their probability\\nuntil we either reach a pre-determined length or select the end-of-sentence token. To\\ngenerate text from a trained transformer language model we’ll just generalize this\\nmodel a bit: at each step we’ll sample words according to their probability condi-\\ntioned on our previous choices , and we’ll use a transformer language model as the\\nprobability model that tells us this probability.\\nWe can formalize this algorithm for generating a sequence of words W = w 1\\n, w 2\\n,..., w N\\nuntil we hit the end-of-sequence token, using x ⇠ p ( x ) to mean ‘choose x by sam-\\npling from the distribution p ( x ) :\\ni  1\\nw i\\n⇠ p( w )\\nwhile w i\\n!= EOS\\ni  i+1\\nw i\\n⇠ p( w i\\n| w < i\\n)\\nThe algorithm above is called random sampling , and it turns out random sam-\\nrandom\\nsampling\\npling doesn’t work well enough. The problem is that even though random sampling\\nis mostly going to generate sensible, high-probable words, there are many odd, low-\\nprobability words in the tail of the distribution, and even though each one is low-\\nprobability, if you add up all the rare words, they constitute a large enough portion\\nof the distribution that they get chosen often enough to result in generating weird\\nsentences. For this reason, instead of random sampling, we usually use sampling\\nmethods that avoid generating the very unlikely words.\\nThe sampling methods we introduce below each have parameters that enable\\ntrading off two important factors in generation: quality and diversity . Methods\\nthat emphasize the most probable words tend to produce generations that are rated\\nby people as more accurate, more coherent, and more factual, but also more boring\\nand more repetitive. Methods that give a bit more weight to the middle-probability\\nwords tend to be more creative and more diverse, but less factual and more likely to\\nbe incoherent or otherwise low-quality.\\n10.2.1 Top- k sampling\\nTop-k sampling is a simple generalization of greedy decoding. Instead of choosingtop-k sampling\\nthe single most probable word to generate, we ﬁrst truncate the distribution to the\\ntop k most likely words, renormalize to produce a legitimate probability distribution,\\nand then randomly sample from within these k words according to their renormalized\\nprobabilities. More formally:\\n1. Choose in advance a number of words k\\n2. For each word in the vocabulary V , use the language model to compute the\\nlikelihood of this word given the context p ( w t\\n| w < t\\n)\\n3. Sort the words by their likelihood, and throw away any word that is not one of\\nthe top k most probable words.\\n4. Renormalize the scores of the k words to be a legitimate probability distribu-\\ntion.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 19, 'page_label': '20', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"Random sampling doesn't work very wellEven though random sampling mostly generate sensible, high-probable words, There are many odd, low- probability words in the tail of the distribution Each one is low- probability but added up they constitute a large portion of the distribution So they get picked enough to generate weird sentences\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 20, 'page_label': '21', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Factors in word sampling: quality and diversityEmphasize high-probability words  + quality: more  accurate, coherent, and factual, - diversity: boring, repetitive. Emphasize middle-probability words + diversity: more creative, diverse, - quality: less factual, incoherent'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 21, 'page_label': '22', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Top-k sampling:1. Choose # of words k 2. For each word in the vocabulary V , use the language model to compute the likelihood of this word given the context p(wt |w<t ) 3. Sort the words by likelihood, keep only the top k most probable words. 4. Renormalize the scores of the k words to be a legitimate probability distribution. 5. Randomly sample a word from within these remaining k most-probable words according to its probability.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 22, 'page_label': '23', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Top-p sampling (= nucleus sampling)Problem with top-k:  k is fixed so may cover very different amounts of probability mass in different situationsIdea: Instead, keep the top p percent of the probability massGiven a distribution P(wt |w<t ), the top-p vocabulary V ( p) is the smallest set of words such that \\nHoltzman et al., 2020 \\n10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.\\nWhen k = 1, top- k sampling is identical to greedy decoding. Setting k to a larger\\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\\nprobable, but is still probable enough, and whose choice results in generating more\\ndiverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes\\nthe top 10 words will be very likely and include most of the probability mass, but\\nother times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling\\n2020 ), is to keep not the top k words, but the top p percent of the probability mass.\\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\\nBut by measuring probability rather than the number of words, the hope is that the\\nmeasure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w t\\n| w < t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w < t\\n) \\x00 p . (10.2)\\n10.2.3 Temperature sampling\\nIn temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,\\nwhile a system at a lower temperature is likely to explore a subset of lower energy\\n(better) states. In low-temperature sampling, we smoothly increase the probability\\nof the most probable words and decrease the probability of the rare words.\\nWe implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the\\nvocabulary directly from the logit as in the following (repeated from ( ?? )):\\ny = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.\\nBut the lower t is, the larger the scores being passed to the softmax (dividing by a\\nsmaller fraction t \\uf8ff 1 results in making each score larger). Recall that one of the\\nuseful properties of a softmax is that it tends to push high values toward 1 and low\\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more\\ngreedy. As t approaches 0 the probability of the most likely word approaches 1.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 23, 'page_label': '24', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Temperature samplingReshape the distribution instead of truncating itIntuition from thermodynamics, •a system at high temperature is flexible and can explore many possible states,•a system at lower temperature is likely to explore a subset of lower energy (better) states. In low-temperature sampling,  (τ ≤ 1) we smoothly•increase the probability of the most probable words•decrease the probability of the rare words.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Temperature samplingDivide the logit by a temperature parameter τ before passing it through the softmax.Instead ofWe do  \\n10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.\\nWhen k = 1, top- k sampling is identical to greedy decoding. Setting k to a larger\\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\\nprobable, but is still probable enough, and whose choice results in generating more\\ndiverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes\\nthe top 10 words will be very likely and include most of the probability mass, but\\nother times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling\\n2020 ), is to keep not the top k words, but the top p percent of the probability mass.\\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\\nBut by measuring probability rather than the number of words, the hope is that the\\nmeasure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w\\nt\\n| w\\n< t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n) \\x00 p . (10.2)\\n10.2.3 Temperature sampling\\nIn temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,\\nwhile a system at a lower temperature is likely to explore a subset of lower energy\\n(better) states. In low-temperature sampling, we smoothly increase the probability\\nof the most probable words and decrease the probability of the rare words.\\nWe implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the\\nvocabulary directly from the logit as in the following (repeated from ( ?? )):\\ny = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.\\nBut the lower t is, the larger the scores being passed to the softmax (dividing by a\\nsmaller fraction t \\uf8ff 1 results in making each score larger). Recall that one of the\\nuseful properties of a softmax is that it tends to push high values toward 1 and low\\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more\\ngreedy. As t approaches 0 the probability of the most likely word approaches 1.\\n10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.\\nWhen k = 1, top- k sampling is identical to greedy decoding. Setting k to a larger\\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\\nprobable, but is still probable enough, and whose choice results in generating more\\ndiverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes\\nthe top 10 words will be very likely and include most of the probability mass, but\\nother times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling\\n2020 ), is to keep not the top k words, but the top p percent of the probability mass.\\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\\nBut by measuring probability rather than the number of words, the hope is that the\\nmeasure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w\\nt\\n| w\\n< t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n) \\x00 p . (10.2)\\n10.2.3 Temperature sampling\\nIn temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,\\nwhile a system at a lower temperature is likely to explore a subset of lower energy\\n(better) states. In low-temperature sampling, we smoothly increase the probability\\nof the most probable words and decrease the probability of the rare words.\\nWe implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the\\nvocabulary directly from the logit as in the following (repeated from ( ?? )):\\ny = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.\\nBut the lower t is, the larger the scores being passed to the softmax (dividing by a\\nsmaller fraction t \\uf8ff 1 results in making each score larger). Recall that one of the\\nuseful properties of a softmax is that it tends to push high values toward 1 and low\\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more\\ngreedy. As t approaches 0 the probability of the most likely word approaches 1.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 25, 'page_label': '26', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Temperature sampling\\nWhy does this work?•When τ is close to 1 the distribution doesn’t change much. •The lower τ is, the larger the scores being passed to the softmax•Softmax pushes high values toward 1 and low values toward 0. •Large inputs pushes high-probability words higher and low probability word lower,  making the distribution more greedy. •As τ approaches 0, the probability of most likely word approaches 1 \\n10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.\\nWhen k = 1, top- k sampling is identical to greedy decoding. Setting k to a larger\\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\\nprobable, but is still probable enough, and whose choice results in generating more\\ndiverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes\\nthe top 10 words will be very likely and include most of the probability mass, but\\nother times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling\\n2020 ), is to keep not the top k words, but the top p percent of the probability mass.\\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\\nBut by measuring probability rather than the number of words, the hope is that the\\nmeasure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w\\nt\\n| w\\n< t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n) \\x00 p . (10.2)\\n10.2.3 Temperature sampling\\nIn temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,\\nwhile a system at a lower temperature is likely to explore a subset of lower energy\\n(better) states. In low-temperature sampling, we smoothly increase the probability\\nof the most probable words and decrease the probability of the rare words.\\nWe implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the\\nvocabulary directly from the logit as in the following (repeated from ( ?? )):\\ny = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.\\nBut the lower t is, the larger the scores being passed to the softmax (dividing by a\\nsmaller fraction t \\uf8ff 1 results in making each score larger). Recall that one of the\\nuseful properties of a softmax is that it tends to push high values toward 1 and low\\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more\\ngreedy. As t approaches 0 the probability of the most likely word approaches 1.\\n0 ≤ τ ≤ 1'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 26, 'page_label': '27', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsSampling for LLM Generation'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 27, 'page_label': '28', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsPretraining Large Language Models: Algorithm'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 28, 'page_label': '29', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='PretrainingThe big idea that underlies all the amazing performance of language modelsFirst pretrain a transformer model on enormous amounts of textThen apply it to new tasks.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 29, 'page_label': '30', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Self-supervised training algorithmWe just train them to predict the next word!1.Take a corpus of text 2.At each time step t i.ask the model to predict the next word ii.train the model using gradient descent to minimize the error in this prediction\"Self-supervised\" because it just uses the next word as the label!'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 30, 'page_label': '31', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Intuition of language model training: loss•Same loss function: cross-entropy loss•We want the model to assign a high probability to true word w•= want loss to be high if the model assigns too low a probability to w•CE Loss: The negative log probability that the model assigns to the true next word w•If the model assigns too low a probability to w•We move the model weights in the direction that assigns a higher probability to w'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Cross-entropy loss for language modelingCE loss: difference between the correct probability distribution and the predicted distribution \\nThe correct distribution yt knows the next word, so is 1 for the actual next word and 0 for the others.So in this sum, all terms get multiplied by zero except one: the logp the model assigns to the correct next word, so: \\n8 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\nNote, by the way, that there can be other situations where we may want to do\\nsomething quite different and ﬂatten the word probability distribution instead of\\nmaking it greedy. Temperature sampling can help with this situation too, in this case\\nhigh-temperature sampling, in which case we use t > 1.\\n10.3 Pretraining Large Language Models\\nHow do we teach a transformer to be a language model? What is the algorithm and\\nwhat data do we train on?\\n10.3.1 Self-supervised training algorithm\\nTo train a transformer as a language model, we use the same self-supervision (orself-supervision\\nself-training ) algorithm we saw in Section ?? : we take a corpus of text as training\\nmaterial and at each time step t ask the model to predict the next word. We call\\nsuch a model self-supervised because we don’t have to add any special gold labels\\nto the data; the natural sequence of words is its own supervision! We simply train the\\nmodel to minimize the error in predicting the true next word in the training sequence,\\nusing cross-entropy as the loss function.\\nRecall that the cross-entropy loss measures the difference between a predicted\\nprobability distribution and the correct distribution.\\nL CE\\n= \\x00\\nX\\nw 2 V\\ny t [ w ] log ˆy t [ w ] (10.5)\\nIn the case of language modeling, the correct distribution y t comes from knowing the\\nnext word. This is represented as a one-hot vector corresponding to the vocabulary\\nwhere the entry for the actual next word is 1, and all the other entries are 0. Thus,\\nthe cross-entropy loss for language modeling is determined by the probability the\\nmodel assigns to the correct next word (all other words get multiplied by zero). So\\nat time t the CE loss in ( 10.5 ) can be simpliﬁed as the negative log probability the\\nmodel assigns to the next word in the training sequence.\\nL CE\\n( ˆy t , y t )= \\x00 log ˆy t [ w t + 1\\n] (10.6)\\nThus at each word position t of the input, the model takes as input the correct se-\\nquence of tokens w 1: t\\n, and uses them to compute a probability distribution over\\npossible next words so as to compute the model’s loss for the next token w t + 1\\n. Then\\nwe move to the next word, we ignore what the model predicted for the next word\\nand instead use the correct sequence of tokens w 1: t + 1\\nto estimate the probability of\\ntoken w t + 2\\n. This idea that we always give the model the correct history sequence to\\npredict the next word (rather than feeding the model its best case from the previous\\ntime step) is called teacher forcing .teacher forcing\\nFig. 10.4 illustrates the general training approach. At each step, given all the\\npreceding words, the ﬁnal transformer layer produces an output distribution over\\nthe entire vocabulary. During training, the probability assigned to the correct word\\nis used to calculate the cross-entropy loss for each item in the sequence. The loss\\nfor a training sequence is the average cross-entropy loss over the entire sequence.\\nThe weights in the network are adjusted to minimize the average CE loss over the\\ntraining sequence via gradient descent.\\n8 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\nNote, by the way, that there can be other situations where we may want to do\\nsomething quite different and ﬂatten the word probability distribution instead of\\nmaking it greedy. Temperature sampling can help with this situation too, in this case\\nhigh-temperature sampling, in which case we use t > 1.\\n10.3 Pretraining Large Language Models\\nHow do we teach a transformer to be a language model? What is the algorithm and\\nwhat data do we train on?\\n10.3.1 Self-supervised training algorithm\\nTo train a transformer as a language model, we use the same self-supervision (orself-supervision\\nself-training ) algorithm we saw in Section ?? : we take a corpus of text as training\\nmaterial and at each time step t ask the model to predict the next word. We call\\nsuch a model self-supervised because we don’t have to add any special gold labels\\nto the data; the natural sequence of words is its own supervision! We simply train the\\nmodel to minimize the error in predicting the true next word in the training sequence,\\nusing cross-entropy as the loss function.\\nRecall that the cross-entropy loss measures the difference between a predicted\\nprobability distribution and the correct distribution.\\nL CE\\n= \\x00\\nX\\nw 2 V\\ny t\\n[ w ] log ˆy t\\n[ w ] (10.5)\\nIn the case of language modeling, the correct distribution y t\\ncomes from knowing the\\nnext word. This is represented as a one-hot vector corresponding to the vocabulary\\nwhere the entry for the actual next word is 1, and all the other entries are 0. Thus,\\nthe cross-entropy loss for language modeling is determined by the probability the\\nmodel assigns to the correct next word (all other words get multiplied by zero). So\\nat time t the CE loss in ( 10.5 ) can be simpliﬁed as the negative log probability the\\nmodel assigns to the next word in the training sequence.\\nL CE\\n( ˆy t\\n, y t\\n)= \\x00 log ˆy t\\n[ w t + 1\\n] (10.6)\\nThus at each word position t of the input, the model takes as input the correct se-\\nquence of tokens w 1: t\\n, and uses them to compute a probability distribution over\\npossible next words so as to compute the model’s loss for the next token w t + 1\\n. Then\\nwe move to the next word, we ignore what the model predicted for the next word\\nand instead use the correct sequence of tokens w 1: t + 1\\nto estimate the probability of\\ntoken w t + 2\\n. This idea that we always give the model the correct history sequence to\\npredict the next word (rather than feeding the model its best case from the previous\\ntime step) is called teacher forcing .teacher forcing\\nFig. 10.4 illustrates the general training approach. At each step, given all the\\npreceding words, the ﬁnal transformer layer produces an output distribution over\\nthe entire vocabulary. During training, the probability assigned to the correct word\\nis used to calculate the cross-entropy loss for each item in the sequence. The loss\\nfor a training sequence is the average cross-entropy loss over the entire sequence.\\nThe weights in the network are adjusted to minimize the average CE loss over the\\ntraining sequence via gradient descent.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 32, 'page_label': '33', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Teacher forcing•At each token position t, model sees correct tokens w1:t, •Computes  loss (–log probability) for the next token wt+1 •At next token position t+1 we ignore what model predicted for wt+1 •Instead we take the correct word wt+1, add it to context, move on'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 33, 'page_label': '34', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Training a transformer language model\\nlong and thanks forNext token all\\nLoss\\n…\\n=\\n<latexit sha1_base64=\"AovqpaL476UmJ1EU1xZPgDZ70tQ=\">AAAB9nicbVDLSsNAFL2pr1pfURcu3AwWwY0lEakui25cVrAPaEqYTCbt0EkmzEzEEvIrbkTcKPgZ/oJ/Y9Jm09YDA4dzznDvPV7MmdKW9WtU1tY3Nreq27Wd3b39A/PwqKtEIgntEMGF7HtYUc4i2tFMc9qPJcWhx2nPm9wXfu+ZSsVE9KSnMR2GeBSxgBGsc8k1Ty4dLkZo6qZOiPVYhimO/CyruWbdalgzoFVil6QOJdqu+eP4giQhjTThWKmBbcV6mGKpGeE0qzmJojEmEzyi6WztDJ3nko8CIfMXaTRTF3I4VGoaenmy2E0te4X4nzdIdHA7TFkUJ5pGZD4oSDjSAhUdIJ9JSjSf5gQTyfINERljiYnOmypOt5cPXSXdq4bdbDQfr+utu7KEKpzCGVyADTfQggdoQwcIZPAGn/BlvBivxrvxMY9WjPLPMSzA+P4DPEiSHA==</latexit>\\n\\x00 log y and\\nStacked\\nTransformer\\nBlocks\\nSo long and thanks for\\n…\\n…\\n…\\nU\\nInput tokens\\nx1 x2\\nLanguage\\nModeling\\nHead\\nx3 x4 x5\\nInput\\nEncoding\\n E\\n1+\\nE\\n2+\\nE\\n3+\\nE\\n4+\\nE\\n5+\\n…\\n… ………\\nU\\n U\\n U\\n U\\n…\\nlogits logits logits logits logits\\n…\\n<latexit sha1_base64=\"q3ZgXDyG7qtkT7t8hT47RdlwYG4=\">AAAB+XicbVDLSsNAFJ3UV62vWHe6GVsEN5bERXUlBUVcVrAPaEqYTCft0MlMmJkIIQT8AT/CTRE3Cv6Ev+DfmLTdtPXAwOGcM9x7jxcyqrRl/RqFtfWNza3idmlnd2//wDwst5WIJCYtLJiQXQ8pwignLU01I91QEhR4jHS88W3ud56JVFTwJx2HpB+gIac+xUhnkmseXzhMDGHsJk6A9EgGiR4hPlZpWnLNqlWzpoCrxJ6TauP0tXw3qdw0XfPHGQgcBYRrzJBSPdsKdT9BUlPMSFpyIkVChMdoSJLp5ik8y6QB9IXMHtdwqi7kUKBUHHhZMl9PLXu5+J/Xi7R/3U8oDyNNOJ4N8iMGtYB5DXBAJcGaxRlBWNJsQ4hHSCKss7Ly0+3lQ1dJ+7Jm12v1x6yDezBDEZyACjgHNrgCDfAAmqAFMHgBE/AJvozEeDPejY9ZtGDM/xyBBRjff79pldo=</latexit>\\n\\x00 log y thanks'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 34, 'page_label': '35', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsPretraining Large Language Models: Algorithm'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 35, 'page_label': '36', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsPretraining data for LLMs'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 36, 'page_label': '37', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"LLMs are mainly trained on the webCommon crawl, snapshots of the entire web produced by the non- profit Common Crawl with billions of pagesColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156 billion tokens of English,  filtered What's in it? Mostly patent text documents, Wikipedia, and news sites\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 37, 'page_label': '38', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='The Pile: a pretraining corpus\\nFigure 1: Treemap of Pile components by effective size.\\ntroduce a new ﬁltered subset of Common Crawl,\\nPile-CC, with improved extraction quality.\\nThrough our analyses, we conﬁrm that the Pile is\\nsigniﬁcantly distinct from pure Common Crawl\\ndata. Additionally, our evaluations show that the\\nexisting GPT-2 and GPT-3 models perform poorly\\non many components of the Pile, and that models\\ntrained on the Pile signiﬁcantly outperform both\\nraw and ﬁltered Common Crawl models. To com-\\nplement the performance evaluations, we also per-\\nform an exploratory analysis of the text within the\\nPile to provide a detailed picture of the data. We\\nhope that our extensive documentation of the con-\\nstruction and characteristics of the Pile will help\\nresearchers make informed decisions about poten-\\ntial downstream applications.\\nFinally, we make publicly available the preprocess-\\ning code for the constituent datasets of the Pile and\\nthe code for constructing alternative versions 2\\n. In\\nthe interest of reproducibility, we also document\\nall processing performed on each dataset (and the\\nPile as a whole) in as much detail as possible. For\\nfurther details about the processing of each dataset,\\nsee Section 2 and Appendix C .\\n2\\nhttps://github.com/EleutherAI/\\nthe-pile\\n1.1 Contributions\\nThe core contributions of this paper are:\\n1. The introduction of a 825 . 18 GiB english-\\nlanguage dataset for language modeling com-\\nbining 22 diverse sources.\\n2. The introduction of 14 new language model-\\ning datasets, which we expect to be of inde-\\npendent interest to researchers.\\n3. Evaluations demonstrating signiﬁcant im-\\nprovements across many domains by GPT-2-\\nsized models trained on this new dataset, com-\\npared to training on CC-100 and raw Common\\nCrawl.\\n4. The investigation and documentation of this\\ndataset, which we hope will better inform re-\\nsearchers about how to use it as well as moti-\\nvate them to undertake similar investigations\\nof their own data.\\n2 The Pile Datasets\\nThe Pile is composed of 22 constituent sub-datasets,\\nas shown in Table 1 . Following Brown et al. ( 2020 ),\\nwe increase the weights of higher quality compo-\\nnents, with certain high-quality datasets such as\\nWikipedia being seen up to 3 times (“epochs”) for\\n2\\nwebacademics books\\ndialog'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 38, 'page_label': '39', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Filtering for quality and safetyQuality is subjective•Many LLMs attempt to match Wikipedia, books, particular websites•Need to remove boilerplate, adult content•Deduplication at many levels (URLs, documents, even lines)Safety also subjective•Toxicity detection is important, although that has mixed results•Can mistakenly flag data written in dialects like African American English'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 39, 'page_label': '40', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='What does a model learn from pretraining?•There are canines everywhere! One dog in the front room, and two dogs•It wasn\\'t just big it was enormous•The author of \"A Room of One\\'s Own\" is Virginia Woolf•The doctor told me that he•The square root of 4 is 2'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 40, 'page_label': '41', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Big ideaText contains enormous amounts of knowledgePretraining on lots of text with all that knowledge is what gives language models their ability to do so much'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 41, 'page_label': '42', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"But there are problems with scraping from the webCopyright: much of the text in these datasets is copyrighted•Not clear if fair use doctrine in US allows for this use•This remains an open legal questionData consent•Website owners can indicate they don't want their site crawledPrivacy: •Websites can contain private IP addresses and phone numbers\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 42, 'page_label': '43', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsPretraining data for LLMs'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 43, 'page_label': '44', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsFinetuning'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 44, 'page_label': '45', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"Finetuning for daptation to new domainsWhat happens if we need our LLM to work well on a domain it didn't see in pretraining?Perhaps some specific medical or legal domain?Or maybe a multilingual LM needs to see more data on some language that was rare in pretraining?\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 45, 'page_label': '46', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='FinetuningFine-\\ntuning \\nData\\nPretraining Data\\nPretraining\\n…\\n …\\n …\\nFine-tuning\\n…\\n …\\n …\\nPretrained LM Fine-tuned LM'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 46, 'page_label': '47', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='\"Finetuning\" means 4 different thingsWe\\'ll discuss 1 here, and 3 in later lecturesIn all four cases, finetuning means:taking a pretrained model and further adapting some or all of its parameters to some new data'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 47, 'page_label': '48', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='1. Finetuning as \"continued pretraining\" on new data•Further train all the parameters of model on new data•using the same method (word prediction) and loss function (cross-entropy loss) as for pretraining.•as if the new data were at the tail end of the pretraining data•Hence sometimes called continued pretraining'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 48, 'page_label': '49', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsFinetuning'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 49, 'page_label': '50', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsEvaluating Large Language Models'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 50, 'page_label': '51', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='PerplexityJust as for n-gram grammars, we use perplexity to measure how well the LM predicts unseen textThe perplexity of a model θ on an unseen test set is the inverse probability that θ assigns to the test set, normalized by the test set length. For a test set of n tokens w1:n the perplexity is :\\n12 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\nthe pretraining data, and so you’ll sometimes see this method called continued pre-\\ntraining .\\ncontinued\\npretraining\\nRetraining all the parameters of the model is very slow and expensive when the\\nlanguage model is huge. So instead we can freeze some of the parameters (i.e., leavefreeze\\nthem unchanged from their pretrained value) and train only a subset of parameters\\non the new data. In Section 10.5.3 we’ll describe this second variety of ﬁnetun-\\ning, called parameter-efﬁcient ﬁnetuning , or PEFT . because we efﬁciently select\\nspeciﬁc parameters to update when ﬁnetuning, and leave the rest in their pretrained\\nvalues.\\nIn Chapter 11 we’ll introduce a third kind of ﬁnetuning, also parameter-efﬁcient.\\nIn this version, the goal is to use a language model as a kind of classiﬁer or labeler\\nfor a speciﬁc task. For example we might train the model to be a sentiment classiﬁer.\\nWe do this by adding extra neural circuitry (an extra head ) after the top layer of the\\nmodel. This classiﬁcation head takes as input some of the top layer embeddings of\\nthe transformer and produces as output a classiﬁcation. In this method, most com-\\nmonly used with masked language models like BERT, we freeze the entire pretrained\\nmodel and only train the classiﬁcation head on some new data, usually labeled with\\nsome class that we want to predict.\\nFinally, in Chapter 12 we’ll introduce a fourth kind of ﬁnetuning, that is a cru-\\ncial component of the largest language models: supervised ﬁnetuning or SFT . SFT\\nis often used for instruction ﬁnetuning , in which we want a pretrained language\\nmodel to learn to follow text instructions, for example to answer questions or follow\\na command to write something. Here we create a dataset of prompts and desired\\nresponses (for example questions and their answers, or commands and their ful-\\nﬁllments), and we train the language model using the normal cross-entropy loss to\\npredict each token in the instruction prompt iteratively, essentially training it to pro-\\nduce the desired response from the command in the prompt. It’s called supervised\\nbecause unlike in pretraining, where we just take any data and predict the words in\\nit, we build the special ﬁnetuning dataset by hand, creating supervised responses to\\neach command.\\nOften everything that happens after pretraining is lumped together as post-training ;\\nwe’ll discuss the various parts of post-training in Chapter 12 and Chapter 13.\\n10.4 Evaluating Large Language Models\\nPerplexity As we ﬁrst saw in Chapter 3, one way to evaluate language models is\\nto measure how well they predict unseen text. Intuitively, good models are those that\\nassign higher probabilities to unseen data (are less surprised when encountering the\\nnew words).\\nWe instantiate this intuition by using perplexity to measure the quality of aperplexity\\nlanguage model. Recall from page ?? that the perplexity of a model q on an unseen\\ntest set is the inverse probability that q assigns to the test set, normalized by the test\\nset length. For a test set of n tokens w 1: n , the perplexity is\\nPerplexity\\nq\\n( w 1: n )= P q ( w 1: n )\\n\\x00\\n1\\nn\\n=\\nn\\ns\\n1\\nP q ( w 1: n )\\n(10.7)\\nTo visualize how perplexity can be computed as a function of the probabilities the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 51, 'page_label': '52', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='•Probability depends on size of test set•Probability gets smaller the longer the text•Better: a metric that is per-word, normalized by length•Perplexity is the inverse probability of the test set, normalized by the number of words(The inverse comes from the original definition of perplexity from cross-entropy rate in information theory)Probability range is  [0,1], perplexity range is [1,∞]\\nWhy perplexity instead of raw probability of the test set?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 52, 'page_label': '53', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Perplexity•The higher the probability of the word sequence, the lower the perplexity.•Thus the lower the perplexity of a model on the data, the better the model. •Minimizing perplexity is the same as maximizing probabilityAlso: perplexity is sensitive to length/tokenization so best used when comparing LMs that use the same tokenizer.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 53, 'page_label': '54', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Many other factors that we evaluate, like:Size Big models take lots of GPUs and time to train, memory to storeEnergy usageCan measure kWh or kilograms of CO2 emitted FairnessBenchmarks measure gendered and racial stereotypes, or decreased performance for language from or about some groups.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 54, 'page_label': '55', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsDealing with Scale'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 55, 'page_label': '56', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Scaling LawsLLM performance depends on•Model size: the number of parameters not counting embeddings•Dataset size: the amount of training data•Compute: Amount of compute (in FLOPS or etcCan improve a model by adding  parameters (more layers, wider contexts), more data, or training for more iterationsThe performance of a large language model (the loss) scales as a power-law with each of these three'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 56, 'page_label': '57', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Scaling LawsLoss L as a function of # parameters N, dataset size D, compute budget C (if other two are held constant)\\n14 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\n10.5 Dealing with Scale\\nLarge language models are large. For example the Llama 3.1 405B Instruct model\\nfrom Meta has 405 billion parameters (126 layers, a model dimensionality of 16,384,\\n128 attention heads) and was trained on 15.6 terabytes of text tokens ( Llama Team ,\\n2024 ), using a vocabulary of 128K tokens. So there is a lot of research on un-\\nderstanding how LLMs scale, and especially how to implement them given limited\\nresources. In the next few sections we discuss how to think about scale (the concept\\nof scaling laws ), and important techniques for getting language models to work\\nefﬁciently, such as the KV cache and parameter-efﬁcient ﬁne tuning.\\n10.5.1 Scaling laws\\nThe performance of large language models has shown to be mainly determined by\\n3 factors: model size (the number of parameters not counting embeddings), dataset\\nsize (the amount of training data), and the amount of compute used for training. That\\nis, we can improve a model by adding parameters (adding more layers or having\\nwider contexts or both), by training on more data, or by training for more iterations.\\nThe relationships between these factors and performance are known as scaling\\nlaws . Roughly speaking, the performance of a large language model (the loss) scalesscaling laws\\nas a power-law with each of these three properties of model training.\\nFor example, Kaplan et al. ( 2020 ) found the following three relationships for\\nloss L as a function of the number of non-embedding parameters N , the dataset size\\nD , and the compute budget C , for models training with limited parameters, dataset,\\nor compute budget, if in each case the other two properties are held constant:\\nL ( N )=\\n✓\\nN c\\nN\\n◆\\na N\\n(10.9)\\nL ( D )=\\n✓\\nD c\\nD\\n◆\\na D\\n(10.10)\\nL ( C )=\\n✓\\nC c\\nC\\n◆\\na\\nC\\n(10.11)\\nThe number of (non-embedding) parameters N can be roughly computed as fol-\\nlows (ignoring biases, and with d as the input and output dimensionality of the\\nmodel, d attn\\nas the self-attention layer size, and d ff\\nthe size of the feedforward layer):\\nN ⇡ 2 dn layer\\n( 2 d attn\\n+ d ff\\n)\\n⇡ 12 n layer\\nd\\n2\\n(10.12)\\n( assuming d attn\\n= d ff\\n/ 4 = d )\\nThus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 ⇥ 96 ⇥\\n12288\\n2\\n⇡ 175 billion parameters.\\nThe values of N c\\n, D c\\n, C c\\n, a N\\n, a D\\n, and a C\\ndepend on the exact transformer\\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,\\nscaling laws focus on the relationship with loss.\\n2\\nScaling laws can be useful in deciding how to train a model to a particular per-\\nformance, for example by looking at early in the training curve, or performance with\\n2\\nFor the initial experiment in Kaplan et al. ( 2020 ) the precise values were a N\\n= 0.076, N c\\n= 8.8 ⇥ 10\\n13\\n(parameters), a D\\n= 0.095, D c\\n= 5.4 ⇥ 10\\n13\\n(tokens), a C\\n= 0.050, C c\\n= 3.1 ⇥ 10\\n8\\n(petaﬂop-days).\\nScaling laws can be used early in training to predict what the loss would be if we were to add more data or increase model size.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 57, 'page_label': '58', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Number of non-embedding parameters N\\n14 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\n10.5 Dealing with Scale\\nLarge language models are large. For example the Llama 3.1 405B Instruct model\\nfrom Meta has 405 billion parameters (126 layers, a model dimensionality of 16,384,\\n128 attention heads) and was trained on 15.6 terabytes of text tokens ( Llama Team ,\\n2024 ), using a vocabulary of 128K tokens. So there is a lot of research on un-\\nderstanding how LLMs scale, and especially how to implement them given limited\\nresources. In the next few sections we discuss how to think about scale (the concept\\nof scaling laws ), and important techniques for getting language models to work\\nefﬁciently, such as the KV cache and parameter-efﬁcient ﬁne tuning.\\n10.5.1 Scaling laws\\nThe performance of large language models has shown to be mainly determined by\\n3 factors: model size (the number of parameters not counting embeddings), dataset\\nsize (the amount of training data), and the amount of compute used for training. That\\nis, we can improve a model by adding parameters (adding more layers or having\\nwider contexts or both), by training on more data, or by training for more iterations.\\nThe relationships between these factors and performance are known as scaling\\nlaws . Roughly speaking, the performance of a large language model (the loss) scalesscaling laws\\nas a power-law with each of these three properties of model training.\\nFor example, Kaplan et al. ( 2020 ) found the following three relationships for\\nloss L as a function of the number of non-embedding parameters N , the dataset size\\nD , and the compute budget C , for models training with limited parameters, dataset,\\nor compute budget, if in each case the other two properties are held constant:\\nL ( N )=\\n✓\\nN c\\nN\\n◆\\na N\\n(10.9)\\nL ( D )=\\n✓\\nD c\\nD\\n◆\\na D\\n(10.10)\\nL ( C )=\\n✓\\nC c\\nC\\n◆\\na\\nC\\n(10.11)\\nThe number of (non-embedding) parameters N can be roughly computed as fol-\\nlows (ignoring biases, and with d as the input and output dimensionality of the\\nmodel, d attn\\nas the self-attention layer size, and d ff\\nthe size of the feedforward layer):\\nN ⇡ 2 dn layer\\n( 2 d attn\\n+ d ff\\n)\\n⇡ 12 n layer\\nd\\n2\\n(10.12)\\n( assuming d attn\\n= d ff\\n/ 4 = d )\\nThus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 ⇥ 96 ⇥\\n12288\\n2\\n⇡ 175 billion parameters.\\nThe values of N c\\n, D c\\n, C c\\n, a N\\n, a D\\n, and a C\\ndepend on the exact transformer\\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,\\nscaling laws focus on the relationship with loss.\\n2\\nScaling laws can be useful in deciding how to train a model to a particular per-\\nformance, for example by looking at early in the training curve, or performance with\\n2\\nFor the initial experiment in Kaplan et al. ( 2020 ) the precise values were a N\\n= 0.076, N c\\n= 8.8 ⇥ 10\\n13\\n(parameters), a D\\n= 0.095, D c\\n= 5.4 ⇥ 10\\n13\\n(tokens), a C\\n= 0.050, C c\\n= 3.1 ⇥ 10\\n8\\n(petaﬂop-days).\\nThus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 × 96 × 122882 ≈ 175 billion parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 58, 'page_label': '59', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"KV CacheIn training, we can compute attention very efficiently in parallel:\\nBut not at inference! We generate the next tokens one at a time!For a new token x, need to multiply by WQ, WK, and WV to get query, key, valuesBut don't want to recompute the key and value vectors for all the prior tokens x<iInstead, store key and value vectors in memory in the KV cache, and then we can just grab them from the cache \\n10.5 • D EALING WITH S CALE 15\\nsmaller amounts of data, to predict what the loss would be if we were to add more\\ndata or increase model size. Other aspects of scaling laws can also tell us how much\\ndata we need to add when scaling up a model.\\n10.5.2 KV Cache\\nWe saw in Fig. ?? and in Eq. ?? (repeated below) how the attention vector can be\\nvery efﬁciently computed in parallel for training, via two matrix multiplications:\\nA = softmax\\n✓\\nQK\\n|\\np\\nd k\\n◆\\nV (10.13)\\nUnfortunately we can’t do quite the same efﬁcient computation in inference as\\nin training. That’s because at inference time, we iteratively generate the next tokens\\none at a time. For a new token that we have just generated, call it x i , we need to\\ncompute its query, key, and values by multiplying by W\\nQ\\n, W\\nK\\n, and W\\nV\\nrespec-\\ntively. But it would be a waste of computation time to recompute the key and value\\nvectors for all the prior tokens x < i ; at prior steps we already computed these key\\nand value vectors! So instead of recomputing these, whenever we compute the key\\nand value vectors we store them in memory in the KV cache , and then we can justKV cache\\ngrab them from the cache when we need them. Fig. 10.7 modiﬁes Fig. ?? to show\\nthe computation that takes place for a single new token, showing which values we\\ncan take from the cache rather than recompute.\\nq4\\nk1\\nk2\\nk4\\nQ\\nKT\\nQKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq4•k1q4•k2q4•k3q4•k4\\nx = =x\\na4\\nA\\n1 x dk\\ndk x N\\n1 x N N x dv 1 x dv\\nk3\\nFigure 10.7 Parts of the attention computation (extracted from Fig. ?? ) showing, in black,\\nthe vectors that can be stored in the cache rather than recomputed when computing the atten-\\ntion score for the 4th token.\\n10.5.3 Parameter Efﬁcient Fine Tuning\\nAs we mentioned above, it’s very common to take a language model and give it more\\ninformation about a new domain by ﬁnetuning it (continuing to train it to predict\\nupcoming words) on some additional data.\\nFine-tuning can be very difﬁcult with very large language models, because there\\nare enormous numbers of parameters to train; each pass of batch gradient descent\\nhas to backpropagate through many many huge layers. This makes ﬁnetuning huge\\nlanguage models extremely expensive in processing power, in memory, and in time.\\nFor this reason, there are alternative methods that allow a model to be ﬁnetuned\\nwithout changing all the parameters. Such methods are called parameter-efﬁcient\\nﬁne tuning or sometimes PEFT , because we efﬁciently select a subset of parameters\\nparameter-\\nefﬁcient ﬁne\\ntuning\\nPEFT to update when ﬁnetuning. For example we freeze some of the parameters (don’t\\nchange them), and only update some particular subset of parameters.\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 59, 'page_label': '60', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='KV Cache\\nq4\\nk1\\nk2\\nk4\\nQ\\nKT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq4•k1q4•k2q4•k3q4•k4\\nx = =x\\na4\\nA\\n1 x dk\\ndk x N\\n1 x N N x dv 1 x dv\\nk3\\nq1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2q4•k3q4•k4\\nq3•k2q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1q2•k2\\nq4•k1q4•k2q4•k3q4•k4\\nq3•k1q3•k2q3•k3\\nq1•k2\\nq2•k3\\nq1•k3\\nq3•k4\\nq2•k4\\nq1•k4x =\\nQKT masked\\nmask =\\nq1•k1\\nq2•k1\\nq4•k1\\nq3•k1\\nq1•k1q1•k1\\n=x\\na1\\na2\\na3\\na4\\nA\\nQuery \\nToken 1\\nQuery \\nToken 2\\nQuery \\nToken 3\\nQuery \\nToken 4\\nQ\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nx\\nWQ\\n=\\nValue \\nToken 1\\nValue \\nToken 2\\nValue \\nToken 3\\nValue \\nToken 4\\nV\\nx\\nWV\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nKey \\nToken 1\\nKey \\nToken 2\\nKey \\nToken 3\\nKey \\nToken 4\\nK\\nx\\nWK\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nN x dk\\ndk x N\\nN x N N x N N x dv N x dv\\nd x dk d x dk d x dvN x d N x dk N x d N x dk N x d N x dv\\nq1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2q4•k3q4•k4\\nq3•k2q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1q2•k2\\nq4•k1q4•k2q4•k3q4•k4\\nq3•k1q3•k2q3•k3\\nq1•k2\\nq2•k3\\nq1•k3\\nq3•k4\\nq2•k4\\nq1•k4x =\\nQKT masked\\nmask =\\nq1•k1\\nq2•k1\\nq4•k1\\nq3•k1\\nq1•k1q1•k1\\n=x\\na1\\na2\\na3\\na4\\nA\\nQuery \\nToken 1\\nQuery \\nToken 2\\nQuery \\nToken 3\\nQuery \\nToken 4\\nQ\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nx\\nWQ\\n=\\nValue \\nToken 1\\nValue \\nToken 2\\nValue \\nToken 3\\nValue \\nToken 4\\nV\\nx\\nWV\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nKey \\nToken 1\\nKey \\nToken 2\\nKey \\nToken 3\\nKey \\nToken 4\\nK\\nx\\nWK\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nN x dk\\ndk x N\\nN x N N x N N x dv N x dv\\nd x dk d x dk d x dvN x d N x dk N x d N x dk N x d N x dv\\nq1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2q4•k3q4•k4\\nq3•k2q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1q2•k2\\nq4•k1q4•k2q4•k3q4•k4\\nq3•k1q3•k2q3•k3\\nq1•k2\\nq2•k3\\nq1•k3\\nq3•k4\\nq2•k4\\nq1•k4x =\\nQKT masked\\nmask =\\nq1•k1\\nq2•k1\\nq4•k1\\nq3•k1\\nq1•k1q1•k1\\n=x\\na1\\na2\\na3\\na4\\nA\\nQuery \\nToken 1\\nQuery \\nToken 2\\nQuery \\nToken 3\\nQuery \\nToken 4\\nQ\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nx\\nWQ\\n=\\nValue \\nToken 1\\nValue \\nToken 2\\nValue \\nToken 3\\nValue \\nToken 4\\nV\\nx\\nWV\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nKey \\nToken 1\\nKey \\nToken 2\\nKey \\nToken 3\\nKey \\nToken 4\\nK\\nx\\nWK\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nN x dk\\ndk x N\\nN x N N x N N x dv N x dv\\nd x dk d x dk d x dvN x d N x dk N x d N x dk N x d N x dv'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 60, 'page_label': '61', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Parameter-Efficient FinetuningAdapting to a new domain by continued pretraining (finetuning) is a problem with huge LLMs.•Enormous numbers of parameters to train •Each pass of batch gradient descent has to backpropagate through many many huge layers. •Expensive in processing power, in memory, and in time. Instead, parameter-efficient fine tuning (PEFT)•Efficiently select a subset of parameters to update when finetuning.•E.g., freeze some of the parameters (don’t change them), •And only update some a few parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 61, 'page_label': '62', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='LoRA (Low-Rank Adaptation)•Trransformers have many dense matrix multiply layers•Like WQ, WK, WV, WO layers in attention•Instead of updating these layers during finetuning, •Freeze these layers •Update a low-rank approximation with fewer parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 62, 'page_label': '63', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='LoRA•Consider a matrix W (shape [N × d])  that needs to be updated during finetuning via gradient descent. •Normally updates are ∆W  (shape [N × d])•In LoRA, we freeze W and update instead a low-rank decomposition of W:•A of shape [N×r], •B of shape [r×d], r is very small  (like 1 or 2)•That is, during  finetuning we update A and B instead of W. •Replace W + ∆W with W + BA. Forward pass: instead of     h = xW We do     h = xW + xAB'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 63, 'page_label': '64', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='LoRAh\\nPretrained \\nWeights\\nW\\nd\\nk r\\nk\\nA\\nBr\\nx\\nd\\n1\\n1\\nk\\nd\\n×'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 64, 'page_label': '65', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsDealing with Scale'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 65, 'page_label': '66', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsHarms of Large Language Models'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 66, 'page_label': '67', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Hallucination'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 67, 'page_label': '68', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Copyright'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 68, 'page_label': '69', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Privacy'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 69, 'page_label': '70', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Toxicity and Abuse'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 70, 'page_label': '71', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Misinformation'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 71, 'page_label': '72', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsHarms of Large Language Models')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "582eefad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32b51a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 72 documents into 134 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Large Language ModelsIntroduction to Large Language Models...\n",
      "Metadata: {'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 0, 'page_label': '1', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 0, 'page_label': '1', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsIntroduction to Large Language Models'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 1, 'page_label': '2', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Language models•Remember the simple n-gram language model•Assigns probabilities to sequences of words•Generate text by sampling possible next words•Is trained on counts computed from lots of text•Large language models are similar and different:•Assigns probabilities to sequences of words•Generate text by sampling possible next words•Are trained by learning to guess the next word'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 2, 'page_label': '3', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large language models•Even through pretrained only to predict words•Learn a lot of useful language knowledge•Since training on a lot of text'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 3, 'page_label': '4', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Three architectures for large language models\\nDecoders   Encoders     Encoder-decodersGPT, Claude,  BERT family,  Flan-T5, WhisperLlama    HuBERTMixtral\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 3, 'page_label': '4', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Encoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 4, 'page_label': '5', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='EncodersMany varieties!•Popular: Masked Language Models (MLMs)•BERT family•Trained by predicting words from surrounding words on both sides•Are usually finetuned (trained on supervised data) for classification tasks.\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 5, 'page_label': '6', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Encoder-Decoders\\n•Trained to map from one sequence to another•Very popular for:•machine translation (map from one language to another)•speech recognition (map from acoustics to words)\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 6, 'page_label': '7', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsIntroduction to Large Language Models'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 7, 'page_label': '8', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsLarge Language Models: What tasks can they do?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 8, 'page_label': '9', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Big ideaMany tasks can be turned into tasks of predicting words!'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 9, 'page_label': '10', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='This lecture: decoder-only modelsAlso called:•Causal LLMs•Autoregressive LLMs•Left-to-right LLMs•Predict words left to right\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 10, 'page_label': '11', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Conditional Generation: Generating text conditioned on previous text!\\nPreﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead logits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 11, 'page_label': '12', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Many practical NLP tasks can be cast as word prediction!Sentiment analysis: “I like Jackie Chan”1.We give the language model this string:The sentiment of the sentence \"I like Jackie Chan\" is: 2.And see what word it thinks comes next:\\n10.1 • L ARGE L ANGUAGE M ODELS WITH T RANSFORMERS 3\\nPreﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead logits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…\\nFigure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language\\nmodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.\\nword “negative” to see which is higher:\\nP ( positive | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nP ( negative | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 11, 'page_label': '12', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='P ( negative | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is\\npositive, otherwise we say the sentiment is negative.\\nWe can also cast more complex tasks as word prediction. Consider question\\nanswering, in which the system is given a question (for example a question with\\na simple factual answer) and must give a textual answer; we introduce this task in\\ndetail in Chapter 15. We can cast the task of question answering as word prediction\\nby giving a language model a question and a token like A: suggesting that an answer\\nshould come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute the probability distribution over possible\\nnext words given this preﬁx:\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: )\\nand look at which words w have high probabilities, we might expect to see that'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 11, 'page_label': '12', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='next words given this preﬁx:\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: )\\nand look at which words w have high probabilities, we might expect to see that\\nCharles is very likely, and then if we choose Charles and continue and ask\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles )\\nwe might now see that Darwin is the most probable token, and select it.\\nConditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext\\nsummarization\\ntext, such as a full-length article, and produce an effective shorter summary of it. We\\ncan cast summarization as language modeling by giving a large language model a\\ntext, and follow the text by a token like tl;dr ; this token is short for something like'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Framing lots of tasks as conditional generationQA: “Who wrote The Origin of Species”1.We give the language model this string:2.And see what word it thinks comes next:\\n3.And iterate:\\n20 C HAPTER 10 • T RANSFORMERS AND L ARGE L ANGUAGE M ODELS\\nPreﬁx Text\\nCompletion Text\\nInput\\nEmbeddings\\nTransformer\\nBlocks\\nSample from Softmax\\nSo long\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nlinear layer\\nFigure 10.15 Autoregressive text completion with transformer-based large language models.\\nword “negative” to see which is higher:\\nP ( positive | The sentiment of the sentence “I like Jackie Chan” is: )\\nP ( negative | The sentiment of the sentence “I like Jackie Chan” is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is\\npositive, otherwise we say the sentiment is negative.\\nWe can also cast more complex tasks as word prediction. Consider the task\\nof answering simple questions, a task we return to in Chapter 14. In this task the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='We can also cast more complex tasks as word prediction. Consider the task\\nof answering simple questions, a task we return to in Chapter 14. In this task the\\nsystem is given some question and must give a textual answer. We can cast the task\\nof question answering as word prediction by giving a language model a question and\\na token like A: suggesting that an answer should come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute\\nP ( w | Q: Who wrote the book “The Origin of Species”? A: )\\nand look at which words w have high probabilities, we might expect to see that\\nCharles is very likely, and then if we choose Charles and continue and ask\\nP ( w | Q: Who wrote the book “The Origin of Species”? A: Charles )\\nwe might now see that Darwin is the most probable word, and select it.\\nConditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Conditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext\\nsummarization\\ntext, such as a full-length article, and produce an effective shorter summary of it.\\nWe can cast summarization as language modeling by giving a large language model\\na text, and follow the text by a token like tl;dr ; this token is short for something\\nlike ‘too long; don’t read’ and in recent years people often use this token, especially\\nin informal work emails, when they are going to give a short summary. We can\\nthen do conditional generation: give the language model this preﬁx, and then ask\\n10.1 • L ARGE L ANGUAGE M ODELS WITH T RANSFORMERS 3\\nPreﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead\\nlogits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Preﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead\\nlogits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…\\nFigure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language\\nmodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.\\nword “negative” to see which is higher:\\nP ( positive | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nP ( negative | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is\\npositive, otherwise we say the sentiment is negative.\\nWe can also cast more complex tasks as word prediction. Consider question\\nanswering, in which the system is given a question (for example a question with\\na simple factual answer) and must give a textual answer; we introduce this task in'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='answering, in which the system is given a question (for example a question with\\na simple factual answer) and must give a textual answer; we introduce this task in\\ndetail in Chapter 15. We can cast the task of question answering as word prediction\\nby giving a language model a question and a token like A: suggesting that an answer\\nshould come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute the probability distribution over possible\\nnext words given this preﬁx:\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: )\\nand look at which words w have high probabilities, we might expect to see that\\nCharles is very likely, and then if we choose Charles and continue and ask\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles )\\nwe might now see that Darwin is the most probable token, and select it.\\nConditional generation can even be used to accomplish tasks that must generate'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='we might now see that Darwin is the most probable token, and select it.\\nConditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext\\nsummarization\\ntext, such as a full-length article, and produce an effective shorter summary of it. We\\ncan cast summarization as language modeling by giving a large language model a\\ntext, and follow the text by a token like tl;dr ; this token is short for something like\\n10.1 • L ARGE L ANGUAGE M ODELS WITH T RANSFORMERS 3\\nPreﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead logits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…\\nFigure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language\\nmodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='models. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.\\nword “negative” to see which is higher:\\nP ( positive | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nP ( negative | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is\\npositive, otherwise we say the sentiment is negative.\\nWe can also cast more complex tasks as word prediction. Consider question\\nanswering, in which the system is given a question (for example a question with\\na simple factual answer) and must give a textual answer; we introduce this task in\\ndetail in Chapter 15. We can cast the task of question answering as word prediction\\nby giving a language model a question and a token like A: suggesting that an answer\\nshould come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute the probability distribution over possible'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='should come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute the probability distribution over possible\\nnext words given this preﬁx:\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: )\\nand look at which words w have high probabilities, we might expect to see that\\nCharles is very likely, and then if we choose Charles and continue and ask\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles )\\nwe might now see that Darwin is the most probable token, and select it.\\nConditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext\\nsummarization\\ntext, such as a full-length article, and produce an effective shorter summary of it. We\\ncan cast summarization as language modeling by giving a large language model a\\ntext, and follow the text by a token like tl;dr ; this token is short for something like'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Summarization\\n4 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\n‘too long; didn’t read’ and in recent years people often use this token, especially in\\ninformal work emails, when they are going to give a short summary. Since this token\\nis sufﬁciently frequent in language model training data, language models have seen\\nmany texts in which the token occurs before a summary, and hence will interpret the\\ntoken as instructions to generate a summary. We can then do conditional generation:\\ngive the language model this preﬁx, and then have it generate the following words,\\none by one, and take the entire response as a summary. Fig. 10.2 shows an example\\nof a text and a human-produced summary from a widely-used summarization corpus\\nconsisting of CNN and Daily Mirror news articles.\\nOriginal Article\\nThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff\\nand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='The only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff\\nand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says.\\nBut not if you live in New England or surrounding states. “We will not ship snow to any states\\nin the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging\\nsnow!”\\nHis website and social media accounts claim to have ﬁlled more than 133 orders for snow – more\\nthan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a\\nrecord this winter for the snowiest month in its history. Most residents see the huge piles of snow\\nchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.\\nAccording to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='choking their yards and sidewalks as a nuisance, but Waring saw an opportunity.\\nAccording to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-\\neling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. He\\njoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]\\nSummary\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.\\nFigure 10.2 Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus\\n( Hermann et al. , 2015 ), ( Nallapati et al. , 2016 ).\\nIf we take this full article and append the token tl;dr , we can use this as the con-\\ntext to prime the generation process to produce a summary as illustrated in Fig. 10.3 .\\nAgain, what makes transformers able to succeed at this task (as compared, say, to'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='text to prime the generation process to produce a summary as illustrated in Fig. 10.3 .\\nAgain, what makes transformers able to succeed at this task (as compared, say, to\\nthe primitive n-gram language model) is that attention can incorporate information\\nfrom the large context window, giving the model access to the original article as well\\nas to the newly generated text throughout the process.\\nWhich words do we generate at each step? One simple way to generate words\\nis to always generate the most likely word given the context. Generating the most\\nlikely word given the context is called greedy decoding . A greedy algorithm is one\\ngreedy\\ndecoding\\nthat make a choice that is locally optimal, whether or not it will turn out to have\\nbeen the best choice with hindsight. Thus in greedy decoding, at each time step in\\ngeneration, the output y t is chosen by computing the probability for each possible\\noutput (every word in the vocabulary) and then choosing the highest probability'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='generation, the output y t is chosen by computing the probability for each possible\\noutput (every word in the vocabulary) and then choosing the highest probability\\nword (the argmax):\\nˆw t = argmax w 2 V P ( w | w < t ) (10.1)\\nIn practice, however, we don’t use greedy decoding with large language models.\\nA major problem with greedy decoding is that because the words it chooses are (by\\ndeﬁnition) extremely predictable, the resulting text is generic and often quite repeti-\\ntive. Indeed, greedy decoding is so predictable that it is deterministic; if the context\\n4 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\n‘too long; didn’t read’ and in recent years people often use this token, especially in\\ninformal work emails, when they are going to give a short summary. Since this token\\nis sufﬁciently frequent in language model training data, language models have seen\\nmany texts in which the token occurs before a summary, and hence will interpret the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='is sufﬁciently frequent in language model training data, language models have seen\\nmany texts in which the token occurs before a summary, and hence will interpret the\\ntoken as instructions to generate a summary. We can then do conditional generation:\\ngive the language model this preﬁx, and then have it generate the following words,\\none by one, and take the entire response as a summary. Fig. 10.2 shows an example\\nof a text and a human-produced summary from a widely-used summarization corpus\\nconsisting of CNN and Daily Mirror news articles.\\nOriginal Article\\nThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff\\nand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says.\\nBut not if you live in New England or surrounding states. “We will not ship snow to any states'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='for 10 to 15 snowballs, he says.\\nBut not if you live in New England or surrounding states. “We will not ship snow to any states\\nin the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging\\nsnow!”\\nHis website and social media accounts claim to have ﬁlled more than 133 orders for snow – more\\nthan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a\\nrecord this winter for the snowiest month in its history. Most residents see the huge piles of snow\\nchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.\\nAccording to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-\\neling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. He\\njoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]\\nSummary\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Summary\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.\\nFigure 10.2 Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus\\n( Hermann et al. , 2015 ), ( Nallapati et al. , 2016 ).\\nIf we take this full article and append the token tl;dr , we can use this as the con-\\ntext to prime the generation process to produce a summary as illustrated in Fig. 10.3 .\\nAgain, what makes transformers able to succeed at this task (as compared, say, to\\nthe primitive n-gram language model) is that attention can incorporate information\\nfrom the large context window, giving the model access to the original article as well\\nas to the newly generated text throughout the process.\\nWhich words do we generate at each step? One simple way to generate words\\nis to always generate the most likely word given the context. Generating the most'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Which words do we generate at each step? One simple way to generate words\\nis to always generate the most likely word given the context. Generating the most\\nlikely word given the context is called greedy decoding . A greedy algorithm is one\\ngreedy\\ndecoding\\nthat make a choice that is locally optimal, whether or not it will turn out to have\\nbeen the best choice with hindsight. Thus in greedy decoding, at each time step in\\ngeneration, the output y t is chosen by computing the probability for each possible\\noutput (every word in the vocabulary) and then choosing the highest probability\\nword (the argmax):\\nˆw t = argmax w 2 V\\nP ( w | w < t ) (10.1)\\nIn practice, however, we don’t use greedy decoding with large language models.\\nA major problem with greedy decoding is that because the words it chooses are (by\\ndeﬁnition) extremely predictable, the resulting text is generic and often quite repeti-\\ntive. Indeed, greedy decoding is so predictable that it is deterministic; if the context\\nOriginal'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='deﬁnition) extremely predictable, the resulting text is generic and often quite repeti-\\ntive. Indeed, greedy decoding is so predictable that it is deterministic; if the context\\nOriginal\\nSummary'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 14, 'page_label': '15', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='LLMs for summarization (using  tl;dr)\\nOriginal Story\\nGenerated Summary\\n… idea\\nKyle\\nwas born. Kyle\\nWaring\\nWaringonlyThe\\n…\\nwill\\nDelimiter\\nwill\\nU U U\\ntl;dr\\nLM Head\\nE\\n E\\n E\\n E\\n E\\n E\\n E\\n E\\n…'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 15, 'page_label': '16', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsLarge Language Models: What tasks can they do?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 16, 'page_label': '17', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsSampling for LLM Generation'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 17, 'page_label': '18', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Decoding and SamplingThis task of choosing a word to generate based on the model’s probabilities is called decoding. The most common method for decoding in LLMs: sampling. Sampling from a model’s distribution over words:•choose random words according to their probability assigned by the model. After each token we’ll sample words to generate according to their probability conditioned on our previous choices, •A transformer language model will give the probability'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 18, 'page_label': '19', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Random sampling\\n6 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\nas deﬁned by the model. Thus we are more likely to generate words that the model\\nthinks have a high probability in the context and less likely to generate words that\\nthe model thinks have a low probability.\\nWe saw back in Chapter 3 on page ?? how to generate text from a unigram lan-\\nguage model , by repeatedly randomly sampling words according to their probability\\nuntil we either reach a pre-determined length or select the end-of-sentence token. To\\ngenerate text from a trained transformer language model we’ll just generalize this\\nmodel a bit: at each step we’ll sample words according to their probability condi-\\ntioned on our previous choices , and we’ll use a transformer language model as the\\nprobability model that tells us this probability.\\nWe can formalize this algorithm for generating a sequence of words W = w 1\\n, w 2\\n,..., w N\\nuntil we hit the end-of-sequence token, using x ⇠ p ( x ) to mean ‘choose x by sam-'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 18, 'page_label': '19', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='We can formalize this algorithm for generating a sequence of words W = w 1\\n, w 2\\n,..., w N\\nuntil we hit the end-of-sequence token, using x ⇠ p ( x ) to mean ‘choose x by sam-\\npling from the distribution p ( x ) :\\ni  1\\nw i\\n⇠ p( w )\\nwhile w i\\n!= EOS\\ni  i+1\\nw i\\n⇠ p( w i\\n| w < i\\n)\\nThe algorithm above is called random sampling , and it turns out random sam-\\nrandom\\nsampling\\npling doesn’t work well enough. The problem is that even though random sampling\\nis mostly going to generate sensible, high-probable words, there are many odd, low-\\nprobability words in the tail of the distribution, and even though each one is low-\\nprobability, if you add up all the rare words, they constitute a large enough portion\\nof the distribution that they get chosen often enough to result in generating weird\\nsentences. For this reason, instead of random sampling, we usually use sampling\\nmethods that avoid generating the very unlikely words.\\nThe sampling methods we introduce below each have parameters that enable'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 18, 'page_label': '19', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='methods that avoid generating the very unlikely words.\\nThe sampling methods we introduce below each have parameters that enable\\ntrading off two important factors in generation: quality and diversity . Methods\\nthat emphasize the most probable words tend to produce generations that are rated\\nby people as more accurate, more coherent, and more factual, but also more boring\\nand more repetitive. Methods that give a bit more weight to the middle-probability\\nwords tend to be more creative and more diverse, but less factual and more likely to\\nbe incoherent or otherwise low-quality.\\n10.2.1 Top- k sampling\\nTop-k sampling is a simple generalization of greedy decoding. Instead of choosingtop-k sampling\\nthe single most probable word to generate, we ﬁrst truncate the distribution to the\\ntop k most likely words, renormalize to produce a legitimate probability distribution,\\nand then randomly sample from within these k words according to their renormalized\\nprobabilities. More formally:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 18, 'page_label': '19', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='top k most likely words, renormalize to produce a legitimate probability distribution,\\nand then randomly sample from within these k words according to their renormalized\\nprobabilities. More formally:\\n1. Choose in advance a number of words k\\n2. For each word in the vocabulary V , use the language model to compute the\\nlikelihood of this word given the context p ( w t\\n| w < t\\n)\\n3. Sort the words by their likelihood, and throw away any word that is not one of\\nthe top k most probable words.\\n4. Renormalize the scores of the k words to be a legitimate probability distribu-\\ntion.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 19, 'page_label': '20', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"Random sampling doesn't work very wellEven though random sampling mostly generate sensible, high-probable words, There are many odd, low- probability words in the tail of the distribution Each one is low- probability but added up they constitute a large portion of the distribution So they get picked enough to generate weird sentences\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 20, 'page_label': '21', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Factors in word sampling: quality and diversityEmphasize high-probability words  + quality: more  accurate, coherent, and factual, - diversity: boring, repetitive. Emphasize middle-probability words + diversity: more creative, diverse, - quality: less factual, incoherent'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 21, 'page_label': '22', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Top-k sampling:1. Choose # of words k 2. For each word in the vocabulary V , use the language model to compute the likelihood of this word given the context p(wt |w<t ) 3. Sort the words by likelihood, keep only the top k most probable words. 4. Renormalize the scores of the k words to be a legitimate probability distribution. 5. Randomly sample a word from within these remaining k most-probable words according to its probability.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 22, 'page_label': '23', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Top-p sampling (= nucleus sampling)Problem with top-k:  k is fixed so may cover very different amounts of probability mass in different situationsIdea: Instead, keep the top p percent of the probability massGiven a distribution P(wt |w<t ), the top-p vocabulary V ( p) is the smallest set of words such that \\nHoltzman et al., 2020 \\n10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.\\nWhen k = 1, top- k sampling is identical to greedy decoding. Setting k to a larger\\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\\nprobable, but is still probable enough, and whose choice results in generating more\\ndiverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 22, 'page_label': '23', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='One problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes\\nthe top 10 words will be very likely and include most of the probability mass, but\\nother times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling\\n2020 ), is to keep not the top k words, but the top p percent of the probability mass.\\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\\nBut by measuring probability rather than the number of words, the hope is that the\\nmeasure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w t\\n| w < t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w < t'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 22, 'page_label': '23', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='decreasing the pool of word candidates.\\nGiven a distribution P ( w t\\n| w < t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w < t\\n) \\x00 p . (10.2)\\n10.2.3 Temperature sampling\\nIn temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,\\nwhile a system at a lower temperature is likely to explore a subset of lower energy\\n(better) states. In low-temperature sampling, we smoothly increase the probability\\nof the most probable words and decrease the probability of the rare words.\\nWe implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 22, 'page_label': '23', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='eter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the\\nvocabulary directly from the logit as in the following (repeated from ( ?? )):\\ny = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.\\nBut the lower t is, the larger the scores being passed to the softmax (dividing by a\\nsmaller fraction t \\uf8ff 1 results in making each score larger). Recall that one of the\\nuseful properties of a softmax is that it tends to push high values toward 1 and low\\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 22, 'page_label': '23', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='a distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more\\ngreedy. As t approaches 0 the probability of the most likely word approaches 1.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 23, 'page_label': '24', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Temperature samplingReshape the distribution instead of truncating itIntuition from thermodynamics, •a system at high temperature is flexible and can explore many possible states,•a system at lower temperature is likely to explore a subset of lower energy (better) states. In low-temperature sampling,  (τ ≤ 1) we smoothly•increase the probability of the most probable words•decrease the probability of the rare words.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Temperature samplingDivide the logit by a temperature parameter τ before passing it through the softmax.Instead ofWe do  \\n10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.\\nWhen k = 1, top- k sampling is identical to greedy decoding. Setting k to a larger\\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\\nprobable, but is still probable enough, and whose choice results in generating more\\ndiverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes\\nthe top 10 words will be very likely and include most of the probability mass, but\\nother times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='other times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling\\n2020 ), is to keep not the top k words, but the top p percent of the probability mass.\\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\\nBut by measuring probability rather than the number of words, the hope is that the\\nmeasure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w\\nt\\n| w\\n< t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n) \\x00 p . (10.2)\\n10.2.3 Temperature sampling\\nIn temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='In temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,\\nwhile a system at a lower temperature is likely to explore a subset of lower energy\\n(better) states. In low-temperature sampling, we smoothly increase the probability\\nof the most probable words and decrease the probability of the rare words.\\nWe implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the\\nvocabulary directly from the logit as in the following (repeated from ( ?? )):\\ny = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='y = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.\\nBut the lower t is, the larger the scores being passed to the softmax (dividing by a\\nsmaller fraction t \\uf8ff 1 results in making each score larger). Recall that one of the\\nuseful properties of a softmax is that it tends to push high values toward 1 and low\\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more\\ngreedy. As t approaches 0 the probability of the most likely word approaches 1.\\n10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.\\nWhen k = 1, top- k sampling is identical to greedy decoding. Setting k to a larger\\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\\nprobable, but is still probable enough, and whose choice results in generating more\\ndiverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes\\nthe top 10 words will be very likely and include most of the probability mass, but\\nother times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='include a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling\\n2020 ), is to keep not the top k words, but the top p percent of the probability mass.\\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\\nBut by measuring probability rather than the number of words, the hope is that the\\nmeasure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w\\nt\\n| w\\n< t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n) \\x00 p . (10.2)\\n10.2.3 Temperature sampling\\nIn temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='temperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,\\nwhile a system at a lower temperature is likely to explore a subset of lower energy\\n(better) states. In low-temperature sampling, we smoothly increase the probability\\nof the most probable words and decrease the probability of the rare words.\\nWe implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the\\nvocabulary directly from the logit as in the following (repeated from ( ?? )):\\ny = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='we instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.\\nBut the lower t is, the larger the scores being passed to the softmax (dividing by a\\nsmaller fraction t \\uf8ff 1 results in making each score larger). Recall that one of the\\nuseful properties of a softmax is that it tends to push high values toward 1 and low\\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more\\ngreedy. As t approaches 0 the probability of the most likely word approaches 1.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 25, 'page_label': '26', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Temperature sampling\\nWhy does this work?•When τ is close to 1 the distribution doesn’t change much. •The lower τ is, the larger the scores being passed to the softmax•Softmax pushes high values toward 1 and low values toward 0. •Large inputs pushes high-probability words higher and low probability word lower,  making the distribution more greedy. •As τ approaches 0, the probability of most likely word approaches 1 \\n10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.\\nWhen k = 1, top- k sampling is identical to greedy decoding. Setting k to a larger\\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\\nprobable, but is still probable enough, and whose choice results in generating more\\ndiverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 25, 'page_label': '26', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='diverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes\\nthe top 10 words will be very likely and include most of the probability mass, but\\nother times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling\\n2020 ), is to keep not the top k words, but the top p percent of the probability mass.\\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\\nBut by measuring probability rather than the number of words, the hope is that the\\nmeasure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w\\nt\\n| w\\n< t\\n) , the top- p vocabulary V'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 25, 'page_label': '26', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='measure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w\\nt\\n| w\\n< t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n) \\x00 p . (10.2)\\n10.2.3 Temperature sampling\\nIn temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,\\nwhile a system at a lower temperature is likely to explore a subset of lower energy\\n(better) states. In low-temperature sampling, we smoothly increase the probability\\nof the most probable words and decrease the probability of the rare words.\\nWe implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 25, 'page_label': '26', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='We implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the\\nvocabulary directly from the logit as in the following (repeated from ( ?? )):\\ny = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.\\nBut the lower t is, the larger the scores being passed to the softmax (dividing by a\\nsmaller fraction t \\uf8ff 1 results in making each score larger). Recall that one of the\\nuseful properties of a softmax is that it tends to push high values toward 1 and low\\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 25, 'page_label': '26', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='values toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more\\ngreedy. As t approaches 0 the probability of the most likely word approaches 1.\\n0 ≤ τ ≤ 1'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 26, 'page_label': '27', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsSampling for LLM Generation'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 27, 'page_label': '28', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsPretraining Large Language Models: Algorithm'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 28, 'page_label': '29', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='PretrainingThe big idea that underlies all the amazing performance of language modelsFirst pretrain a transformer model on enormous amounts of textThen apply it to new tasks.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 29, 'page_label': '30', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Self-supervised training algorithmWe just train them to predict the next word!1.Take a corpus of text 2.At each time step t i.ask the model to predict the next word ii.train the model using gradient descent to minimize the error in this prediction\"Self-supervised\" because it just uses the next word as the label!'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 30, 'page_label': '31', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Intuition of language model training: loss•Same loss function: cross-entropy loss•We want the model to assign a high probability to true word w•= want loss to be high if the model assigns too low a probability to w•CE Loss: The negative log probability that the model assigns to the true next word w•If the model assigns too low a probability to w•We move the model weights in the direction that assigns a higher probability to w'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Cross-entropy loss for language modelingCE loss: difference between the correct probability distribution and the predicted distribution \\nThe correct distribution yt knows the next word, so is 1 for the actual next word and 0 for the others.So in this sum, all terms get multiplied by zero except one: the logp the model assigns to the correct next word, so: \\n8 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\nNote, by the way, that there can be other situations where we may want to do\\nsomething quite different and ﬂatten the word probability distribution instead of\\nmaking it greedy. Temperature sampling can help with this situation too, in this case\\nhigh-temperature sampling, in which case we use t > 1.\\n10.3 Pretraining Large Language Models\\nHow do we teach a transformer to be a language model? What is the algorithm and\\nwhat data do we train on?\\n10.3.1 Self-supervised training algorithm\\nTo train a transformer as a language model, we use the same self-supervision (orself-supervision'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='what data do we train on?\\n10.3.1 Self-supervised training algorithm\\nTo train a transformer as a language model, we use the same self-supervision (orself-supervision\\nself-training ) algorithm we saw in Section ?? : we take a corpus of text as training\\nmaterial and at each time step t ask the model to predict the next word. We call\\nsuch a model self-supervised because we don’t have to add any special gold labels\\nto the data; the natural sequence of words is its own supervision! We simply train the\\nmodel to minimize the error in predicting the true next word in the training sequence,\\nusing cross-entropy as the loss function.\\nRecall that the cross-entropy loss measures the difference between a predicted\\nprobability distribution and the correct distribution.\\nL CE\\n= \\x00\\nX\\nw 2 V\\ny t [ w ] log ˆy t [ w ] (10.5)\\nIn the case of language modeling, the correct distribution y t comes from knowing the\\nnext word. This is represented as a one-hot vector corresponding to the vocabulary'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='In the case of language modeling, the correct distribution y t comes from knowing the\\nnext word. This is represented as a one-hot vector corresponding to the vocabulary\\nwhere the entry for the actual next word is 1, and all the other entries are 0. Thus,\\nthe cross-entropy loss for language modeling is determined by the probability the\\nmodel assigns to the correct next word (all other words get multiplied by zero). So\\nat time t the CE loss in ( 10.5 ) can be simpliﬁed as the negative log probability the\\nmodel assigns to the next word in the training sequence.\\nL CE\\n( ˆy t , y t )= \\x00 log ˆy t [ w t + 1\\n] (10.6)\\nThus at each word position t of the input, the model takes as input the correct se-\\nquence of tokens w 1: t\\n, and uses them to compute a probability distribution over\\npossible next words so as to compute the model’s loss for the next token w t + 1\\n. Then\\nwe move to the next word, we ignore what the model predicted for the next word'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='possible next words so as to compute the model’s loss for the next token w t + 1\\n. Then\\nwe move to the next word, we ignore what the model predicted for the next word\\nand instead use the correct sequence of tokens w 1: t + 1\\nto estimate the probability of\\ntoken w t + 2\\n. This idea that we always give the model the correct history sequence to\\npredict the next word (rather than feeding the model its best case from the previous\\ntime step) is called teacher forcing .teacher forcing\\nFig. 10.4 illustrates the general training approach. At each step, given all the\\npreceding words, the ﬁnal transformer layer produces an output distribution over\\nthe entire vocabulary. During training, the probability assigned to the correct word\\nis used to calculate the cross-entropy loss for each item in the sequence. The loss\\nfor a training sequence is the average cross-entropy loss over the entire sequence.\\nThe weights in the network are adjusted to minimize the average CE loss over the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='for a training sequence is the average cross-entropy loss over the entire sequence.\\nThe weights in the network are adjusted to minimize the average CE loss over the\\ntraining sequence via gradient descent.\\n8 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\nNote, by the way, that there can be other situations where we may want to do\\nsomething quite different and ﬂatten the word probability distribution instead of\\nmaking it greedy. Temperature sampling can help with this situation too, in this case\\nhigh-temperature sampling, in which case we use t > 1.\\n10.3 Pretraining Large Language Models\\nHow do we teach a transformer to be a language model? What is the algorithm and\\nwhat data do we train on?\\n10.3.1 Self-supervised training algorithm\\nTo train a transformer as a language model, we use the same self-supervision (orself-supervision\\nself-training ) algorithm we saw in Section ?? : we take a corpus of text as training\\nmaterial and at each time step t ask the model to predict the next word. We call'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='self-training ) algorithm we saw in Section ?? : we take a corpus of text as training\\nmaterial and at each time step t ask the model to predict the next word. We call\\nsuch a model self-supervised because we don’t have to add any special gold labels\\nto the data; the natural sequence of words is its own supervision! We simply train the\\nmodel to minimize the error in predicting the true next word in the training sequence,\\nusing cross-entropy as the loss function.\\nRecall that the cross-entropy loss measures the difference between a predicted\\nprobability distribution and the correct distribution.\\nL CE\\n= \\x00\\nX\\nw 2 V\\ny t\\n[ w ] log ˆy t\\n[ w ] (10.5)\\nIn the case of language modeling, the correct distribution y t\\ncomes from knowing the\\nnext word. This is represented as a one-hot vector corresponding to the vocabulary\\nwhere the entry for the actual next word is 1, and all the other entries are 0. Thus,\\nthe cross-entropy loss for language modeling is determined by the probability the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='where the entry for the actual next word is 1, and all the other entries are 0. Thus,\\nthe cross-entropy loss for language modeling is determined by the probability the\\nmodel assigns to the correct next word (all other words get multiplied by zero). So\\nat time t the CE loss in ( 10.5 ) can be simpliﬁed as the negative log probability the\\nmodel assigns to the next word in the training sequence.\\nL CE\\n( ˆy t\\n, y t\\n)= \\x00 log ˆy t\\n[ w t + 1\\n] (10.6)\\nThus at each word position t of the input, the model takes as input the correct se-\\nquence of tokens w 1: t\\n, and uses them to compute a probability distribution over\\npossible next words so as to compute the model’s loss for the next token w t + 1\\n. Then\\nwe move to the next word, we ignore what the model predicted for the next word\\nand instead use the correct sequence of tokens w 1: t + 1\\nto estimate the probability of\\ntoken w t + 2\\n. This idea that we always give the model the correct history sequence to'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='and instead use the correct sequence of tokens w 1: t + 1\\nto estimate the probability of\\ntoken w t + 2\\n. This idea that we always give the model the correct history sequence to\\npredict the next word (rather than feeding the model its best case from the previous\\ntime step) is called teacher forcing .teacher forcing\\nFig. 10.4 illustrates the general training approach. At each step, given all the\\npreceding words, the ﬁnal transformer layer produces an output distribution over\\nthe entire vocabulary. During training, the probability assigned to the correct word\\nis used to calculate the cross-entropy loss for each item in the sequence. The loss\\nfor a training sequence is the average cross-entropy loss over the entire sequence.\\nThe weights in the network are adjusted to minimize the average CE loss over the\\ntraining sequence via gradient descent.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 32, 'page_label': '33', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Teacher forcing•At each token position t, model sees correct tokens w1:t, •Computes  loss (–log probability) for the next token wt+1 •At next token position t+1 we ignore what model predicted for wt+1 •Instead we take the correct word wt+1, add it to context, move on'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 33, 'page_label': '34', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Training a transformer language model\\nlong and thanks forNext token all\\nLoss\\n…\\n=\\n<latexit sha1_base64=\"AovqpaL476UmJ1EU1xZPgDZ70tQ=\">AAAB9nicbVDLSsNAFL2pr1pfURcu3AwWwY0lEakui25cVrAPaEqYTCbt0EkmzEzEEvIrbkTcKPgZ/oJ/Y9Jm09YDA4dzznDvPV7MmdKW9WtU1tY3Nreq27Wd3b39A/PwqKtEIgntEMGF7HtYUc4i2tFMc9qPJcWhx2nPm9wXfu+ZSsVE9KSnMR2GeBSxgBGsc8k1Ty4dLkZo6qZOiPVYhimO/CyruWbdalgzoFVil6QOJdqu+eP4giQhjTThWKmBbcV6mGKpGeE0qzmJojEmEzyi6WztDJ3nko8CIfMXaTRTF3I4VGoaenmy2E0te4X4nzdIdHA7TFkUJ5pGZD4oSDjSAhUdIJ9JSjSf5gQTyfINERljiYnOmypOt5cPXSXdq4bdbDQfr+utu7KEKpzCGVyADTfQggdoQwcIZPAGn/BlvBivxrvxMY9WjPLPMSzA+P4DPEiSHA==</latexit>\\n\\x00 log y and\\nStacked\\nTransformer\\nBlocks\\nSo long and thanks for\\n…\\n…\\n…\\nU\\nInput tokens\\nx1 x2\\nLanguage\\nModeling\\nHead\\nx3 x4 x5\\nInput\\nEncoding\\n E\\n1+\\nE\\n2+\\nE\\n3+\\nE\\n4+\\nE\\n5+\\n…\\n… ………\\nU\\n U\\n U\\n U\\n…\\nlogits logits logits logits logits\\n…'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 33, 'page_label': '34', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Transformer\\nBlocks\\nSo long and thanks for\\n…\\n…\\n…\\nU\\nInput tokens\\nx1 x2\\nLanguage\\nModeling\\nHead\\nx3 x4 x5\\nInput\\nEncoding\\n E\\n1+\\nE\\n2+\\nE\\n3+\\nE\\n4+\\nE\\n5+\\n…\\n… ………\\nU\\n U\\n U\\n U\\n…\\nlogits logits logits logits logits\\n…\\n<latexit sha1_base64=\"q3ZgXDyG7qtkT7t8hT47RdlwYG4=\">AAAB+XicbVDLSsNAFJ3UV62vWHe6GVsEN5bERXUlBUVcVrAPaEqYTCft0MlMmJkIIQT8AT/CTRE3Cv6Ev+DfmLTdtPXAwOGcM9x7jxcyqrRl/RqFtfWNza3idmlnd2//wDwst5WIJCYtLJiQXQ8pwignLU01I91QEhR4jHS88W3ud56JVFTwJx2HpB+gIac+xUhnkmseXzhMDGHsJk6A9EgGiR4hPlZpWnLNqlWzpoCrxJ6TauP0tXw3qdw0XfPHGQgcBYRrzJBSPdsKdT9BUlPMSFpyIkVChMdoSJLp5ik8y6QB9IXMHtdwqi7kUKBUHHhZMl9PLXu5+J/Xi7R/3U8oDyNNOJ4N8iMGtYB5DXBAJcGaxRlBWNJsQ4hHSCKss7Ly0+3lQ1dJ+7Jm12v1x6yDezBDEZyACjgHNrgCDfAAmqAFMHgBE/AJvozEeDPejY9ZtGDM/xyBBRjff79pldo=</latexit>\\n\\x00 log y thanks'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 34, 'page_label': '35', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsPretraining Large Language Models: Algorithm'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 35, 'page_label': '36', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsPretraining data for LLMs'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 36, 'page_label': '37', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"LLMs are mainly trained on the webCommon crawl, snapshots of the entire web produced by the non- profit Common Crawl with billions of pagesColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156 billion tokens of English,  filtered What's in it? Mostly patent text documents, Wikipedia, and news sites\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 37, 'page_label': '38', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='The Pile: a pretraining corpus\\nFigure 1: Treemap of Pile components by effective size.\\ntroduce a new ﬁltered subset of Common Crawl,\\nPile-CC, with improved extraction quality.\\nThrough our analyses, we conﬁrm that the Pile is\\nsigniﬁcantly distinct from pure Common Crawl\\ndata. Additionally, our evaluations show that the\\nexisting GPT-2 and GPT-3 models perform poorly\\non many components of the Pile, and that models\\ntrained on the Pile signiﬁcantly outperform both\\nraw and ﬁltered Common Crawl models. To com-\\nplement the performance evaluations, we also per-\\nform an exploratory analysis of the text within the\\nPile to provide a detailed picture of the data. We\\nhope that our extensive documentation of the con-\\nstruction and characteristics of the Pile will help\\nresearchers make informed decisions about poten-\\ntial downstream applications.\\nFinally, we make publicly available the preprocess-\\ning code for the constituent datasets of the Pile and\\nthe code for constructing alternative versions 2'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 37, 'page_label': '38', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='tial downstream applications.\\nFinally, we make publicly available the preprocess-\\ning code for the constituent datasets of the Pile and\\nthe code for constructing alternative versions 2\\n. In\\nthe interest of reproducibility, we also document\\nall processing performed on each dataset (and the\\nPile as a whole) in as much detail as possible. For\\nfurther details about the processing of each dataset,\\nsee Section 2 and Appendix C .\\n2\\nhttps://github.com/EleutherAI/\\nthe-pile\\n1.1 Contributions\\nThe core contributions of this paper are:\\n1. The introduction of a 825 . 18 GiB english-\\nlanguage dataset for language modeling com-\\nbining 22 diverse sources.\\n2. The introduction of 14 new language model-\\ning datasets, which we expect to be of inde-\\npendent interest to researchers.\\n3. Evaluations demonstrating signiﬁcant im-\\nprovements across many domains by GPT-2-\\nsized models trained on this new dataset, com-\\npared to training on CC-100 and raw Common\\nCrawl.\\n4. The investigation and documentation of this'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 37, 'page_label': '38', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='provements across many domains by GPT-2-\\nsized models trained on this new dataset, com-\\npared to training on CC-100 and raw Common\\nCrawl.\\n4. The investigation and documentation of this\\ndataset, which we hope will better inform re-\\nsearchers about how to use it as well as moti-\\nvate them to undertake similar investigations\\nof their own data.\\n2 The Pile Datasets\\nThe Pile is composed of 22 constituent sub-datasets,\\nas shown in Table 1 . Following Brown et al. ( 2020 ),\\nwe increase the weights of higher quality compo-\\nnents, with certain high-quality datasets such as\\nWikipedia being seen up to 3 times (“epochs”) for\\n2\\nwebacademics books\\ndialog'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 38, 'page_label': '39', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Filtering for quality and safetyQuality is subjective•Many LLMs attempt to match Wikipedia, books, particular websites•Need to remove boilerplate, adult content•Deduplication at many levels (URLs, documents, even lines)Safety also subjective•Toxicity detection is important, although that has mixed results•Can mistakenly flag data written in dialects like African American English'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 39, 'page_label': '40', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='What does a model learn from pretraining?•There are canines everywhere! One dog in the front room, and two dogs•It wasn\\'t just big it was enormous•The author of \"A Room of One\\'s Own\" is Virginia Woolf•The doctor told me that he•The square root of 4 is 2'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 40, 'page_label': '41', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Big ideaText contains enormous amounts of knowledgePretraining on lots of text with all that knowledge is what gives language models their ability to do so much'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 41, 'page_label': '42', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"But there are problems with scraping from the webCopyright: much of the text in these datasets is copyrighted•Not clear if fair use doctrine in US allows for this use•This remains an open legal questionData consent•Website owners can indicate they don't want their site crawledPrivacy: •Websites can contain private IP addresses and phone numbers\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 42, 'page_label': '43', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsPretraining data for LLMs'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 43, 'page_label': '44', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsFinetuning'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 44, 'page_label': '45', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"Finetuning for daptation to new domainsWhat happens if we need our LLM to work well on a domain it didn't see in pretraining?Perhaps some specific medical or legal domain?Or maybe a multilingual LM needs to see more data on some language that was rare in pretraining?\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 45, 'page_label': '46', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='FinetuningFine-\\ntuning \\nData\\nPretraining Data\\nPretraining\\n…\\n …\\n …\\nFine-tuning\\n…\\n …\\n …\\nPretrained LM Fine-tuned LM'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 46, 'page_label': '47', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='\"Finetuning\" means 4 different thingsWe\\'ll discuss 1 here, and 3 in later lecturesIn all four cases, finetuning means:taking a pretrained model and further adapting some or all of its parameters to some new data'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 47, 'page_label': '48', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='1. Finetuning as \"continued pretraining\" on new data•Further train all the parameters of model on new data•using the same method (word prediction) and loss function (cross-entropy loss) as for pretraining.•as if the new data were at the tail end of the pretraining data•Hence sometimes called continued pretraining'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 48, 'page_label': '49', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsFinetuning'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 49, 'page_label': '50', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsEvaluating Large Language Models'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 50, 'page_label': '51', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='PerplexityJust as for n-gram grammars, we use perplexity to measure how well the LM predicts unseen textThe perplexity of a model θ on an unseen test set is the inverse probability that θ assigns to the test set, normalized by the test set length. For a test set of n tokens w1:n the perplexity is :\\n12 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\nthe pretraining data, and so you’ll sometimes see this method called continued pre-\\ntraining .\\ncontinued\\npretraining\\nRetraining all the parameters of the model is very slow and expensive when the\\nlanguage model is huge. So instead we can freeze some of the parameters (i.e., leavefreeze\\nthem unchanged from their pretrained value) and train only a subset of parameters\\non the new data. In Section 10.5.3 we’ll describe this second variety of ﬁnetun-\\ning, called parameter-efﬁcient ﬁnetuning , or PEFT . because we efﬁciently select\\nspeciﬁc parameters to update when ﬁnetuning, and leave the rest in their pretrained\\nvalues.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 50, 'page_label': '51', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='ing, called parameter-efﬁcient ﬁnetuning , or PEFT . because we efﬁciently select\\nspeciﬁc parameters to update when ﬁnetuning, and leave the rest in their pretrained\\nvalues.\\nIn Chapter 11 we’ll introduce a third kind of ﬁnetuning, also parameter-efﬁcient.\\nIn this version, the goal is to use a language model as a kind of classiﬁer or labeler\\nfor a speciﬁc task. For example we might train the model to be a sentiment classiﬁer.\\nWe do this by adding extra neural circuitry (an extra head ) after the top layer of the\\nmodel. This classiﬁcation head takes as input some of the top layer embeddings of\\nthe transformer and produces as output a classiﬁcation. In this method, most com-\\nmonly used with masked language models like BERT, we freeze the entire pretrained\\nmodel and only train the classiﬁcation head on some new data, usually labeled with\\nsome class that we want to predict.\\nFinally, in Chapter 12 we’ll introduce a fourth kind of ﬁnetuning, that is a cru-'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 50, 'page_label': '51', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='some class that we want to predict.\\nFinally, in Chapter 12 we’ll introduce a fourth kind of ﬁnetuning, that is a cru-\\ncial component of the largest language models: supervised ﬁnetuning or SFT . SFT\\nis often used for instruction ﬁnetuning , in which we want a pretrained language\\nmodel to learn to follow text instructions, for example to answer questions or follow\\na command to write something. Here we create a dataset of prompts and desired\\nresponses (for example questions and their answers, or commands and their ful-\\nﬁllments), and we train the language model using the normal cross-entropy loss to\\npredict each token in the instruction prompt iteratively, essentially training it to pro-\\nduce the desired response from the command in the prompt. It’s called supervised\\nbecause unlike in pretraining, where we just take any data and predict the words in\\nit, we build the special ﬁnetuning dataset by hand, creating supervised responses to\\neach command.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 50, 'page_label': '51', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='because unlike in pretraining, where we just take any data and predict the words in\\nit, we build the special ﬁnetuning dataset by hand, creating supervised responses to\\neach command.\\nOften everything that happens after pretraining is lumped together as post-training ;\\nwe’ll discuss the various parts of post-training in Chapter 12 and Chapter 13.\\n10.4 Evaluating Large Language Models\\nPerplexity As we ﬁrst saw in Chapter 3, one way to evaluate language models is\\nto measure how well they predict unseen text. Intuitively, good models are those that\\nassign higher probabilities to unseen data (are less surprised when encountering the\\nnew words).\\nWe instantiate this intuition by using perplexity to measure the quality of aperplexity\\nlanguage model. Recall from page ?? that the perplexity of a model q on an unseen\\ntest set is the inverse probability that q assigns to the test set, normalized by the test\\nset length. For a test set of n tokens w 1: n , the perplexity is\\nPerplexity\\nq'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 50, 'page_label': '51', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='test set is the inverse probability that q assigns to the test set, normalized by the test\\nset length. For a test set of n tokens w 1: n , the perplexity is\\nPerplexity\\nq\\n( w 1: n )= P q ( w 1: n )\\n\\x00\\n1\\nn\\n=\\nn\\ns\\n1\\nP q ( w 1: n )\\n(10.7)\\nTo visualize how perplexity can be computed as a function of the probabilities the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 51, 'page_label': '52', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='•Probability depends on size of test set•Probability gets smaller the longer the text•Better: a metric that is per-word, normalized by length•Perplexity is the inverse probability of the test set, normalized by the number of words(The inverse comes from the original definition of perplexity from cross-entropy rate in information theory)Probability range is  [0,1], perplexity range is [1,∞]\\nWhy perplexity instead of raw probability of the test set?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 52, 'page_label': '53', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Perplexity•The higher the probability of the word sequence, the lower the perplexity.•Thus the lower the perplexity of a model on the data, the better the model. •Minimizing perplexity is the same as maximizing probabilityAlso: perplexity is sensitive to length/tokenization so best used when comparing LMs that use the same tokenizer.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 53, 'page_label': '54', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Many other factors that we evaluate, like:Size Big models take lots of GPUs and time to train, memory to storeEnergy usageCan measure kWh or kilograms of CO2 emitted FairnessBenchmarks measure gendered and racial stereotypes, or decreased performance for language from or about some groups.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 54, 'page_label': '55', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsDealing with Scale'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 55, 'page_label': '56', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Scaling LawsLLM performance depends on•Model size: the number of parameters not counting embeddings•Dataset size: the amount of training data•Compute: Amount of compute (in FLOPS or etcCan improve a model by adding  parameters (more layers, wider contexts), more data, or training for more iterationsThe performance of a large language model (the loss) scales as a power-law with each of these three'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 56, 'page_label': '57', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Scaling LawsLoss L as a function of # parameters N, dataset size D, compute budget C (if other two are held constant)\\n14 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\n10.5 Dealing with Scale\\nLarge language models are large. For example the Llama 3.1 405B Instruct model\\nfrom Meta has 405 billion parameters (126 layers, a model dimensionality of 16,384,\\n128 attention heads) and was trained on 15.6 terabytes of text tokens ( Llama Team ,\\n2024 ), using a vocabulary of 128K tokens. So there is a lot of research on un-\\nderstanding how LLMs scale, and especially how to implement them given limited\\nresources. In the next few sections we discuss how to think about scale (the concept\\nof scaling laws ), and important techniques for getting language models to work\\nefﬁciently, such as the KV cache and parameter-efﬁcient ﬁne tuning.\\n10.5.1 Scaling laws\\nThe performance of large language models has shown to be mainly determined by'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 56, 'page_label': '57', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='efﬁciently, such as the KV cache and parameter-efﬁcient ﬁne tuning.\\n10.5.1 Scaling laws\\nThe performance of large language models has shown to be mainly determined by\\n3 factors: model size (the number of parameters not counting embeddings), dataset\\nsize (the amount of training data), and the amount of compute used for training. That\\nis, we can improve a model by adding parameters (adding more layers or having\\nwider contexts or both), by training on more data, or by training for more iterations.\\nThe relationships between these factors and performance are known as scaling\\nlaws . Roughly speaking, the performance of a large language model (the loss) scalesscaling laws\\nas a power-law with each of these three properties of model training.\\nFor example, Kaplan et al. ( 2020 ) found the following three relationships for\\nloss L as a function of the number of non-embedding parameters N , the dataset size\\nD , and the compute budget C , for models training with limited parameters, dataset,'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 56, 'page_label': '57', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='loss L as a function of the number of non-embedding parameters N , the dataset size\\nD , and the compute budget C , for models training with limited parameters, dataset,\\nor compute budget, if in each case the other two properties are held constant:\\nL ( N )=\\n✓\\nN c\\nN\\n◆\\na N\\n(10.9)\\nL ( D )=\\n✓\\nD c\\nD\\n◆\\na D\\n(10.10)\\nL ( C )=\\n✓\\nC c\\nC\\n◆\\na\\nC\\n(10.11)\\nThe number of (non-embedding) parameters N can be roughly computed as fol-\\nlows (ignoring biases, and with d as the input and output dimensionality of the\\nmodel, d attn\\nas the self-attention layer size, and d ff\\nthe size of the feedforward layer):\\nN ⇡ 2 dn layer\\n( 2 d attn\\n+ d ff\\n)\\n⇡ 12 n layer\\nd\\n2\\n(10.12)\\n( assuming d attn\\n= d ff\\n/ 4 = d )\\nThus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 ⇥ 96 ⇥\\n12288\\n2\\n⇡ 175 billion parameters.\\nThe values of N c\\n, D c\\n, C c\\n, a N\\n, a D\\n, and a C\\ndepend on the exact transformer\\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 56, 'page_label': '57', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='2\\n⇡ 175 billion parameters.\\nThe values of N c\\n, D c\\n, C c\\n, a N\\n, a D\\n, and a C\\ndepend on the exact transformer\\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,\\nscaling laws focus on the relationship with loss.\\n2\\nScaling laws can be useful in deciding how to train a model to a particular per-\\nformance, for example by looking at early in the training curve, or performance with\\n2\\nFor the initial experiment in Kaplan et al. ( 2020 ) the precise values were a N\\n= 0.076, N c\\n= 8.8 ⇥ 10\\n13\\n(parameters), a D\\n= 0.095, D c\\n= 5.4 ⇥ 10\\n13\\n(tokens), a C\\n= 0.050, C c\\n= 3.1 ⇥ 10\\n8\\n(petaﬂop-days).\\nScaling laws can be used early in training to predict what the loss would be if we were to add more data or increase model size.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 57, 'page_label': '58', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Number of non-embedding parameters N\\n14 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\n10.5 Dealing with Scale\\nLarge language models are large. For example the Llama 3.1 405B Instruct model\\nfrom Meta has 405 billion parameters (126 layers, a model dimensionality of 16,384,\\n128 attention heads) and was trained on 15.6 terabytes of text tokens ( Llama Team ,\\n2024 ), using a vocabulary of 128K tokens. So there is a lot of research on un-\\nderstanding how LLMs scale, and especially how to implement them given limited\\nresources. In the next few sections we discuss how to think about scale (the concept\\nof scaling laws ), and important techniques for getting language models to work\\nefﬁciently, such as the KV cache and parameter-efﬁcient ﬁne tuning.\\n10.5.1 Scaling laws\\nThe performance of large language models has shown to be mainly determined by\\n3 factors: model size (the number of parameters not counting embeddings), dataset'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 57, 'page_label': '58', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='10.5.1 Scaling laws\\nThe performance of large language models has shown to be mainly determined by\\n3 factors: model size (the number of parameters not counting embeddings), dataset\\nsize (the amount of training data), and the amount of compute used for training. That\\nis, we can improve a model by adding parameters (adding more layers or having\\nwider contexts or both), by training on more data, or by training for more iterations.\\nThe relationships between these factors and performance are known as scaling\\nlaws . Roughly speaking, the performance of a large language model (the loss) scalesscaling laws\\nas a power-law with each of these three properties of model training.\\nFor example, Kaplan et al. ( 2020 ) found the following three relationships for\\nloss L as a function of the number of non-embedding parameters N , the dataset size\\nD , and the compute budget C , for models training with limited parameters, dataset,'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 57, 'page_label': '58', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='loss L as a function of the number of non-embedding parameters N , the dataset size\\nD , and the compute budget C , for models training with limited parameters, dataset,\\nor compute budget, if in each case the other two properties are held constant:\\nL ( N )=\\n✓\\nN c\\nN\\n◆\\na N\\n(10.9)\\nL ( D )=\\n✓\\nD c\\nD\\n◆\\na D\\n(10.10)\\nL ( C )=\\n✓\\nC c\\nC\\n◆\\na\\nC\\n(10.11)\\nThe number of (non-embedding) parameters N can be roughly computed as fol-\\nlows (ignoring biases, and with d as the input and output dimensionality of the\\nmodel, d attn\\nas the self-attention layer size, and d ff\\nthe size of the feedforward layer):\\nN ⇡ 2 dn layer\\n( 2 d attn\\n+ d ff\\n)\\n⇡ 12 n layer\\nd\\n2\\n(10.12)\\n( assuming d attn\\n= d ff\\n/ 4 = d )\\nThus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 ⇥ 96 ⇥\\n12288\\n2\\n⇡ 175 billion parameters.\\nThe values of N c\\n, D c\\n, C c\\n, a N\\n, a D\\n, and a C\\ndepend on the exact transformer\\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 57, 'page_label': '58', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='2\\n⇡ 175 billion parameters.\\nThe values of N c\\n, D c\\n, C c\\n, a N\\n, a D\\n, and a C\\ndepend on the exact transformer\\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,\\nscaling laws focus on the relationship with loss.\\n2\\nScaling laws can be useful in deciding how to train a model to a particular per-\\nformance, for example by looking at early in the training curve, or performance with\\n2\\nFor the initial experiment in Kaplan et al. ( 2020 ) the precise values were a N\\n= 0.076, N c\\n= 8.8 ⇥ 10\\n13\\n(parameters), a D\\n= 0.095, D c\\n= 5.4 ⇥ 10\\n13\\n(tokens), a C\\n= 0.050, C c\\n= 3.1 ⇥ 10\\n8\\n(petaﬂop-days).\\nThus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 × 96 × 122882 ≈ 175 billion parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 58, 'page_label': '59', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"KV CacheIn training, we can compute attention very efficiently in parallel:\\nBut not at inference! We generate the next tokens one at a time!For a new token x, need to multiply by WQ, WK, and WV to get query, key, valuesBut don't want to recompute the key and value vectors for all the prior tokens x<iInstead, store key and value vectors in memory in the KV cache, and then we can just grab them from the cache \\n10.5 • D EALING WITH S CALE 15\\nsmaller amounts of data, to predict what the loss would be if we were to add more\\ndata or increase model size. Other aspects of scaling laws can also tell us how much\\ndata we need to add when scaling up a model.\\n10.5.2 KV Cache\\nWe saw in Fig. ?? and in Eq. ?? (repeated below) how the attention vector can be\\nvery efﬁciently computed in parallel for training, via two matrix multiplications:\\nA = softmax\\n✓\\nQK\\n|\\np\\nd k\\n◆\\nV (10.13)\\nUnfortunately we can’t do quite the same efﬁcient computation in inference as\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 58, 'page_label': '59', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='very efﬁciently computed in parallel for training, via two matrix multiplications:\\nA = softmax\\n✓\\nQK\\n|\\np\\nd k\\n◆\\nV (10.13)\\nUnfortunately we can’t do quite the same efﬁcient computation in inference as\\nin training. That’s because at inference time, we iteratively generate the next tokens\\none at a time. For a new token that we have just generated, call it x i , we need to\\ncompute its query, key, and values by multiplying by W\\nQ\\n, W\\nK\\n, and W\\nV\\nrespec-\\ntively. But it would be a waste of computation time to recompute the key and value\\nvectors for all the prior tokens x < i ; at prior steps we already computed these key\\nand value vectors! So instead of recomputing these, whenever we compute the key\\nand value vectors we store them in memory in the KV cache , and then we can justKV cache\\ngrab them from the cache when we need them. Fig. 10.7 modiﬁes Fig. ?? to show\\nthe computation that takes place for a single new token, showing which values we\\ncan take from the cache rather than recompute.\\nq4'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 58, 'page_label': '59', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='the computation that takes place for a single new token, showing which values we\\ncan take from the cache rather than recompute.\\nq4\\nk1\\nk2\\nk4\\nQ\\nKT\\nQKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq4•k1q4•k2q4•k3q4•k4\\nx = =x\\na4\\nA\\n1 x dk\\ndk x N\\n1 x N N x dv 1 x dv\\nk3\\nFigure 10.7 Parts of the attention computation (extracted from Fig. ?? ) showing, in black,\\nthe vectors that can be stored in the cache rather than recomputed when computing the atten-\\ntion score for the 4th token.\\n10.5.3 Parameter Efﬁcient Fine Tuning\\nAs we mentioned above, it’s very common to take a language model and give it more\\ninformation about a new domain by ﬁnetuning it (continuing to train it to predict\\nupcoming words) on some additional data.\\nFine-tuning can be very difﬁcult with very large language models, because there\\nare enormous numbers of parameters to train; each pass of batch gradient descent\\nhas to backpropagate through many many huge layers. This makes ﬁnetuning huge'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 58, 'page_label': '59', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='are enormous numbers of parameters to train; each pass of batch gradient descent\\nhas to backpropagate through many many huge layers. This makes ﬁnetuning huge\\nlanguage models extremely expensive in processing power, in memory, and in time.\\nFor this reason, there are alternative methods that allow a model to be ﬁnetuned\\nwithout changing all the parameters. Such methods are called parameter-efﬁcient\\nﬁne tuning or sometimes PEFT , because we efﬁciently select a subset of parameters\\nparameter-\\nefﬁcient ﬁne\\ntuning\\nPEFT to update when ﬁnetuning. For example we freeze some of the parameters (don’t\\nchange them), and only update some particular subset of parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 59, 'page_label': '60', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='KV Cache\\nq4\\nk1\\nk2\\nk4\\nQ\\nKT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq4•k1q4•k2q4•k3q4•k4\\nx = =x\\na4\\nA\\n1 x dk\\ndk x N\\n1 x N N x dv 1 x dv\\nk3\\nq1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2q4•k3q4•k4\\nq3•k2q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1q2•k2\\nq4•k1q4•k2q4•k3q4•k4\\nq3•k1q3•k2q3•k3\\nq1•k2\\nq2•k3\\nq1•k3\\nq3•k4\\nq2•k4\\nq1•k4x =\\nQKT masked\\nmask =\\nq1•k1\\nq2•k1\\nq4•k1\\nq3•k1\\nq1•k1q1•k1\\n=x\\na1\\na2\\na3\\na4\\nA\\nQuery \\nToken 1\\nQuery \\nToken 2\\nQuery \\nToken 3\\nQuery \\nToken 4\\nQ\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nx\\nWQ\\n=\\nValue \\nToken 1\\nValue \\nToken 2\\nValue \\nToken 3\\nValue \\nToken 4\\nV\\nx\\nWV\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nKey \\nToken 1\\nKey \\nToken 2\\nKey \\nToken 3\\nKey \\nToken 4\\nK\\nx\\nWK\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nN x dk\\ndk x N\\nN x N N x N N x dv N x dv\\nd x dk d x dk d x dvN x d N x dk N x d N x dk N x d N x dv\\nq1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2q4•k3q4•k4\\nq3•k2q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1q2•k2\\nq4•k1q4•k2q4•k3q4•k4'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 59, 'page_label': '60', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='d x dk d x dk d x dvN x d N x dk N x d N x dk N x d N x dv\\nq1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2q4•k3q4•k4\\nq3•k2q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1q2•k2\\nq4•k1q4•k2q4•k3q4•k4\\nq3•k1q3•k2q3•k3\\nq1•k2\\nq2•k3\\nq1•k3\\nq3•k4\\nq2•k4\\nq1•k4x =\\nQKT masked\\nmask =\\nq1•k1\\nq2•k1\\nq4•k1\\nq3•k1\\nq1•k1q1•k1\\n=x\\na1\\na2\\na3\\na4\\nA\\nQuery \\nToken 1\\nQuery \\nToken 2\\nQuery \\nToken 3\\nQuery \\nToken 4\\nQ\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nx\\nWQ\\n=\\nValue \\nToken 1\\nValue \\nToken 2\\nValue \\nToken 3\\nValue \\nToken 4\\nV\\nx\\nWV\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nKey \\nToken 1\\nKey \\nToken 2\\nKey \\nToken 3\\nKey \\nToken 4\\nK\\nx\\nWK\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nN x dk\\ndk x N\\nN x N N x N N x dv N x dv\\nd x dk d x dk d x dvN x d N x dk N x d N x dk N x d N x dv\\nq1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2q4•k3q4•k4\\nq3•k2q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1q2•k2\\nq4•k1q4•k2q4•k3q4•k4\\nq3•k1q3•k2q3•k3\\nq1•k2\\nq2•k3\\nq1•k3\\nq3•k4\\nq2•k4\\nq1•k4x ='),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 59, 'page_label': '60', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='q1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2q4•k3q4•k4\\nq3•k2q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1q2•k2\\nq4•k1q4•k2q4•k3q4•k4\\nq3•k1q3•k2q3•k3\\nq1•k2\\nq2•k3\\nq1•k3\\nq3•k4\\nq2•k4\\nq1•k4x =\\nQKT masked\\nmask =\\nq1•k1\\nq2•k1\\nq4•k1\\nq3•k1\\nq1•k1q1•k1\\n=x\\na1\\na2\\na3\\na4\\nA\\nQuery \\nToken 1\\nQuery \\nToken 2\\nQuery \\nToken 3\\nQuery \\nToken 4\\nQ\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nx\\nWQ\\n=\\nValue \\nToken 1\\nValue \\nToken 2\\nValue \\nToken 3\\nValue \\nToken 4\\nV\\nx\\nWV\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nKey \\nToken 1\\nKey \\nToken 2\\nKey \\nToken 3\\nKey \\nToken 4\\nK\\nx\\nWK\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nN x dk\\ndk x N\\nN x N N x N N x dv N x dv\\nd x dk d x dk d x dvN x d N x dk N x d N x dk N x d N x dv'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 60, 'page_label': '61', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Parameter-Efficient FinetuningAdapting to a new domain by continued pretraining (finetuning) is a problem with huge LLMs.•Enormous numbers of parameters to train •Each pass of batch gradient descent has to backpropagate through many many huge layers. •Expensive in processing power, in memory, and in time. Instead, parameter-efficient fine tuning (PEFT)•Efficiently select a subset of parameters to update when finetuning.•E.g., freeze some of the parameters (don’t change them), •And only update some a few parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 61, 'page_label': '62', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='LoRA (Low-Rank Adaptation)•Trransformers have many dense matrix multiply layers•Like WQ, WK, WV, WO layers in attention•Instead of updating these layers during finetuning, •Freeze these layers •Update a low-rank approximation with fewer parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 62, 'page_label': '63', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='LoRA•Consider a matrix W (shape [N × d])  that needs to be updated during finetuning via gradient descent. •Normally updates are ∆W  (shape [N × d])•In LoRA, we freeze W and update instead a low-rank decomposition of W:•A of shape [N×r], •B of shape [r×d], r is very small  (like 1 or 2)•That is, during  finetuning we update A and B instead of W. •Replace W + ∆W with W + BA. Forward pass: instead of     h = xW We do     h = xW + xAB'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 63, 'page_label': '64', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='LoRAh\\nPretrained \\nWeights\\nW\\nd\\nk r\\nk\\nA\\nBr\\nx\\nd\\n1\\n1\\nk\\nd\\n×'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 64, 'page_label': '65', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsDealing with Scale'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 65, 'page_label': '66', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsHarms of Large Language Models'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 66, 'page_label': '67', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Hallucination'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 67, 'page_label': '68', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Copyright'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 68, 'page_label': '69', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Privacy'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 69, 'page_label': '70', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Toxicity and Abuse'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 70, 'page_label': '71', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Misinformation'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 71, 'page_label': '72', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsHarms of Large Language Models')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c601f365",
   "metadata": {},
   "source": [
    "embedding And vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78cea334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64777ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x26791deba10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e0e4d7",
   "metadata": {},
   "source": [
    "VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c07ea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x26791f13b60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "219fe0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 0, 'page_label': '1', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsIntroduction to Large Language Models'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 1, 'page_label': '2', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Language models•Remember the simple n-gram language model•Assigns probabilities to sequences of words•Generate text by sampling possible next words•Is trained on counts computed from lots of text•Large language models are similar and different:•Assigns probabilities to sequences of words•Generate text by sampling possible next words•Are trained by learning to guess the next word'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 2, 'page_label': '3', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large language models•Even through pretrained only to predict words•Learn a lot of useful language knowledge•Since training on a lot of text'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 3, 'page_label': '4', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Three architectures for large language models\\nDecoders   Encoders     Encoder-decodersGPT, Claude,  BERT family,  Flan-T5, WhisperLlama    HuBERTMixtral\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 3, 'page_label': '4', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Encoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 4, 'page_label': '5', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='EncodersMany varieties!•Popular: Masked Language Models (MLMs)•BERT family•Trained by predicting words from surrounding words on both sides•Are usually finetuned (trained on supervised data) for classification tasks.\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 5, 'page_label': '6', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Encoder-Decoders\\n•Trained to map from one sequence to another•Very popular for:•machine translation (map from one language to another)•speech recognition (map from acoustics to words)\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 6, 'page_label': '7', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsIntroduction to Large Language Models'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 7, 'page_label': '8', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsLarge Language Models: What tasks can they do?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 8, 'page_label': '9', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Big ideaMany tasks can be turned into tasks of predicting words!'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 9, 'page_label': '10', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='This lecture: decoder-only modelsAlso called:•Causal LLMs•Autoregressive LLMs•Left-to-right LLMs•Predict words left to right\\nPretraining for three types of architecturesThe neural architecture influences the type of pretraining, and natural use cases.\\n32\\nDecoders• Language models! What we’ve seen so far.• Nice to generate from; can’t condition on future words\\nEncoders• Gets bidirectional context – can condition on future!• How do we train them to build strong representations?\\nEncoder-Decoders• Good parts of decoders and encoders?• What’s the best way to pretrain them?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 10, 'page_label': '11', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Conditional Generation: Generating text conditioned on previous text!\\nPreﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead logits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 11, 'page_label': '12', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Many practical NLP tasks can be cast as word prediction!Sentiment analysis: “I like Jackie Chan”1.We give the language model this string:The sentiment of the sentence \"I like Jackie Chan\" is: 2.And see what word it thinks comes next:\\n10.1 • L ARGE L ANGUAGE M ODELS WITH T RANSFORMERS 3\\nPreﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead logits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…\\nFigure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language\\nmodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.\\nword “negative” to see which is higher:\\nP ( positive | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nP ( negative | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 11, 'page_label': '12', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='P ( negative | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is\\npositive, otherwise we say the sentiment is negative.\\nWe can also cast more complex tasks as word prediction. Consider question\\nanswering, in which the system is given a question (for example a question with\\na simple factual answer) and must give a textual answer; we introduce this task in\\ndetail in Chapter 15. We can cast the task of question answering as word prediction\\nby giving a language model a question and a token like A: suggesting that an answer\\nshould come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute the probability distribution over possible\\nnext words given this preﬁx:\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: )\\nand look at which words w have high probabilities, we might expect to see that'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 11, 'page_label': '12', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='next words given this preﬁx:\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: )\\nand look at which words w have high probabilities, we might expect to see that\\nCharles is very likely, and then if we choose Charles and continue and ask\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles )\\nwe might now see that Darwin is the most probable token, and select it.\\nConditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext\\nsummarization\\ntext, such as a full-length article, and produce an effective shorter summary of it. We\\ncan cast summarization as language modeling by giving a large language model a\\ntext, and follow the text by a token like tl;dr ; this token is short for something like'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Framing lots of tasks as conditional generationQA: “Who wrote The Origin of Species”1.We give the language model this string:2.And see what word it thinks comes next:\\n3.And iterate:\\n20 C HAPTER 10 • T RANSFORMERS AND L ARGE L ANGUAGE M ODELS\\nPreﬁx Text\\nCompletion Text\\nInput\\nEmbeddings\\nTransformer\\nBlocks\\nSample from Softmax\\nSo long\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nlinear layer\\nFigure 10.15 Autoregressive text completion with transformer-based large language models.\\nword “negative” to see which is higher:\\nP ( positive | The sentiment of the sentence “I like Jackie Chan” is: )\\nP ( negative | The sentiment of the sentence “I like Jackie Chan” is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is\\npositive, otherwise we say the sentiment is negative.\\nWe can also cast more complex tasks as word prediction. Consider the task\\nof answering simple questions, a task we return to in Chapter 14. In this task the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='We can also cast more complex tasks as word prediction. Consider the task\\nof answering simple questions, a task we return to in Chapter 14. In this task the\\nsystem is given some question and must give a textual answer. We can cast the task\\nof question answering as word prediction by giving a language model a question and\\na token like A: suggesting that an answer should come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute\\nP ( w | Q: Who wrote the book “The Origin of Species”? A: )\\nand look at which words w have high probabilities, we might expect to see that\\nCharles is very likely, and then if we choose Charles and continue and ask\\nP ( w | Q: Who wrote the book “The Origin of Species”? A: Charles )\\nwe might now see that Darwin is the most probable word, and select it.\\nConditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Conditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext\\nsummarization\\ntext, such as a full-length article, and produce an effective shorter summary of it.\\nWe can cast summarization as language modeling by giving a large language model\\na text, and follow the text by a token like tl;dr ; this token is short for something\\nlike ‘too long; don’t read’ and in recent years people often use this token, especially\\nin informal work emails, when they are going to give a short summary. We can\\nthen do conditional generation: give the language model this preﬁx, and then ask\\n10.1 • L ARGE L ANGUAGE M ODELS WITH T RANSFORMERS 3\\nPreﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead\\nlogits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Preﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead\\nlogits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…\\nFigure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language\\nmodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.\\nword “negative” to see which is higher:\\nP ( positive | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nP ( negative | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is\\npositive, otherwise we say the sentiment is negative.\\nWe can also cast more complex tasks as word prediction. Consider question\\nanswering, in which the system is given a question (for example a question with\\na simple factual answer) and must give a textual answer; we introduce this task in'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='answering, in which the system is given a question (for example a question with\\na simple factual answer) and must give a textual answer; we introduce this task in\\ndetail in Chapter 15. We can cast the task of question answering as word prediction\\nby giving a language model a question and a token like A: suggesting that an answer\\nshould come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute the probability distribution over possible\\nnext words given this preﬁx:\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: )\\nand look at which words w have high probabilities, we might expect to see that\\nCharles is very likely, and then if we choose Charles and continue and ask\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles )\\nwe might now see that Darwin is the most probable token, and select it.\\nConditional generation can even be used to accomplish tasks that must generate'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='we might now see that Darwin is the most probable token, and select it.\\nConditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext\\nsummarization\\ntext, such as a full-length article, and produce an effective shorter summary of it. We\\ncan cast summarization as language modeling by giving a large language model a\\ntext, and follow the text by a token like tl;dr ; this token is short for something like\\n10.1 • L ARGE L ANGUAGE M ODELS WITH T RANSFORMERS 3\\nPreﬁx Text\\nCompletion Text\\nEncoder\\nTransformer\\nBlocks\\nSoftmax\\nlong\\nall\\nand thanks for all\\nthe\\nthe\\n…\\nU UUnencoder layer\\nLanguage \\nModeling\\nHead logits\\nSo\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\nE\\ni+\\n…\\nFigure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language\\nmodels. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='models. As each token is generated, it gets added onto the context as a preﬁx for generating the next token.\\nword “negative” to see which is higher:\\nP ( positive | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nP ( negative | The sentiment of the sentence ‘‘I like Jackie Chan\" is: )\\nIf the word “positive” is more probable, we say the sentiment of the sentence is\\npositive, otherwise we say the sentiment is negative.\\nWe can also cast more complex tasks as word prediction. Consider question\\nanswering, in which the system is given a question (for example a question with\\na simple factual answer) and must give a textual answer; we introduce this task in\\ndetail in Chapter 15. We can cast the task of question answering as word prediction\\nby giving a language model a question and a token like A: suggesting that an answer\\nshould come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute the probability distribution over possible'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 12, 'page_label': '13', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='should come next:\\nQ: Who wrote the book ‘‘The Origin of Species\"? A:\\nIf we ask a language model to compute the probability distribution over possible\\nnext words given this preﬁx:\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: )\\nand look at which words w have high probabilities, we might expect to see that\\nCharles is very likely, and then if we choose Charles and continue and ask\\nP ( w | Q: Who wrote the book ‘‘The Origin of Species\"? A: Charles )\\nwe might now see that Darwin is the most probable token, and select it.\\nConditional generation can even be used to accomplish tasks that must generate\\nlonger responses. Consider the task of text summarization , which is to take a long\\ntext\\nsummarization\\ntext, such as a full-length article, and produce an effective shorter summary of it. We\\ncan cast summarization as language modeling by giving a large language model a\\ntext, and follow the text by a token like tl;dr ; this token is short for something like'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Summarization\\n4 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\n‘too long; didn’t read’ and in recent years people often use this token, especially in\\ninformal work emails, when they are going to give a short summary. Since this token\\nis sufﬁciently frequent in language model training data, language models have seen\\nmany texts in which the token occurs before a summary, and hence will interpret the\\ntoken as instructions to generate a summary. We can then do conditional generation:\\ngive the language model this preﬁx, and then have it generate the following words,\\none by one, and take the entire response as a summary. Fig. 10.2 shows an example\\nof a text and a human-produced summary from a widely-used summarization corpus\\nconsisting of CNN and Daily Mirror news articles.\\nOriginal Article\\nThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff\\nand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='The only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff\\nand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says.\\nBut not if you live in New England or surrounding states. “We will not ship snow to any states\\nin the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging\\nsnow!”\\nHis website and social media accounts claim to have ﬁlled more than 133 orders for snow – more\\nthan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a\\nrecord this winter for the snowiest month in its history. Most residents see the huge piles of snow\\nchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.\\nAccording to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='choking their yards and sidewalks as a nuisance, but Waring saw an opportunity.\\nAccording to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-\\neling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. He\\njoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]\\nSummary\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.\\nFigure 10.2 Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus\\n( Hermann et al. , 2015 ), ( Nallapati et al. , 2016 ).\\nIf we take this full article and append the token tl;dr , we can use this as the con-\\ntext to prime the generation process to produce a summary as illustrated in Fig. 10.3 .\\nAgain, what makes transformers able to succeed at this task (as compared, say, to'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='text to prime the generation process to produce a summary as illustrated in Fig. 10.3 .\\nAgain, what makes transformers able to succeed at this task (as compared, say, to\\nthe primitive n-gram language model) is that attention can incorporate information\\nfrom the large context window, giving the model access to the original article as well\\nas to the newly generated text throughout the process.\\nWhich words do we generate at each step? One simple way to generate words\\nis to always generate the most likely word given the context. Generating the most\\nlikely word given the context is called greedy decoding . A greedy algorithm is one\\ngreedy\\ndecoding\\nthat make a choice that is locally optimal, whether or not it will turn out to have\\nbeen the best choice with hindsight. Thus in greedy decoding, at each time step in\\ngeneration, the output y t is chosen by computing the probability for each possible\\noutput (every word in the vocabulary) and then choosing the highest probability'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='generation, the output y t is chosen by computing the probability for each possible\\noutput (every word in the vocabulary) and then choosing the highest probability\\nword (the argmax):\\nˆw t = argmax w 2 V P ( w | w < t ) (10.1)\\nIn practice, however, we don’t use greedy decoding with large language models.\\nA major problem with greedy decoding is that because the words it chooses are (by\\ndeﬁnition) extremely predictable, the resulting text is generic and often quite repeti-\\ntive. Indeed, greedy decoding is so predictable that it is deterministic; if the context\\n4 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\n‘too long; didn’t read’ and in recent years people often use this token, especially in\\ninformal work emails, when they are going to give a short summary. Since this token\\nis sufﬁciently frequent in language model training data, language models have seen\\nmany texts in which the token occurs before a summary, and hence will interpret the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='is sufﬁciently frequent in language model training data, language models have seen\\nmany texts in which the token occurs before a summary, and hence will interpret the\\ntoken as instructions to generate a summary. We can then do conditional generation:\\ngive the language model this preﬁx, and then have it generate the following words,\\none by one, and take the entire response as a summary. Fig. 10.2 shows an example\\nof a text and a human-produced summary from a widely-used summarization corpus\\nconsisting of CNN and Daily Mirror news articles.\\nOriginal Article\\nThe only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff\\nand offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says.\\nBut not if you live in New England or surrounding states. “We will not ship snow to any states'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='for 10 to 15 snowballs, he says.\\nBut not if you live in New England or surrounding states. “We will not ship snow to any states\\nin the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging\\nsnow!”\\nHis website and social media accounts claim to have ﬁlled more than 133 orders for snow – more\\nthan 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a\\nrecord this winter for the snowiest month in its history. Most residents see the huge piles of snow\\nchoking their yards and sidewalks as a nuisance, but Waring saw an opportunity.\\nAccording to Boston.com, it all started a few weeks ago, when Waring and his wife were shov-\\neling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston. He\\njoked about shipping the stuff to friends and family in warmer states, and an idea was born. [...]\\nSummary\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Summary\\nKyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough\\nfor 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.\\nFigure 10.2 Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus\\n( Hermann et al. , 2015 ), ( Nallapati et al. , 2016 ).\\nIf we take this full article and append the token tl;dr , we can use this as the con-\\ntext to prime the generation process to produce a summary as illustrated in Fig. 10.3 .\\nAgain, what makes transformers able to succeed at this task (as compared, say, to\\nthe primitive n-gram language model) is that attention can incorporate information\\nfrom the large context window, giving the model access to the original article as well\\nas to the newly generated text throughout the process.\\nWhich words do we generate at each step? One simple way to generate words\\nis to always generate the most likely word given the context. Generating the most'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Which words do we generate at each step? One simple way to generate words\\nis to always generate the most likely word given the context. Generating the most\\nlikely word given the context is called greedy decoding . A greedy algorithm is one\\ngreedy\\ndecoding\\nthat make a choice that is locally optimal, whether or not it will turn out to have\\nbeen the best choice with hindsight. Thus in greedy decoding, at each time step in\\ngeneration, the output y t is chosen by computing the probability for each possible\\noutput (every word in the vocabulary) and then choosing the highest probability\\nword (the argmax):\\nˆw t = argmax w 2 V\\nP ( w | w < t ) (10.1)\\nIn practice, however, we don’t use greedy decoding with large language models.\\nA major problem with greedy decoding is that because the words it chooses are (by\\ndeﬁnition) extremely predictable, the resulting text is generic and often quite repeti-\\ntive. Indeed, greedy decoding is so predictable that it is deterministic; if the context\\nOriginal'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 13, 'page_label': '14', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='deﬁnition) extremely predictable, the resulting text is generic and often quite repeti-\\ntive. Indeed, greedy decoding is so predictable that it is deterministic; if the context\\nOriginal\\nSummary'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 14, 'page_label': '15', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='LLMs for summarization (using  tl;dr)\\nOriginal Story\\nGenerated Summary\\n… idea\\nKyle\\nwas born. Kyle\\nWaring\\nWaringonlyThe\\n…\\nwill\\nDelimiter\\nwill\\nU U U\\ntl;dr\\nLM Head\\nE\\n E\\n E\\n E\\n E\\n E\\n E\\n E\\n…'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 15, 'page_label': '16', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsLarge Language Models: What tasks can they do?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 16, 'page_label': '17', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsSampling for LLM Generation'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 17, 'page_label': '18', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Decoding and SamplingThis task of choosing a word to generate based on the model’s probabilities is called decoding. The most common method for decoding in LLMs: sampling. Sampling from a model’s distribution over words:•choose random words according to their probability assigned by the model. After each token we’ll sample words to generate according to their probability conditioned on our previous choices, •A transformer language model will give the probability'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 18, 'page_label': '19', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Random sampling\\n6 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\nas deﬁned by the model. Thus we are more likely to generate words that the model\\nthinks have a high probability in the context and less likely to generate words that\\nthe model thinks have a low probability.\\nWe saw back in Chapter 3 on page ?? how to generate text from a unigram lan-\\nguage model , by repeatedly randomly sampling words according to their probability\\nuntil we either reach a pre-determined length or select the end-of-sentence token. To\\ngenerate text from a trained transformer language model we’ll just generalize this\\nmodel a bit: at each step we’ll sample words according to their probability condi-\\ntioned on our previous choices , and we’ll use a transformer language model as the\\nprobability model that tells us this probability.\\nWe can formalize this algorithm for generating a sequence of words W = w 1\\n, w 2\\n,..., w N\\nuntil we hit the end-of-sequence token, using x ⇠ p ( x ) to mean ‘choose x by sam-'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 18, 'page_label': '19', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='We can formalize this algorithm for generating a sequence of words W = w 1\\n, w 2\\n,..., w N\\nuntil we hit the end-of-sequence token, using x ⇠ p ( x ) to mean ‘choose x by sam-\\npling from the distribution p ( x ) :\\ni  1\\nw i\\n⇠ p( w )\\nwhile w i\\n!= EOS\\ni  i+1\\nw i\\n⇠ p( w i\\n| w < i\\n)\\nThe algorithm above is called random sampling , and it turns out random sam-\\nrandom\\nsampling\\npling doesn’t work well enough. The problem is that even though random sampling\\nis mostly going to generate sensible, high-probable words, there are many odd, low-\\nprobability words in the tail of the distribution, and even though each one is low-\\nprobability, if you add up all the rare words, they constitute a large enough portion\\nof the distribution that they get chosen often enough to result in generating weird\\nsentences. For this reason, instead of random sampling, we usually use sampling\\nmethods that avoid generating the very unlikely words.\\nThe sampling methods we introduce below each have parameters that enable'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 18, 'page_label': '19', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='methods that avoid generating the very unlikely words.\\nThe sampling methods we introduce below each have parameters that enable\\ntrading off two important factors in generation: quality and diversity . Methods\\nthat emphasize the most probable words tend to produce generations that are rated\\nby people as more accurate, more coherent, and more factual, but also more boring\\nand more repetitive. Methods that give a bit more weight to the middle-probability\\nwords tend to be more creative and more diverse, but less factual and more likely to\\nbe incoherent or otherwise low-quality.\\n10.2.1 Top- k sampling\\nTop-k sampling is a simple generalization of greedy decoding. Instead of choosingtop-k sampling\\nthe single most probable word to generate, we ﬁrst truncate the distribution to the\\ntop k most likely words, renormalize to produce a legitimate probability distribution,\\nand then randomly sample from within these k words according to their renormalized\\nprobabilities. More formally:'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 18, 'page_label': '19', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='top k most likely words, renormalize to produce a legitimate probability distribution,\\nand then randomly sample from within these k words according to their renormalized\\nprobabilities. More formally:\\n1. Choose in advance a number of words k\\n2. For each word in the vocabulary V , use the language model to compute the\\nlikelihood of this word given the context p ( w t\\n| w < t\\n)\\n3. Sort the words by their likelihood, and throw away any word that is not one of\\nthe top k most probable words.\\n4. Renormalize the scores of the k words to be a legitimate probability distribu-\\ntion.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 19, 'page_label': '20', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"Random sampling doesn't work very wellEven though random sampling mostly generate sensible, high-probable words, There are many odd, low- probability words in the tail of the distribution Each one is low- probability but added up they constitute a large portion of the distribution So they get picked enough to generate weird sentences\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 20, 'page_label': '21', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Factors in word sampling: quality and diversityEmphasize high-probability words  + quality: more  accurate, coherent, and factual, - diversity: boring, repetitive. Emphasize middle-probability words + diversity: more creative, diverse, - quality: less factual, incoherent'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 21, 'page_label': '22', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Top-k sampling:1. Choose # of words k 2. For each word in the vocabulary V , use the language model to compute the likelihood of this word given the context p(wt |w<t ) 3. Sort the words by likelihood, keep only the top k most probable words. 4. Renormalize the scores of the k words to be a legitimate probability distribution. 5. Randomly sample a word from within these remaining k most-probable words according to its probability.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 22, 'page_label': '23', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Top-p sampling (= nucleus sampling)Problem with top-k:  k is fixed so may cover very different amounts of probability mass in different situationsIdea: Instead, keep the top p percent of the probability massGiven a distribution P(wt |w<t ), the top-p vocabulary V ( p) is the smallest set of words such that \\nHoltzman et al., 2020 \\n10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.\\nWhen k = 1, top- k sampling is identical to greedy decoding. Setting k to a larger\\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\\nprobable, but is still probable enough, and whose choice results in generating more\\ndiverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 22, 'page_label': '23', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='One problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes\\nthe top 10 words will be very likely and include most of the probability mass, but\\nother times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling\\n2020 ), is to keep not the top k words, but the top p percent of the probability mass.\\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\\nBut by measuring probability rather than the number of words, the hope is that the\\nmeasure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w t\\n| w < t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w < t'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 22, 'page_label': '23', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='decreasing the pool of word candidates.\\nGiven a distribution P ( w t\\n| w < t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w < t\\n) \\x00 p . (10.2)\\n10.2.3 Temperature sampling\\nIn temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,\\nwhile a system at a lower temperature is likely to explore a subset of lower energy\\n(better) states. In low-temperature sampling, we smoothly increase the probability\\nof the most probable words and decrease the probability of the rare words.\\nWe implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 22, 'page_label': '23', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='eter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the\\nvocabulary directly from the logit as in the following (repeated from ( ?? )):\\ny = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.\\nBut the lower t is, the larger the scores being passed to the softmax (dividing by a\\nsmaller fraction t \\uf8ff 1 results in making each score larger). Recall that one of the\\nuseful properties of a softmax is that it tends to push high values toward 1 and low\\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 22, 'page_label': '23', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='a distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more\\ngreedy. As t approaches 0 the probability of the most likely word approaches 1.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 23, 'page_label': '24', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Temperature samplingReshape the distribution instead of truncating itIntuition from thermodynamics, •a system at high temperature is flexible and can explore many possible states,•a system at lower temperature is likely to explore a subset of lower energy (better) states. In low-temperature sampling,  (τ ≤ 1) we smoothly•increase the probability of the most probable words•decrease the probability of the rare words.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Temperature samplingDivide the logit by a temperature parameter τ before passing it through the softmax.Instead ofWe do  \\n10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.\\nWhen k = 1, top- k sampling is identical to greedy decoding. Setting k to a larger\\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\\nprobable, but is still probable enough, and whose choice results in generating more\\ndiverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes\\nthe top 10 words will be very likely and include most of the probability mass, but\\nother times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='other times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling\\n2020 ), is to keep not the top k words, but the top p percent of the probability mass.\\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\\nBut by measuring probability rather than the number of words, the hope is that the\\nmeasure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w\\nt\\n| w\\n< t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n) \\x00 p . (10.2)\\n10.2.3 Temperature sampling\\nIn temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='In temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,\\nwhile a system at a lower temperature is likely to explore a subset of lower energy\\n(better) states. In low-temperature sampling, we smoothly increase the probability\\nof the most probable words and decrease the probability of the rare words.\\nWe implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the\\nvocabulary directly from the logit as in the following (repeated from ( ?? )):\\ny = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='y = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.\\nBut the lower t is, the larger the scores being passed to the softmax (dividing by a\\nsmaller fraction t \\uf8ff 1 results in making each score larger). Recall that one of the\\nuseful properties of a softmax is that it tends to push high values toward 1 and low\\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more\\ngreedy. As t approaches 0 the probability of the most likely word approaches 1.\\n10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.\\nWhen k = 1, top- k sampling is identical to greedy decoding. Setting k to a larger\\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\\nprobable, but is still probable enough, and whose choice results in generating more\\ndiverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes\\nthe top 10 words will be very likely and include most of the probability mass, but\\nother times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='include a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling\\n2020 ), is to keep not the top k words, but the top p percent of the probability mass.\\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\\nBut by measuring probability rather than the number of words, the hope is that the\\nmeasure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w\\nt\\n| w\\n< t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n) \\x00 p . (10.2)\\n10.2.3 Temperature sampling\\nIn temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='temperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,\\nwhile a system at a lower temperature is likely to explore a subset of lower energy\\n(better) states. In low-temperature sampling, we smoothly increase the probability\\nof the most probable words and decrease the probability of the rare words.\\nWe implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the\\nvocabulary directly from the logit as in the following (repeated from ( ?? )):\\ny = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 24, 'page_label': '25', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='we instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.\\nBut the lower t is, the larger the scores being passed to the softmax (dividing by a\\nsmaller fraction t \\uf8ff 1 results in making each score larger). Recall that one of the\\nuseful properties of a softmax is that it tends to push high values toward 1 and low\\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more\\ngreedy. As t approaches 0 the probability of the most likely word approaches 1.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 25, 'page_label': '26', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Temperature sampling\\nWhy does this work?•When τ is close to 1 the distribution doesn’t change much. •The lower τ is, the larger the scores being passed to the softmax•Softmax pushes high values toward 1 and low values toward 0. •Large inputs pushes high-probability words higher and low probability word lower,  making the distribution more greedy. •As τ approaches 0, the probability of most likely word approaches 1 \\n10.2 • S AMPLING FOR LLM G ENERATION 7\\n5. Randomly sample a word from within these remaining k most-probable words\\naccording to its probability.\\nWhen k = 1, top- k sampling is identical to greedy decoding. Setting k to a larger\\nnumber than 1 leads us to sometimes select a word which is not necessarily the most\\nprobable, but is still probable enough, and whose choice results in generating more\\ndiverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 25, 'page_label': '26', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='diverse but still high-enough-quality text.\\n10.2.2 Nucleus or top- p sampling\\nOne problem with top- k sampling is that k is ﬁxed, but the shape of the probability\\ndistribution over words differs in different contexts. If we set k = 10, sometimes\\nthe top 10 words will be very likely and include most of the probability mass, but\\nother times the probability distribution will be ﬂatter and the top 10 words will only\\ninclude a small part of the probability mass.\\nAn alternative, called top-p sampling or nucleus sampling ( Holtzman et al. ,top-p sampling\\n2020 ), is to keep not the top k words, but the top p percent of the probability mass.\\nThe goal is the same; to truncate the distribution to remove the very unlikely words.\\nBut by measuring probability rather than the number of words, the hope is that the\\nmeasure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w\\nt\\n| w\\n< t\\n) , the top- p vocabulary V'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 25, 'page_label': '26', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='measure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidates.\\nGiven a distribution P ( w\\nt\\n| w\\n< t\\n) , the top- p vocabulary V\\n( p )\\nis the smallest set of\\nwords such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n) \\x00 p . (10.2)\\n10.2.3 Temperature sampling\\nIn temperature sampling , we don’t truncate the distribution, but instead reshape\\ntemperature\\nsampling\\nit. The intuition for temperature sampling comes from thermodynamics, where a\\nsystem at a high temperature is very ﬂexible and can explore many possible states,\\nwhile a system at a lower temperature is likely to explore a subset of lower energy\\n(better) states. In low-temperature sampling, we smoothly increase the probability\\nof the most probable words and decrease the probability of the rare words.\\nWe implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 25, 'page_label': '26', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='We implement this intuition by simply dividing the logit by a temperature param-\\neter t before we normalize it by passing it through the softmax. In low-temperature\\nsampling, t 2 ( 0 , 1 ] . Thus instead of computing the probability distribution over the\\nvocabulary directly from the logit as in the following (repeated from ( ?? )):\\ny = softmax ( u ) (10.3)\\nwe instead ﬁrst divide the logits by t , computing the probability vector y as\\ny = softmax ( u / t ) (10.4)\\nWhy does this work? When t is close to 1 the distribution doesn’t change much.\\nBut the lower t is, the larger the scores being passed to the softmax (dividing by a\\nsmaller fraction t \\uf8ff 1 results in making each score larger). Recall that one of the\\nuseful properties of a softmax is that it tends to push high values toward 1 and low\\nvalues toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 25, 'page_label': '26', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='values toward 0. Thus when larger numbers are passed to a softmax the result is\\na distribution with increased probabilities of the most high-probability words and\\ndecreased probabilities of the low probability words, making the distribution more\\ngreedy. As t approaches 0 the probability of the most likely word approaches 1.\\n0 ≤ τ ≤ 1'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 26, 'page_label': '27', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsSampling for LLM Generation'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 27, 'page_label': '28', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsPretraining Large Language Models: Algorithm'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 28, 'page_label': '29', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='PretrainingThe big idea that underlies all the amazing performance of language modelsFirst pretrain a transformer model on enormous amounts of textThen apply it to new tasks.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 29, 'page_label': '30', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Self-supervised training algorithmWe just train them to predict the next word!1.Take a corpus of text 2.At each time step t i.ask the model to predict the next word ii.train the model using gradient descent to minimize the error in this prediction\"Self-supervised\" because it just uses the next word as the label!'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 30, 'page_label': '31', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Intuition of language model training: loss•Same loss function: cross-entropy loss•We want the model to assign a high probability to true word w•= want loss to be high if the model assigns too low a probability to w•CE Loss: The negative log probability that the model assigns to the true next word w•If the model assigns too low a probability to w•We move the model weights in the direction that assigns a higher probability to w'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Cross-entropy loss for language modelingCE loss: difference between the correct probability distribution and the predicted distribution \\nThe correct distribution yt knows the next word, so is 1 for the actual next word and 0 for the others.So in this sum, all terms get multiplied by zero except one: the logp the model assigns to the correct next word, so: \\n8 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\nNote, by the way, that there can be other situations where we may want to do\\nsomething quite different and ﬂatten the word probability distribution instead of\\nmaking it greedy. Temperature sampling can help with this situation too, in this case\\nhigh-temperature sampling, in which case we use t > 1.\\n10.3 Pretraining Large Language Models\\nHow do we teach a transformer to be a language model? What is the algorithm and\\nwhat data do we train on?\\n10.3.1 Self-supervised training algorithm\\nTo train a transformer as a language model, we use the same self-supervision (orself-supervision'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='what data do we train on?\\n10.3.1 Self-supervised training algorithm\\nTo train a transformer as a language model, we use the same self-supervision (orself-supervision\\nself-training ) algorithm we saw in Section ?? : we take a corpus of text as training\\nmaterial and at each time step t ask the model to predict the next word. We call\\nsuch a model self-supervised because we don’t have to add any special gold labels\\nto the data; the natural sequence of words is its own supervision! We simply train the\\nmodel to minimize the error in predicting the true next word in the training sequence,\\nusing cross-entropy as the loss function.\\nRecall that the cross-entropy loss measures the difference between a predicted\\nprobability distribution and the correct distribution.\\nL CE\\n= \\x00\\nX\\nw 2 V\\ny t [ w ] log ˆy t [ w ] (10.5)\\nIn the case of language modeling, the correct distribution y t comes from knowing the\\nnext word. This is represented as a one-hot vector corresponding to the vocabulary'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='In the case of language modeling, the correct distribution y t comes from knowing the\\nnext word. This is represented as a one-hot vector corresponding to the vocabulary\\nwhere the entry for the actual next word is 1, and all the other entries are 0. Thus,\\nthe cross-entropy loss for language modeling is determined by the probability the\\nmodel assigns to the correct next word (all other words get multiplied by zero). So\\nat time t the CE loss in ( 10.5 ) can be simpliﬁed as the negative log probability the\\nmodel assigns to the next word in the training sequence.\\nL CE\\n( ˆy t , y t )= \\x00 log ˆy t [ w t + 1\\n] (10.6)\\nThus at each word position t of the input, the model takes as input the correct se-\\nquence of tokens w 1: t\\n, and uses them to compute a probability distribution over\\npossible next words so as to compute the model’s loss for the next token w t + 1\\n. Then\\nwe move to the next word, we ignore what the model predicted for the next word'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='possible next words so as to compute the model’s loss for the next token w t + 1\\n. Then\\nwe move to the next word, we ignore what the model predicted for the next word\\nand instead use the correct sequence of tokens w 1: t + 1\\nto estimate the probability of\\ntoken w t + 2\\n. This idea that we always give the model the correct history sequence to\\npredict the next word (rather than feeding the model its best case from the previous\\ntime step) is called teacher forcing .teacher forcing\\nFig. 10.4 illustrates the general training approach. At each step, given all the\\npreceding words, the ﬁnal transformer layer produces an output distribution over\\nthe entire vocabulary. During training, the probability assigned to the correct word\\nis used to calculate the cross-entropy loss for each item in the sequence. The loss\\nfor a training sequence is the average cross-entropy loss over the entire sequence.\\nThe weights in the network are adjusted to minimize the average CE loss over the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='for a training sequence is the average cross-entropy loss over the entire sequence.\\nThe weights in the network are adjusted to minimize the average CE loss over the\\ntraining sequence via gradient descent.\\n8 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\nNote, by the way, that there can be other situations where we may want to do\\nsomething quite different and ﬂatten the word probability distribution instead of\\nmaking it greedy. Temperature sampling can help with this situation too, in this case\\nhigh-temperature sampling, in which case we use t > 1.\\n10.3 Pretraining Large Language Models\\nHow do we teach a transformer to be a language model? What is the algorithm and\\nwhat data do we train on?\\n10.3.1 Self-supervised training algorithm\\nTo train a transformer as a language model, we use the same self-supervision (orself-supervision\\nself-training ) algorithm we saw in Section ?? : we take a corpus of text as training\\nmaterial and at each time step t ask the model to predict the next word. We call'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='self-training ) algorithm we saw in Section ?? : we take a corpus of text as training\\nmaterial and at each time step t ask the model to predict the next word. We call\\nsuch a model self-supervised because we don’t have to add any special gold labels\\nto the data; the natural sequence of words is its own supervision! We simply train the\\nmodel to minimize the error in predicting the true next word in the training sequence,\\nusing cross-entropy as the loss function.\\nRecall that the cross-entropy loss measures the difference between a predicted\\nprobability distribution and the correct distribution.\\nL CE\\n= \\x00\\nX\\nw 2 V\\ny t\\n[ w ] log ˆy t\\n[ w ] (10.5)\\nIn the case of language modeling, the correct distribution y t\\ncomes from knowing the\\nnext word. This is represented as a one-hot vector corresponding to the vocabulary\\nwhere the entry for the actual next word is 1, and all the other entries are 0. Thus,\\nthe cross-entropy loss for language modeling is determined by the probability the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='where the entry for the actual next word is 1, and all the other entries are 0. Thus,\\nthe cross-entropy loss for language modeling is determined by the probability the\\nmodel assigns to the correct next word (all other words get multiplied by zero). So\\nat time t the CE loss in ( 10.5 ) can be simpliﬁed as the negative log probability the\\nmodel assigns to the next word in the training sequence.\\nL CE\\n( ˆy t\\n, y t\\n)= \\x00 log ˆy t\\n[ w t + 1\\n] (10.6)\\nThus at each word position t of the input, the model takes as input the correct se-\\nquence of tokens w 1: t\\n, and uses them to compute a probability distribution over\\npossible next words so as to compute the model’s loss for the next token w t + 1\\n. Then\\nwe move to the next word, we ignore what the model predicted for the next word\\nand instead use the correct sequence of tokens w 1: t + 1\\nto estimate the probability of\\ntoken w t + 2\\n. This idea that we always give the model the correct history sequence to'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 31, 'page_label': '32', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='and instead use the correct sequence of tokens w 1: t + 1\\nto estimate the probability of\\ntoken w t + 2\\n. This idea that we always give the model the correct history sequence to\\npredict the next word (rather than feeding the model its best case from the previous\\ntime step) is called teacher forcing .teacher forcing\\nFig. 10.4 illustrates the general training approach. At each step, given all the\\npreceding words, the ﬁnal transformer layer produces an output distribution over\\nthe entire vocabulary. During training, the probability assigned to the correct word\\nis used to calculate the cross-entropy loss for each item in the sequence. The loss\\nfor a training sequence is the average cross-entropy loss over the entire sequence.\\nThe weights in the network are adjusted to minimize the average CE loss over the\\ntraining sequence via gradient descent.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 32, 'page_label': '33', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Teacher forcing•At each token position t, model sees correct tokens w1:t, •Computes  loss (–log probability) for the next token wt+1 •At next token position t+1 we ignore what model predicted for wt+1 •Instead we take the correct word wt+1, add it to context, move on'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 33, 'page_label': '34', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Training a transformer language model\\nlong and thanks forNext token all\\nLoss\\n…\\n=\\n<latexit sha1_base64=\"AovqpaL476UmJ1EU1xZPgDZ70tQ=\">AAAB9nicbVDLSsNAFL2pr1pfURcu3AwWwY0lEakui25cVrAPaEqYTCbt0EkmzEzEEvIrbkTcKPgZ/oJ/Y9Jm09YDA4dzznDvPV7MmdKW9WtU1tY3Nreq27Wd3b39A/PwqKtEIgntEMGF7HtYUc4i2tFMc9qPJcWhx2nPm9wXfu+ZSsVE9KSnMR2GeBSxgBGsc8k1Ty4dLkZo6qZOiPVYhimO/CyruWbdalgzoFVil6QOJdqu+eP4giQhjTThWKmBbcV6mGKpGeE0qzmJojEmEzyi6WztDJ3nko8CIfMXaTRTF3I4VGoaenmy2E0te4X4nzdIdHA7TFkUJ5pGZD4oSDjSAhUdIJ9JSjSf5gQTyfINERljiYnOmypOt5cPXSXdq4bdbDQfr+utu7KEKpzCGVyADTfQggdoQwcIZPAGn/BlvBivxrvxMY9WjPLPMSzA+P4DPEiSHA==</latexit>\\n\\x00 log y and\\nStacked\\nTransformer\\nBlocks\\nSo long and thanks for\\n…\\n…\\n…\\nU\\nInput tokens\\nx1 x2\\nLanguage\\nModeling\\nHead\\nx3 x4 x5\\nInput\\nEncoding\\n E\\n1+\\nE\\n2+\\nE\\n3+\\nE\\n4+\\nE\\n5+\\n…\\n… ………\\nU\\n U\\n U\\n U\\n…\\nlogits logits logits logits logits\\n…'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 33, 'page_label': '34', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Transformer\\nBlocks\\nSo long and thanks for\\n…\\n…\\n…\\nU\\nInput tokens\\nx1 x2\\nLanguage\\nModeling\\nHead\\nx3 x4 x5\\nInput\\nEncoding\\n E\\n1+\\nE\\n2+\\nE\\n3+\\nE\\n4+\\nE\\n5+\\n…\\n… ………\\nU\\n U\\n U\\n U\\n…\\nlogits logits logits logits logits\\n…\\n<latexit sha1_base64=\"q3ZgXDyG7qtkT7t8hT47RdlwYG4=\">AAAB+XicbVDLSsNAFJ3UV62vWHe6GVsEN5bERXUlBUVcVrAPaEqYTCft0MlMmJkIIQT8AT/CTRE3Cv6Ev+DfmLTdtPXAwOGcM9x7jxcyqrRl/RqFtfWNza3idmlnd2//wDwst5WIJCYtLJiQXQ8pwignLU01I91QEhR4jHS88W3ud56JVFTwJx2HpB+gIac+xUhnkmseXzhMDGHsJk6A9EgGiR4hPlZpWnLNqlWzpoCrxJ6TauP0tXw3qdw0XfPHGQgcBYRrzJBSPdsKdT9BUlPMSFpyIkVChMdoSJLp5ik8y6QB9IXMHtdwqi7kUKBUHHhZMl9PLXu5+J/Xi7R/3U8oDyNNOJ4N8iMGtYB5DXBAJcGaxRlBWNJsQ4hHSCKss7Ly0+3lQ1dJ+7Jm12v1x6yDezBDEZyACjgHNrgCDfAAmqAFMHgBE/AJvozEeDPejY9ZtGDM/xyBBRjff79pldo=</latexit>\\n\\x00 log y thanks'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 34, 'page_label': '35', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsPretraining Large Language Models: Algorithm'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 35, 'page_label': '36', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsPretraining data for LLMs'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 36, 'page_label': '37', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"LLMs are mainly trained on the webCommon crawl, snapshots of the entire web produced by the non- profit Common Crawl with billions of pagesColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156 billion tokens of English,  filtered What's in it? Mostly patent text documents, Wikipedia, and news sites\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 37, 'page_label': '38', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='The Pile: a pretraining corpus\\nFigure 1: Treemap of Pile components by effective size.\\ntroduce a new ﬁltered subset of Common Crawl,\\nPile-CC, with improved extraction quality.\\nThrough our analyses, we conﬁrm that the Pile is\\nsigniﬁcantly distinct from pure Common Crawl\\ndata. Additionally, our evaluations show that the\\nexisting GPT-2 and GPT-3 models perform poorly\\non many components of the Pile, and that models\\ntrained on the Pile signiﬁcantly outperform both\\nraw and ﬁltered Common Crawl models. To com-\\nplement the performance evaluations, we also per-\\nform an exploratory analysis of the text within the\\nPile to provide a detailed picture of the data. We\\nhope that our extensive documentation of the con-\\nstruction and characteristics of the Pile will help\\nresearchers make informed decisions about poten-\\ntial downstream applications.\\nFinally, we make publicly available the preprocess-\\ning code for the constituent datasets of the Pile and\\nthe code for constructing alternative versions 2'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 37, 'page_label': '38', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='tial downstream applications.\\nFinally, we make publicly available the preprocess-\\ning code for the constituent datasets of the Pile and\\nthe code for constructing alternative versions 2\\n. In\\nthe interest of reproducibility, we also document\\nall processing performed on each dataset (and the\\nPile as a whole) in as much detail as possible. For\\nfurther details about the processing of each dataset,\\nsee Section 2 and Appendix C .\\n2\\nhttps://github.com/EleutherAI/\\nthe-pile\\n1.1 Contributions\\nThe core contributions of this paper are:\\n1. The introduction of a 825 . 18 GiB english-\\nlanguage dataset for language modeling com-\\nbining 22 diverse sources.\\n2. The introduction of 14 new language model-\\ning datasets, which we expect to be of inde-\\npendent interest to researchers.\\n3. Evaluations demonstrating signiﬁcant im-\\nprovements across many domains by GPT-2-\\nsized models trained on this new dataset, com-\\npared to training on CC-100 and raw Common\\nCrawl.\\n4. The investigation and documentation of this'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 37, 'page_label': '38', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='provements across many domains by GPT-2-\\nsized models trained on this new dataset, com-\\npared to training on CC-100 and raw Common\\nCrawl.\\n4. The investigation and documentation of this\\ndataset, which we hope will better inform re-\\nsearchers about how to use it as well as moti-\\nvate them to undertake similar investigations\\nof their own data.\\n2 The Pile Datasets\\nThe Pile is composed of 22 constituent sub-datasets,\\nas shown in Table 1 . Following Brown et al. ( 2020 ),\\nwe increase the weights of higher quality compo-\\nnents, with certain high-quality datasets such as\\nWikipedia being seen up to 3 times (“epochs”) for\\n2\\nwebacademics books\\ndialog'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 38, 'page_label': '39', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Filtering for quality and safetyQuality is subjective•Many LLMs attempt to match Wikipedia, books, particular websites•Need to remove boilerplate, adult content•Deduplication at many levels (URLs, documents, even lines)Safety also subjective•Toxicity detection is important, although that has mixed results•Can mistakenly flag data written in dialects like African American English'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 39, 'page_label': '40', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='What does a model learn from pretraining?•There are canines everywhere! One dog in the front room, and two dogs•It wasn\\'t just big it was enormous•The author of \"A Room of One\\'s Own\" is Virginia Woolf•The doctor told me that he•The square root of 4 is 2'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 40, 'page_label': '41', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Big ideaText contains enormous amounts of knowledgePretraining on lots of text with all that knowledge is what gives language models their ability to do so much'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 41, 'page_label': '42', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"But there are problems with scraping from the webCopyright: much of the text in these datasets is copyrighted•Not clear if fair use doctrine in US allows for this use•This remains an open legal questionData consent•Website owners can indicate they don't want their site crawledPrivacy: •Websites can contain private IP addresses and phone numbers\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 42, 'page_label': '43', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsPretraining data for LLMs'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 43, 'page_label': '44', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsFinetuning'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 44, 'page_label': '45', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"Finetuning for daptation to new domainsWhat happens if we need our LLM to work well on a domain it didn't see in pretraining?Perhaps some specific medical or legal domain?Or maybe a multilingual LM needs to see more data on some language that was rare in pretraining?\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 45, 'page_label': '46', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='FinetuningFine-\\ntuning \\nData\\nPretraining Data\\nPretraining\\n…\\n …\\n …\\nFine-tuning\\n…\\n …\\n …\\nPretrained LM Fine-tuned LM'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 46, 'page_label': '47', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='\"Finetuning\" means 4 different thingsWe\\'ll discuss 1 here, and 3 in later lecturesIn all four cases, finetuning means:taking a pretrained model and further adapting some or all of its parameters to some new data'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 47, 'page_label': '48', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='1. Finetuning as \"continued pretraining\" on new data•Further train all the parameters of model on new data•using the same method (word prediction) and loss function (cross-entropy loss) as for pretraining.•as if the new data were at the tail end of the pretraining data•Hence sometimes called continued pretraining'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 48, 'page_label': '49', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsFinetuning'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 49, 'page_label': '50', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsEvaluating Large Language Models'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 50, 'page_label': '51', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='PerplexityJust as for n-gram grammars, we use perplexity to measure how well the LM predicts unseen textThe perplexity of a model θ on an unseen test set is the inverse probability that θ assigns to the test set, normalized by the test set length. For a test set of n tokens w1:n the perplexity is :\\n12 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\nthe pretraining data, and so you’ll sometimes see this method called continued pre-\\ntraining .\\ncontinued\\npretraining\\nRetraining all the parameters of the model is very slow and expensive when the\\nlanguage model is huge. So instead we can freeze some of the parameters (i.e., leavefreeze\\nthem unchanged from their pretrained value) and train only a subset of parameters\\non the new data. In Section 10.5.3 we’ll describe this second variety of ﬁnetun-\\ning, called parameter-efﬁcient ﬁnetuning , or PEFT . because we efﬁciently select\\nspeciﬁc parameters to update when ﬁnetuning, and leave the rest in their pretrained\\nvalues.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 50, 'page_label': '51', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='ing, called parameter-efﬁcient ﬁnetuning , or PEFT . because we efﬁciently select\\nspeciﬁc parameters to update when ﬁnetuning, and leave the rest in their pretrained\\nvalues.\\nIn Chapter 11 we’ll introduce a third kind of ﬁnetuning, also parameter-efﬁcient.\\nIn this version, the goal is to use a language model as a kind of classiﬁer or labeler\\nfor a speciﬁc task. For example we might train the model to be a sentiment classiﬁer.\\nWe do this by adding extra neural circuitry (an extra head ) after the top layer of the\\nmodel. This classiﬁcation head takes as input some of the top layer embeddings of\\nthe transformer and produces as output a classiﬁcation. In this method, most com-\\nmonly used with masked language models like BERT, we freeze the entire pretrained\\nmodel and only train the classiﬁcation head on some new data, usually labeled with\\nsome class that we want to predict.\\nFinally, in Chapter 12 we’ll introduce a fourth kind of ﬁnetuning, that is a cru-'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 50, 'page_label': '51', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='some class that we want to predict.\\nFinally, in Chapter 12 we’ll introduce a fourth kind of ﬁnetuning, that is a cru-\\ncial component of the largest language models: supervised ﬁnetuning or SFT . SFT\\nis often used for instruction ﬁnetuning , in which we want a pretrained language\\nmodel to learn to follow text instructions, for example to answer questions or follow\\na command to write something. Here we create a dataset of prompts and desired\\nresponses (for example questions and their answers, or commands and their ful-\\nﬁllments), and we train the language model using the normal cross-entropy loss to\\npredict each token in the instruction prompt iteratively, essentially training it to pro-\\nduce the desired response from the command in the prompt. It’s called supervised\\nbecause unlike in pretraining, where we just take any data and predict the words in\\nit, we build the special ﬁnetuning dataset by hand, creating supervised responses to\\neach command.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 50, 'page_label': '51', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='because unlike in pretraining, where we just take any data and predict the words in\\nit, we build the special ﬁnetuning dataset by hand, creating supervised responses to\\neach command.\\nOften everything that happens after pretraining is lumped together as post-training ;\\nwe’ll discuss the various parts of post-training in Chapter 12 and Chapter 13.\\n10.4 Evaluating Large Language Models\\nPerplexity As we ﬁrst saw in Chapter 3, one way to evaluate language models is\\nto measure how well they predict unseen text. Intuitively, good models are those that\\nassign higher probabilities to unseen data (are less surprised when encountering the\\nnew words).\\nWe instantiate this intuition by using perplexity to measure the quality of aperplexity\\nlanguage model. Recall from page ?? that the perplexity of a model q on an unseen\\ntest set is the inverse probability that q assigns to the test set, normalized by the test\\nset length. For a test set of n tokens w 1: n , the perplexity is\\nPerplexity\\nq'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 50, 'page_label': '51', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='test set is the inverse probability that q assigns to the test set, normalized by the test\\nset length. For a test set of n tokens w 1: n , the perplexity is\\nPerplexity\\nq\\n( w 1: n )= P q ( w 1: n )\\n\\x00\\n1\\nn\\n=\\nn\\ns\\n1\\nP q ( w 1: n )\\n(10.7)\\nTo visualize how perplexity can be computed as a function of the probabilities the'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 51, 'page_label': '52', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='•Probability depends on size of test set•Probability gets smaller the longer the text•Better: a metric that is per-word, normalized by length•Perplexity is the inverse probability of the test set, normalized by the number of words(The inverse comes from the original definition of perplexity from cross-entropy rate in information theory)Probability range is  [0,1], perplexity range is [1,∞]\\nWhy perplexity instead of raw probability of the test set?'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 52, 'page_label': '53', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Perplexity•The higher the probability of the word sequence, the lower the perplexity.•Thus the lower the perplexity of a model on the data, the better the model. •Minimizing perplexity is the same as maximizing probabilityAlso: perplexity is sensitive to length/tokenization so best used when comparing LMs that use the same tokenizer.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 53, 'page_label': '54', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Many other factors that we evaluate, like:Size Big models take lots of GPUs and time to train, memory to storeEnergy usageCan measure kWh or kilograms of CO2 emitted FairnessBenchmarks measure gendered and racial stereotypes, or decreased performance for language from or about some groups.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 54, 'page_label': '55', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsDealing with Scale'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 55, 'page_label': '56', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Scaling LawsLLM performance depends on•Model size: the number of parameters not counting embeddings•Dataset size: the amount of training data•Compute: Amount of compute (in FLOPS or etcCan improve a model by adding  parameters (more layers, wider contexts), more data, or training for more iterationsThe performance of a large language model (the loss) scales as a power-law with each of these three'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 56, 'page_label': '57', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Scaling LawsLoss L as a function of # parameters N, dataset size D, compute budget C (if other two are held constant)\\n14 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\n10.5 Dealing with Scale\\nLarge language models are large. For example the Llama 3.1 405B Instruct model\\nfrom Meta has 405 billion parameters (126 layers, a model dimensionality of 16,384,\\n128 attention heads) and was trained on 15.6 terabytes of text tokens ( Llama Team ,\\n2024 ), using a vocabulary of 128K tokens. So there is a lot of research on un-\\nderstanding how LLMs scale, and especially how to implement them given limited\\nresources. In the next few sections we discuss how to think about scale (the concept\\nof scaling laws ), and important techniques for getting language models to work\\nefﬁciently, such as the KV cache and parameter-efﬁcient ﬁne tuning.\\n10.5.1 Scaling laws\\nThe performance of large language models has shown to be mainly determined by'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 56, 'page_label': '57', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='efﬁciently, such as the KV cache and parameter-efﬁcient ﬁne tuning.\\n10.5.1 Scaling laws\\nThe performance of large language models has shown to be mainly determined by\\n3 factors: model size (the number of parameters not counting embeddings), dataset\\nsize (the amount of training data), and the amount of compute used for training. That\\nis, we can improve a model by adding parameters (adding more layers or having\\nwider contexts or both), by training on more data, or by training for more iterations.\\nThe relationships between these factors and performance are known as scaling\\nlaws . Roughly speaking, the performance of a large language model (the loss) scalesscaling laws\\nas a power-law with each of these three properties of model training.\\nFor example, Kaplan et al. ( 2020 ) found the following three relationships for\\nloss L as a function of the number of non-embedding parameters N , the dataset size\\nD , and the compute budget C , for models training with limited parameters, dataset,'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 56, 'page_label': '57', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='loss L as a function of the number of non-embedding parameters N , the dataset size\\nD , and the compute budget C , for models training with limited parameters, dataset,\\nor compute budget, if in each case the other two properties are held constant:\\nL ( N )=\\n✓\\nN c\\nN\\n◆\\na N\\n(10.9)\\nL ( D )=\\n✓\\nD c\\nD\\n◆\\na D\\n(10.10)\\nL ( C )=\\n✓\\nC c\\nC\\n◆\\na\\nC\\n(10.11)\\nThe number of (non-embedding) parameters N can be roughly computed as fol-\\nlows (ignoring biases, and with d as the input and output dimensionality of the\\nmodel, d attn\\nas the self-attention layer size, and d ff\\nthe size of the feedforward layer):\\nN ⇡ 2 dn layer\\n( 2 d attn\\n+ d ff\\n)\\n⇡ 12 n layer\\nd\\n2\\n(10.12)\\n( assuming d attn\\n= d ff\\n/ 4 = d )\\nThus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 ⇥ 96 ⇥\\n12288\\n2\\n⇡ 175 billion parameters.\\nThe values of N c\\n, D c\\n, C c\\n, a N\\n, a D\\n, and a C\\ndepend on the exact transformer\\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 56, 'page_label': '57', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='2\\n⇡ 175 billion parameters.\\nThe values of N c\\n, D c\\n, C c\\n, a N\\n, a D\\n, and a C\\ndepend on the exact transformer\\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,\\nscaling laws focus on the relationship with loss.\\n2\\nScaling laws can be useful in deciding how to train a model to a particular per-\\nformance, for example by looking at early in the training curve, or performance with\\n2\\nFor the initial experiment in Kaplan et al. ( 2020 ) the precise values were a N\\n= 0.076, N c\\n= 8.8 ⇥ 10\\n13\\n(parameters), a D\\n= 0.095, D c\\n= 5.4 ⇥ 10\\n13\\n(tokens), a C\\n= 0.050, C c\\n= 3.1 ⇥ 10\\n8\\n(petaﬂop-days).\\nScaling laws can be used early in training to predict what the loss would be if we were to add more data or increase model size.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 57, 'page_label': '58', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Number of non-embedding parameters N\\n14 C HAPTER 10 • L ARGE L ANGUAGE M ODELS\\n10.5 Dealing with Scale\\nLarge language models are large. For example the Llama 3.1 405B Instruct model\\nfrom Meta has 405 billion parameters (126 layers, a model dimensionality of 16,384,\\n128 attention heads) and was trained on 15.6 terabytes of text tokens ( Llama Team ,\\n2024 ), using a vocabulary of 128K tokens. So there is a lot of research on un-\\nderstanding how LLMs scale, and especially how to implement them given limited\\nresources. In the next few sections we discuss how to think about scale (the concept\\nof scaling laws ), and important techniques for getting language models to work\\nefﬁciently, such as the KV cache and parameter-efﬁcient ﬁne tuning.\\n10.5.1 Scaling laws\\nThe performance of large language models has shown to be mainly determined by\\n3 factors: model size (the number of parameters not counting embeddings), dataset'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 57, 'page_label': '58', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='10.5.1 Scaling laws\\nThe performance of large language models has shown to be mainly determined by\\n3 factors: model size (the number of parameters not counting embeddings), dataset\\nsize (the amount of training data), and the amount of compute used for training. That\\nis, we can improve a model by adding parameters (adding more layers or having\\nwider contexts or both), by training on more data, or by training for more iterations.\\nThe relationships between these factors and performance are known as scaling\\nlaws . Roughly speaking, the performance of a large language model (the loss) scalesscaling laws\\nas a power-law with each of these three properties of model training.\\nFor example, Kaplan et al. ( 2020 ) found the following three relationships for\\nloss L as a function of the number of non-embedding parameters N , the dataset size\\nD , and the compute budget C , for models training with limited parameters, dataset,'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 57, 'page_label': '58', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='loss L as a function of the number of non-embedding parameters N , the dataset size\\nD , and the compute budget C , for models training with limited parameters, dataset,\\nor compute budget, if in each case the other two properties are held constant:\\nL ( N )=\\n✓\\nN c\\nN\\n◆\\na N\\n(10.9)\\nL ( D )=\\n✓\\nD c\\nD\\n◆\\na D\\n(10.10)\\nL ( C )=\\n✓\\nC c\\nC\\n◆\\na\\nC\\n(10.11)\\nThe number of (non-embedding) parameters N can be roughly computed as fol-\\nlows (ignoring biases, and with d as the input and output dimensionality of the\\nmodel, d attn\\nas the self-attention layer size, and d ff\\nthe size of the feedforward layer):\\nN ⇡ 2 dn layer\\n( 2 d attn\\n+ d ff\\n)\\n⇡ 12 n layer\\nd\\n2\\n(10.12)\\n( assuming d attn\\n= d ff\\n/ 4 = d )\\nThus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 ⇥ 96 ⇥\\n12288\\n2\\n⇡ 175 billion parameters.\\nThe values of N c\\n, D c\\n, C c\\n, a N\\n, a D\\n, and a C\\ndepend on the exact transformer\\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 57, 'page_label': '58', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='2\\n⇡ 175 billion parameters.\\nThe values of N c\\n, D c\\n, C c\\n, a N\\n, a D\\n, and a C\\ndepend on the exact transformer\\narchitecture, tokenization, and vocabulary size, so rather than all the precise values,\\nscaling laws focus on the relationship with loss.\\n2\\nScaling laws can be useful in deciding how to train a model to a particular per-\\nformance, for example by looking at early in the training curve, or performance with\\n2\\nFor the initial experiment in Kaplan et al. ( 2020 ) the precise values were a N\\n= 0.076, N c\\n= 8.8 ⇥ 10\\n13\\n(parameters), a D\\n= 0.095, D c\\n= 5.4 ⇥ 10\\n13\\n(tokens), a C\\n= 0.050, C c\\n= 3.1 ⇥ 10\\n8\\n(petaﬂop-days).\\nThus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 × 96 × 122882 ≈ 175 billion parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 58, 'page_label': '59', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content=\"KV CacheIn training, we can compute attention very efficiently in parallel:\\nBut not at inference! We generate the next tokens one at a time!For a new token x, need to multiply by WQ, WK, and WV to get query, key, valuesBut don't want to recompute the key and value vectors for all the prior tokens x<iInstead, store key and value vectors in memory in the KV cache, and then we can just grab them from the cache \\n10.5 • D EALING WITH S CALE 15\\nsmaller amounts of data, to predict what the loss would be if we were to add more\\ndata or increase model size. Other aspects of scaling laws can also tell us how much\\ndata we need to add when scaling up a model.\\n10.5.2 KV Cache\\nWe saw in Fig. ?? and in Eq. ?? (repeated below) how the attention vector can be\\nvery efﬁciently computed in parallel for training, via two matrix multiplications:\\nA = softmax\\n✓\\nQK\\n|\\np\\nd k\\n◆\\nV (10.13)\\nUnfortunately we can’t do quite the same efﬁcient computation in inference as\"),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 58, 'page_label': '59', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='very efﬁciently computed in parallel for training, via two matrix multiplications:\\nA = softmax\\n✓\\nQK\\n|\\np\\nd k\\n◆\\nV (10.13)\\nUnfortunately we can’t do quite the same efﬁcient computation in inference as\\nin training. That’s because at inference time, we iteratively generate the next tokens\\none at a time. For a new token that we have just generated, call it x i , we need to\\ncompute its query, key, and values by multiplying by W\\nQ\\n, W\\nK\\n, and W\\nV\\nrespec-\\ntively. But it would be a waste of computation time to recompute the key and value\\nvectors for all the prior tokens x < i ; at prior steps we already computed these key\\nand value vectors! So instead of recomputing these, whenever we compute the key\\nand value vectors we store them in memory in the KV cache , and then we can justKV cache\\ngrab them from the cache when we need them. Fig. 10.7 modiﬁes Fig. ?? to show\\nthe computation that takes place for a single new token, showing which values we\\ncan take from the cache rather than recompute.\\nq4'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 58, 'page_label': '59', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='the computation that takes place for a single new token, showing which values we\\ncan take from the cache rather than recompute.\\nq4\\nk1\\nk2\\nk4\\nQ\\nKT\\nQKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq4•k1q4•k2q4•k3q4•k4\\nx = =x\\na4\\nA\\n1 x dk\\ndk x N\\n1 x N N x dv 1 x dv\\nk3\\nFigure 10.7 Parts of the attention computation (extracted from Fig. ?? ) showing, in black,\\nthe vectors that can be stored in the cache rather than recomputed when computing the atten-\\ntion score for the 4th token.\\n10.5.3 Parameter Efﬁcient Fine Tuning\\nAs we mentioned above, it’s very common to take a language model and give it more\\ninformation about a new domain by ﬁnetuning it (continuing to train it to predict\\nupcoming words) on some additional data.\\nFine-tuning can be very difﬁcult with very large language models, because there\\nare enormous numbers of parameters to train; each pass of batch gradient descent\\nhas to backpropagate through many many huge layers. This makes ﬁnetuning huge'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 58, 'page_label': '59', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='are enormous numbers of parameters to train; each pass of batch gradient descent\\nhas to backpropagate through many many huge layers. This makes ﬁnetuning huge\\nlanguage models extremely expensive in processing power, in memory, and in time.\\nFor this reason, there are alternative methods that allow a model to be ﬁnetuned\\nwithout changing all the parameters. Such methods are called parameter-efﬁcient\\nﬁne tuning or sometimes PEFT , because we efﬁciently select a subset of parameters\\nparameter-\\nefﬁcient ﬁne\\ntuning\\nPEFT to update when ﬁnetuning. For example we freeze some of the parameters (don’t\\nchange them), and only update some particular subset of parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 59, 'page_label': '60', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='KV Cache\\nq4\\nk1\\nk2\\nk4\\nQ\\nKT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq4•k1q4•k2q4•k3q4•k4\\nx = =x\\na4\\nA\\n1 x dk\\ndk x N\\n1 x N N x dv 1 x dv\\nk3\\nq1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2q4•k3q4•k4\\nq3•k2q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1q2•k2\\nq4•k1q4•k2q4•k3q4•k4\\nq3•k1q3•k2q3•k3\\nq1•k2\\nq2•k3\\nq1•k3\\nq3•k4\\nq2•k4\\nq1•k4x =\\nQKT masked\\nmask =\\nq1•k1\\nq2•k1\\nq4•k1\\nq3•k1\\nq1•k1q1•k1\\n=x\\na1\\na2\\na3\\na4\\nA\\nQuery \\nToken 1\\nQuery \\nToken 2\\nQuery \\nToken 3\\nQuery \\nToken 4\\nQ\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nx\\nWQ\\n=\\nValue \\nToken 1\\nValue \\nToken 2\\nValue \\nToken 3\\nValue \\nToken 4\\nV\\nx\\nWV\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nKey \\nToken 1\\nKey \\nToken 2\\nKey \\nToken 3\\nKey \\nToken 4\\nK\\nx\\nWK\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nN x dk\\ndk x N\\nN x N N x N N x dv N x dv\\nd x dk d x dk d x dvN x d N x dk N x d N x dk N x d N x dv\\nq1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2q4•k3q4•k4\\nq3•k2q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1q2•k2\\nq4•k1q4•k2q4•k3q4•k4'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 59, 'page_label': '60', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='d x dk d x dk d x dvN x d N x dk N x d N x dk N x d N x dv\\nq1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2q4•k3q4•k4\\nq3•k2q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1q2•k2\\nq4•k1q4•k2q4•k3q4•k4\\nq3•k1q3•k2q3•k3\\nq1•k2\\nq2•k3\\nq1•k3\\nq3•k4\\nq2•k4\\nq1•k4x =\\nQKT masked\\nmask =\\nq1•k1\\nq2•k1\\nq4•k1\\nq3•k1\\nq1•k1q1•k1\\n=x\\na1\\na2\\na3\\na4\\nA\\nQuery \\nToken 1\\nQuery \\nToken 2\\nQuery \\nToken 3\\nQuery \\nToken 4\\nQ\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nx\\nWQ\\n=\\nValue \\nToken 1\\nValue \\nToken 2\\nValue \\nToken 3\\nValue \\nToken 4\\nV\\nx\\nWV\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nKey \\nToken 1\\nKey \\nToken 2\\nKey \\nToken 3\\nKey \\nToken 4\\nK\\nx\\nWK\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nN x dk\\ndk x N\\nN x N N x N N x dv N x dv\\nd x dk d x dk d x dvN x d N x dk N x d N x dk N x d N x dv\\nq1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2q4•k3q4•k4\\nq3•k2q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1q2•k2\\nq4•k1q4•k2q4•k3q4•k4\\nq3•k1q3•k2q3•k3\\nq1•k2\\nq2•k3\\nq1•k3\\nq3•k4\\nq2•k4\\nq1•k4x ='),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 59, 'page_label': '60', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='q1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT QKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2q4•k3q4•k4\\nq3•k2q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1q2•k2\\nq4•k1q4•k2q4•k3q4•k4\\nq3•k1q3•k2q3•k3\\nq1•k2\\nq2•k3\\nq1•k3\\nq3•k4\\nq2•k4\\nq1•k4x =\\nQKT masked\\nmask =\\nq1•k1\\nq2•k1\\nq4•k1\\nq3•k1\\nq1•k1q1•k1\\n=x\\na1\\na2\\na3\\na4\\nA\\nQuery \\nToken 1\\nQuery \\nToken 2\\nQuery \\nToken 3\\nQuery \\nToken 4\\nQ\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nx\\nWQ\\n=\\nValue \\nToken 1\\nValue \\nToken 2\\nValue \\nToken 3\\nValue \\nToken 4\\nV\\nx\\nWV\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nKey \\nToken 1\\nKey \\nToken 2\\nKey \\nToken 3\\nKey \\nToken 4\\nK\\nx\\nWK\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nN x dk\\ndk x N\\nN x N N x N N x dv N x dv\\nd x dk d x dk d x dvN x d N x dk N x d N x dk N x d N x dv'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 60, 'page_label': '61', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Parameter-Efficient FinetuningAdapting to a new domain by continued pretraining (finetuning) is a problem with huge LLMs.•Enormous numbers of parameters to train •Each pass of batch gradient descent has to backpropagate through many many huge layers. •Expensive in processing power, in memory, and in time. Instead, parameter-efficient fine tuning (PEFT)•Efficiently select a subset of parameters to update when finetuning.•E.g., freeze some of the parameters (don’t change them), •And only update some a few parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 61, 'page_label': '62', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='LoRA (Low-Rank Adaptation)•Trransformers have many dense matrix multiply layers•Like WQ, WK, WV, WO layers in attention•Instead of updating these layers during finetuning, •Freeze these layers •Update a low-rank approximation with fewer parameters.'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 62, 'page_label': '63', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='LoRA•Consider a matrix W (shape [N × d])  that needs to be updated during finetuning via gradient descent. •Normally updates are ∆W  (shape [N × d])•In LoRA, we freeze W and update instead a low-rank decomposition of W:•A of shape [N×r], •B of shape [r×d], r is very small  (like 1 or 2)•That is, during  finetuning we update A and B instead of W. •Replace W + ∆W with W + BA. Forward pass: instead of     h = xW We do     h = xW + xAB'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 63, 'page_label': '64', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='LoRAh\\nPretrained \\nWeights\\nW\\nd\\nk r\\nk\\nA\\nBr\\nx\\nd\\n1\\n1\\nk\\nd\\n×'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 64, 'page_label': '65', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsDealing with Scale'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 65, 'page_label': '66', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsHarms of Large Language Models'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 66, 'page_label': '67', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Hallucination'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 67, 'page_label': '68', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Copyright'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 68, 'page_label': '69', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Privacy'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 69, 'page_label': '70', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Toxicity and Abuse'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 70, 'page_label': '71', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Misinformation'),\n",
       " Document(metadata={'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext', 'creator': 'PowerPoint', 'creationdate': \"D:20240820140818Z00'00'\", 'moddate': \"D:20240820140818Z00'00'\", 'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf', 'total_pages': 72, 'page': 71, 'page_label': '72', 'source_file': 'LLM24aug.pdf', 'file_type': 'pdf'}, page_content='Large Language ModelsHarms of Large Language Models')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203c3d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 134 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:05<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (134, 384)\n",
      "Adding 134 documents to vector store...\n",
      "Successfully added 134 documents to vector store\n",
      "Total documents in collection: 134\n"
     ]
    }
   ],
   "source": [
    "### Convert text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## generate Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store into the vectore database\n",
    "\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee27e4",
   "metadata": {},
   "source": [
    "Retiever pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1acfa100",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2aa1a05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x26791e97a10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aa05786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is LLM Models'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_3d6c58d0_80',\n",
       "  'content': 'Large Language ModelsPretraining data for LLMs',\n",
       "  'metadata': {'page_label': '36',\n",
       "   'page': 35,\n",
       "   'creationdate': \"D:20240820140818Z00'00'\",\n",
       "   'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext',\n",
       "   'total_pages': 72,\n",
       "   'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf',\n",
       "   'doc_index': 80,\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 46,\n",
       "   'creator': 'PowerPoint',\n",
       "   'moddate': \"D:20240820140818Z00'00'\",\n",
       "   'source_file': 'LLM24aug.pdf'},\n",
       "  'similarity_score': 0.06880223751068115,\n",
       "  'distance': 0.9311977624893188,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_7e0ffea5_89',\n",
       "  'content': 'Large Language ModelsPretraining data for LLMs',\n",
       "  'metadata': {'page_label': '43',\n",
       "   'producer': 'macOS Version 14.6.1 (Build 23G93) Quartz PDFContext',\n",
       "   'creationdate': \"D:20240820140818Z00'00'\",\n",
       "   'content_length': 46,\n",
       "   'doc_index': 89,\n",
       "   'page': 42,\n",
       "   'source': '..\\\\data\\\\PDF\\\\LLM24aug.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': 'LLM24aug.pdf',\n",
       "   'moddate': \"D:20240820140818Z00'00'\",\n",
       "   'total_pages': 72,\n",
       "   'creator': 'PowerPoint'},\n",
       "  'similarity_score': 0.06880223751068115,\n",
       "  'distance': 0.9311977624893188,\n",
       "  'rank': 2}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is LLM Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2564d6",
   "metadata": {},
   "source": [
    "Integration Vectordb context pipeline with LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad071e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key =\"gYOUR API KEY\"\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name =\"llama-3.1-8b-instant\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function : retrive context +  generate response\n",
    "def rag_simple(query,retriver,llm,top_k=3):\n",
    "    ## retriver context\n",
    "    results=retriver.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question\"\n",
    "    \n",
    "    ## generate the answer using Groq LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29a85871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is encoders?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 58.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoders are neural networks that can condition on future context, allowing them to get bidirectional context.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is encoders?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333a2174",
   "metadata": {},
   "source": [
    "Enchanced RAG Pipelin features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "573dab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is LLM?'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 1 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Large Language Model (LLM)\n",
      "Sources: [{'source': 'LLM24aug.pdf', 'page': 36, 'score': 0.12569493055343628, 'preview': 'LLMs are mainly trained on the webCommon crawl, snapshots of the entire web produced by the non- profit Common Crawl wit....'}]\n",
      "Confidence: 0.12569493055343628\n",
      "Context Preview: LLMs are mainly trained on the webCommon crawl, snapshots of the entire web produced by the non- profit Common Crawl with billions of pagesColossal Clean Crawled Corpus (C4; Raffel et al. 2020), 156 billion tokens of English,  filtered What's in it? Mostly patent text documents, Wikipedia, and news \n"
     ]
    }
   ],
   "source": [
    "#----Enchanced RAG pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG Pipeline with extra features:\n",
    "    - Returns answer, socurces, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return{'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context= \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources =[{\n",
    "        'source': doc ['metadata'].get('source_file', doc['metadata'].get('source','unknown')),\n",
    "        'page' : doc ['metadata'].get('page','unkown'),\n",
    "        'score' : doc['similarity_score'],\n",
    "        'preview': doc ['content'][:120]+ '....'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate Prompt\n",
    "    prompt= f\"\"\"Use the following context to answer the  question concisely.\\ncontext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer: \"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer' : response.content,\n",
    "        'sources' : sources,\n",
    "        'confidence' : confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"What is LLM?\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b554be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is Temperature sampling?'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "Temperature samplingReshape the distribution instead of truncating itIntuition from thermod"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ynamics, •a system at high temperature is flexible and can explore many possible states,•a system at lower temperature is likely to explore a subset of lower energy (better) states. In low-temperature sampling,  (τ ≤ 1) we smoothly•increase the probability of the most probable words•decrease the probability of the rare words.\n",
      "\n",
      "measure will be more robust in very different contexts, dynamically increasing and\n",
      "decreasing the pool of word candidates.\n",
      "Given a distribution P ( w\n",
      "t\n",
      "| w\n",
      "< t\n",
      ") , the top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of\n",
      "words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ") \u0000 p . (10.2)\n",
      "10.2.3 Temperature sampling\n",
      "In temperature sampling , we don’t truncate the distribution, but instead reshape\n",
      "temperature\n",
      "sampling\n",
      "it. The intuition for temperature sampling comes from thermodynamics, where a\n",
      "system at a high temperature is very ﬂexible and can explore many possible states,\n",
      "while a system at a lower temperature is likely to explore a subset of lower energy\n",
      "(better) states. In low-temperature sampling, we smoothly increase the probability\n",
      "of the most probable words and decrease the probability of the rare words.\n",
      "We implement this intuition by simply dividing the logit by a temperature param-\n",
      "eter t before we normalize it by passing it through the softmax. In low-temperature\n",
      "\n",
      "Question: what is Temperature sampling?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer:  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "P ( w | w\n",
      "< t\n",
      ")  ≥  p\n",
      ".\n",
      "The top- p vocabulary V\n",
      "( p )\n",
      "is the smallest set of words such that\n",
      "X\n",
      "w 2 V\n",
      "( p )\n",
      "\n",
      "\n",
      "Citations:\n",
      "[1] LLM24aug.pdf (page 23)\n",
      "[2] LLM24aug.pdf (page 25)\n",
      "Summary: There is no actual answer provided, only a repeated statement. However, I can provide a summary of the statement in 2 sentences:\n",
      "\n",
      "The top-p vocabulary V(p) is defined as the smallest set of words such that for each word w in V(p), the probability of w given that w is not in the training set (P(w|w<t)) is greater than or equal to p. This definition aims to select the most relevant words from the vocabulary based on their conditional probability.\n",
      "History: {'question': 'what is Temperature sampling?', 'answer': ' ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\nP ( w | w\\n< t\\n)  ≥  p\\n.\\nThe top- p vocabulary V\\n( p )\\nis the smallest set of words such that\\nX\\nw 2 V\\n( p )\\n', 'sources': [{'source': 'LLM24aug.pdf', 'page': 23, 'score': 0.28071725368499756, 'preview': 'Temperature samplingReshape the distribution instead of truncating itIntuition from thermodynamics, •a system at high te...'}, {'source': 'LLM24aug.pdf', 'page': 25, 'score': 0.17660164833068848, 'preview': 'measure will be more robust in very different contexts, dynamically increasing and\\ndecreasing the pool of word candidate...'}], 'summary': 'There is no actual answer provided, only a repeated statement. However, I can provide a summary of the statement in 2 sentences:\\n\\nThe top-p vocabulary V(p) is defined as the smallest set of words such that for each word w in V(p), the probability of w given that w is not in the training set (P(w|w<t)) is greater than or equal to p. This definition aims to select the most relevant words from the vocabulary based on their conditional probability.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"what is Temperature sampling?\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b54b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YTRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
